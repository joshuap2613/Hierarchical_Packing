{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_foo\n",
    "env = gym.make('foo-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_pack\n",
    "env = gym.make('pack-v0', max_moves=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym_pack.envs.pack_env.PackEnv at 0x1627a4d90>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, models\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 10)\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_32 (Flatten)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 5, 10, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 1, 6, 16)          416       \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 26)                546       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 26)                0         \n",
      "=================================================================\n",
      "Total params: 8,890\n",
      "Trainable params: 8,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#model.add(layers.Conv2D(16, (5, 5), activation=\\'relu\\'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(16, (3, 3), activation=\\'relu\\'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(16, (3, 3), activation=\\'relu\\'))\\nmodel.add(layers.Dense(64, activation=\"relu\"))\\nmodel.add(layers.Dense(nb_actions, activation=\\'linear\\'))\\nmodel.add(Activation(\\'softmax\\'))\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.num_possible_moves+1\n",
    "#env.observation_space = np.zeros((3, 3))\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "#model = Sequential()\n",
    "#model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "#model.add(Dense(nb_actions))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "#Option 2: more complicated\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\"\"\"\n",
    "\n",
    "#Option 3: the paper\n",
    "\n",
    "print((1,) + env.observation_space.shape)\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "#(env.observation_space.shape+ (1,))\n",
    "x = Reshape(env.observation_space.shape+ (1,))\n",
    "model.add(x)\n",
    "#model.add(layers.Conv2D(16, (3, 1), activation='relu', input_shape=(len(X[0]), 1, 1)))\n",
    "#model.add(layers.MaxPooling2D((3, 1)))\n",
    "#model.add(layers.Conv2D(32, (9, 1), activation='relu', input_shape=(len(X[0]), 1, 1)))\n",
    "#model.add(layers.MaxPooling2D((3, 1)))\n",
    "model.add(layers.Conv2D(16, (5, 5), activation='relu', input_shape=(1,) +env.observation_space.shape))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "#model.build(env.observation_space.shape+ (1,))\n",
    "model.summary()\n",
    "\"\"\"\n",
    "#model.add(layers.Conv2D(16, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(nb_actions, activation='linear'))\n",
    "model.add(Activation('softmax'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_32 (Flatten)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 5, 10, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 1, 6, 16)          416       \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 26)                546       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 26)                0         \n",
      "=================================================================\n",
      "Total params: 8,890\n",
      "Trainable params: 8,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 10000 steps ...\n",
      "    8/10000: episode: 1, duration: 1.218s, episode steps: 8, steps per second: 7, episode reward: -102.750, mean reward: -12.844 [-100.000, 2.250], mean action: 19.250 [5.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      "   16/10000: episode: 2, duration: 0.015s, episode steps: 8, steps per second: 530, episode reward: 11.000, mean reward: 1.375 [1.000, 3.000], mean action: 12.125 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "   24/10000: episode: 3, duration: 0.014s, episode steps: 8, steps per second: 586, episode reward: -91.375, mean reward: -11.422 [-100.000, 1.562], mean action: 18.000 [8.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "   32/10000: episode: 4, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: 11.833, mean reward: 1.479 [1.000, 2.000], mean action: 10.875 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "   40/10000: episode: 5, duration: 0.012s, episode steps: 8, steps per second: 664, episode reward: -89.000, mean reward: -11.125 [-100.000, 2.000], mean action: 10.625 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "   48/10000: episode: 6, duration: 0.016s, episode steps: 8, steps per second: 489, episode reward: 14.000, mean reward: 1.750 [1.000, 3.000], mean action: 10.125 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "   56/10000: episode: 7, duration: 0.012s, episode steps: 8, steps per second: 653, episode reward: 11.917, mean reward: 1.490 [1.000, 2.667], mean action: 14.250 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "   64/10000: episode: 8, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: 3.500, mean reward: 0.438 [-5.000, 1.500], mean action: 18.500 [11.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "   72/10000: episode: 9, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: -94.667, mean reward: -11.833 [-100.000, 2.778], mean action: 13.625 [6.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      "   80/10000: episode: 10, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 10.000 [0.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "   88/10000: episode: 11, duration: 0.013s, episode steps: 8, steps per second: 619, episode reward: -2.604, mean reward: -0.326 [-5.000, 1.562], mean action: 12.750 [4.000, 20.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "   96/10000: episode: 12, duration: 0.011s, episode steps: 8, steps per second: 698, episode reward: 3.250, mean reward: 0.406 [-5.000, 2.250], mean action: 10.125 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  104/10000: episode: 13, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: 9.500, mean reward: 1.188 [1.000, 1.500], mean action: 8.500 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  112/10000: episode: 14, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 14.000, mean reward: 1.750 [1.000, 3.000], mean action: 11.875 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  120/10000: episode: 15, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 10.250 [1.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      "  128/10000: episode: 16, duration: 0.014s, episode steps: 8, steps per second: 587, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 12.500 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  136/10000: episode: 17, duration: 0.014s, episode steps: 8, steps per second: 587, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 8.750 [3.000, 16.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      "  144/10000: episode: 18, duration: 0.017s, episode steps: 8, steps per second: 467, episode reward: 10.667, mean reward: 1.333 [1.000, 2.667], mean action: 13.250 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  152/10000: episode: 19, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 12.875 [1.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  160/10000: episode: 20, duration: 0.012s, episode steps: 8, steps per second: 680, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.000 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  168/10000: episode: 21, duration: 0.012s, episode steps: 8, steps per second: 645, episode reward: 3.667, mean reward: 0.458 [-5.000, 2.000], mean action: 12.625 [5.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  176/10000: episode: 22, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: 13.250, mean reward: 1.656 [1.000, 3.000], mean action: 8.875 [3.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  184/10000: episode: 23, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 17.875 [9.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  192/10000: episode: 24, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: 13.722, mean reward: 1.715 [1.000, 2.778], mean action: 11.750 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  200/10000: episode: 25, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: -205.000, mean reward: -25.625 [-100.000, 2.000], mean action: 7.875 [0.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      "  208/10000: episode: 26, duration: 0.013s, episode steps: 8, steps per second: 612, episode reward: -98.167, mean reward: -12.271 [-100.000, 1.333], mean action: 15.125 [1.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      "  216/10000: episode: 27, duration: 0.017s, episode steps: 8, steps per second: 469, episode reward: 8.500, mean reward: 1.062 [1.000, 1.500], mean action: 12.625 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  224/10000: episode: 28, duration: 0.015s, episode steps: 8, steps per second: 548, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 9.625 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  232/10000: episode: 29, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 11.500, mean reward: 1.438 [1.000, 2.000], mean action: 13.750 [6.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  240/10000: episode: 30, duration: 0.014s, episode steps: 8, steps per second: 562, episode reward: -7.750, mean reward: -0.969 [-5.000, 2.250], mean action: 14.500 [1.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  248/10000: episode: 31, duration: 0.016s, episode steps: 8, steps per second: 496, episode reward: -95.333, mean reward: -11.917 [-100.000, 2.083], mean action: 10.125 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  256/10000: episode: 32, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: 2.778, mean reward: 0.347 [-5.000, 1.778], mean action: 14.750 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  264/10000: episode: 33, duration: 0.015s, episode steps: 8, steps per second: 527, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 9.875 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  272/10000: episode: 34, duration: 0.013s, episode steps: 8, steps per second: 610, episode reward: -2.722, mean reward: -0.340 [-5.000, 1.778], mean action: 16.000 [7.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  280/10000: episode: 35, duration: 0.015s, episode steps: 8, steps per second: 527, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 8.125 [1.000, 20.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  288/10000: episode: 36, duration: 0.016s, episode steps: 8, steps per second: 514, episode reward: -189.500, mean reward: -23.688 [-100.000, 2.250], mean action: 14.375 [5.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      "  296/10000: episode: 37, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 10.500 [0.000, 23.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      "  304/10000: episode: 38, duration: 0.016s, episode steps: 8, steps per second: 498, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 13.500 [0.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      "  312/10000: episode: 39, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: 12.078, mean reward: 1.510 [1.000, 2.400], mean action: 9.875 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  320/10000: episode: 40, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: -98.000, mean reward: -12.250 [-100.000, 1.500], mean action: 9.000 [1.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      "  328/10000: episode: 41, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: -0.500, mean reward: -0.062 [-5.000, 2.250], mean action: 9.375 [2.000, 19.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  336/10000: episode: 42, duration: 0.017s, episode steps: 8, steps per second: 473, episode reward: -95.972, mean reward: -11.997 [-100.000, 2.250], mean action: 10.750 [2.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      "  344/10000: episode: 43, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: 7.333, mean reward: 0.917 [-5.000, 2.778], mean action: 13.375 [3.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  352/10000: episode: 44, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.000], mean action: 10.250 [0.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  360/10000: episode: 45, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 10.375 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  368/10000: episode: 46, duration: 0.015s, episode steps: 8, steps per second: 544, episode reward: -97.375, mean reward: -12.172 [-100.000, 1.562], mean action: 8.375 [0.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      "  376/10000: episode: 47, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 15.750 [9.000, 23.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      "  384/10000: episode: 48, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: 18.375, mean reward: 2.297 [1.000, 4.500], mean action: 12.125 [5.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  392/10000: episode: 49, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: 7.583, mean reward: 0.948 [-5.000, 2.667], mean action: 11.375 [2.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  400/10000: episode: 50, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 12.625 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  408/10000: episode: 51, duration: 0.015s, episode steps: 8, steps per second: 517, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 13.875 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  416/10000: episode: 52, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 15.125 [2.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  424/10000: episode: 53, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: -88.167, mean reward: -11.021 [-100.000, 2.778], mean action: 10.250 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  432/10000: episode: 54, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: -191.444, mean reward: -23.931 [-100.000, 2.000], mean action: 18.375 [1.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      "  440/10000: episode: 55, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: -192.000, mean reward: -24.000 [-100.000, 2.500], mean action: 14.125 [9.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  448/10000: episode: 56, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 13.875 [3.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  456/10000: episode: 57, duration: 0.013s, episode steps: 8, steps per second: 595, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.250], mean action: 11.875 [1.000, 17.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  464/10000: episode: 58, duration: 0.013s, episode steps: 8, steps per second: 638, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 10.750 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  472/10000: episode: 59, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 11.500 [1.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      "  480/10000: episode: 60, duration: 0.016s, episode steps: 8, steps per second: 486, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.125 [4.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  488/10000: episode: 61, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: -6.917, mean reward: -0.865 [-5.000, 2.083], mean action: 11.500 [0.000, 18.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  496/10000: episode: 62, duration: 0.013s, episode steps: 8, steps per second: 621, episode reward: -294.000, mean reward: -36.750 [-100.000, 2.000], mean action: 13.125 [1.000, 25.000], mean observation: 0.068 [0.000, 1.000], mean_best_reward: --\n",
      "  504/10000: episode: 63, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: 7.694, mean reward: 0.962 [-5.000, 2.778], mean action: 11.375 [1.000, 20.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      "  512/10000: episode: 64, duration: 0.012s, episode steps: 8, steps per second: 640, episode reward: -8.000, mean reward: -1.000 [-5.000, 2.000], mean action: 14.875 [2.000, 21.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      "  520/10000: episode: 65, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 10.750 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  528/10000: episode: 66, duration: 0.013s, episode steps: 8, steps per second: 633, episode reward: -0.833, mean reward: -0.104 [-5.000, 2.667], mean action: 10.875 [1.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  536/10000: episode: 67, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 16.125 [8.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      "  544/10000: episode: 68, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 7.250, mean reward: 0.906 [-5.000, 4.000], mean action: 14.750 [10.000, 19.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  552/10000: episode: 69, duration: 0.014s, episode steps: 8, steps per second: 576, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.250], mean action: 12.375 [2.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      "  560/10000: episode: 70, duration: 0.013s, episode steps: 8, steps per second: 638, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.875 [0.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      "  568/10000: episode: 71, duration: 0.012s, episode steps: 8, steps per second: 659, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 11.750 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  576/10000: episode: 72, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 11.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  584/10000: episode: 73, duration: 0.013s, episode steps: 8, steps per second: 620, episode reward: 1.333, mean reward: 0.167 [-5.000, 2.778], mean action: 10.000 [5.000, 17.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  592/10000: episode: 74, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 12.000, mean reward: 1.500 [1.000, 2.000], mean action: 12.625 [4.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  600/10000: episode: 75, duration: 0.016s, episode steps: 8, steps per second: 494, episode reward: 7.722, mean reward: 0.965 [-5.000, 2.778], mean action: 14.375 [4.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  608/10000: episode: 76, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: -0.500, mean reward: -0.062 [-5.000, 2.250], mean action: 16.250 [13.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  616/10000: episode: 77, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -1.333, mean reward: -0.167 [-5.000, 2.667], mean action: 10.125 [2.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  624/10000: episode: 78, duration: 0.015s, episode steps: 8, steps per second: 521, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 12.375 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  632/10000: episode: 79, duration: 0.013s, episode steps: 8, steps per second: 607, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 13.750 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  640/10000: episode: 80, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 12.000 [2.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  648/10000: episode: 81, duration: 0.018s, episode steps: 8, steps per second: 438, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 10.750 [0.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      "  656/10000: episode: 82, duration: 0.012s, episode steps: 8, steps per second: 674, episode reward: -13.750, mean reward: -1.719 [-5.000, 2.250], mean action: 10.625 [5.000, 18.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      "  664/10000: episode: 83, duration: 0.016s, episode steps: 8, steps per second: 487, episode reward: 6.833, mean reward: 0.854 [-5.000, 2.778], mean action: 12.875 [7.000, 18.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  672/10000: episode: 84, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: 18.125, mean reward: 2.266 [1.000, 4.500], mean action: 9.875 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  680/10000: episode: 85, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: 4.556, mean reward: 0.569 [-5.000, 2.000], mean action: 13.750 [4.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      "  688/10000: episode: 86, duration: 0.018s, episode steps: 8, steps per second: 443, episode reward: -84.778, mean reward: -10.597 [-100.000, 3.000], mean action: 13.000 [2.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  696/10000: episode: 87, duration: 0.019s, episode steps: 8, steps per second: 411, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 13.875 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  704/10000: episode: 88, duration: 0.018s, episode steps: 8, steps per second: 443, episode reward: -104.500, mean reward: -13.062 [-100.000, 1.500], mean action: 18.375 [9.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      "  712/10000: episode: 89, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: 10.222, mean reward: 1.278 [-5.000, 3.000], mean action: 13.625 [4.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  720/10000: episode: 90, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: -95.167, mean reward: -11.896 [-100.000, 2.667], mean action: 13.000 [8.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  728/10000: episode: 91, duration: 0.016s, episode steps: 8, steps per second: 502, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 15.375 [6.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      "  736/10000: episode: 92, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: -191.000, mean reward: -23.875 [-100.000, 3.000], mean action: 13.375 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  744/10000: episode: 93, duration: 0.015s, episode steps: 8, steps per second: 532, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 12.375 [0.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  752/10000: episode: 94, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: 8.000, mean reward: 1.000 [-5.000, 3.000], mean action: 8.500 [2.000, 17.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  760/10000: episode: 95, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 15.125 [9.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  768/10000: episode: 96, duration: 0.014s, episode steps: 8, steps per second: 566, episode reward: -96.000, mean reward: -12.000 [-100.000, 2.000], mean action: 17.875 [10.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      "  776/10000: episode: 97, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: -0.833, mean reward: -0.104 [-5.000, 2.667], mean action: 13.625 [5.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  784/10000: episode: 98, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 15.375 [5.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  792/10000: episode: 99, duration: 0.013s, episode steps: 8, steps per second: 632, episode reward: -91.500, mean reward: -11.438 [-100.000, 2.000], mean action: 13.000 [3.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  800/10000: episode: 100, duration: 0.013s, episode steps: 8, steps per second: 619, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 11.250 [1.000, 21.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      "  808/10000: episode: 101, duration: 0.012s, episode steps: 8, steps per second: 664, episode reward: -89.500, mean reward: -11.188 [-100.000, 2.000], mean action: 13.625 [1.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  816/10000: episode: 102, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: -87.417, mean reward: -10.927 [-100.000, 2.667], mean action: 16.625 [8.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  824/10000: episode: 103, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 11.000, mean reward: 1.375 [1.000, 2.250], mean action: 12.625 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  832/10000: episode: 104, duration: 0.015s, episode steps: 8, steps per second: 533, episode reward: -98.000, mean reward: -12.250 [-100.000, 2.000], mean action: 16.625 [5.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      "  840/10000: episode: 105, duration: 0.014s, episode steps: 8, steps per second: 578, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.000 [1.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      "  848/10000: episode: 106, duration: 0.011s, episode steps: 8, steps per second: 714, episode reward: -190.944, mean reward: -23.868 [-100.000, 2.778], mean action: 9.750 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  856/10000: episode: 107, duration: 0.016s, episode steps: 8, steps per second: 511, episode reward: 16.283, mean reward: 2.035 [1.000, 3.267], mean action: 12.250 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  864/10000: episode: 108, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: 0.250, mean reward: 0.031 [-5.000, 4.000], mean action: 13.500 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  872/10000: episode: 109, duration: 0.012s, episode steps: 8, steps per second: 645, episode reward: -95.833, mean reward: -11.979 [-100.000, 2.083], mean action: 17.500 [8.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  880/10000: episode: 110, duration: 0.013s, episode steps: 8, steps per second: 627, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 18.375 [4.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      "  888/10000: episode: 111, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 9.375 [1.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  896/10000: episode: 112, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: 4.125, mean reward: 0.516 [-5.000, 2.000], mean action: 13.000 [6.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  904/10000: episode: 113, duration: 0.015s, episode steps: 8, steps per second: 552, episode reward: -88.000, mean reward: -11.000 [-100.000, 2.500], mean action: 12.375 [1.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  912/10000: episode: 114, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: 0.000, mean reward: 0.000 [-5.000, 3.000], mean action: 17.000 [3.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  920/10000: episode: 115, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 17.500 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      "  928/10000: episode: 116, duration: 0.017s, episode steps: 8, steps per second: 481, episode reward: 7.250, mean reward: 0.906 [-5.000, 3.125], mean action: 6.375 [1.000, 17.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  936/10000: episode: 117, duration: 0.017s, episode steps: 8, steps per second: 470, episode reward: -0.875, mean reward: -0.109 [-5.000, 3.125], mean action: 11.500 [6.000, 20.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      "  944/10000: episode: 118, duration: 0.013s, episode steps: 8, steps per second: 603, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 11.000 [3.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      "  952/10000: episode: 119, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 14.875 [1.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      "  960/10000: episode: 120, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 11.625 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "  968/10000: episode: 121, duration: 0.015s, episode steps: 8, steps per second: 523, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 14.000 [2.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      "  976/10000: episode: 122, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: -103.750, mean reward: -12.969 [-100.000, 2.250], mean action: 14.125 [2.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      "  984/10000: episode: 123, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 9.500 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      "  992/10000: episode: 124, duration: 0.013s, episode steps: 8, steps per second: 628, episode reward: -6.750, mean reward: -0.844 [-5.000, 2.250], mean action: 14.000 [9.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1000/10000: episode: 125, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: -91.500, mean reward: -11.438 [-100.000, 1.500], mean action: 13.250 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1008/10000: episode: 126, duration: 0.012s, episode steps: 8, steps per second: 686, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 9.875 [5.000, 15.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1016/10000: episode: 127, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 9.875 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1024/10000: episode: 128, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: 11.917, mean reward: 1.490 [1.000, 2.667], mean action: 9.625 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1032/10000: episode: 129, duration: 0.015s, episode steps: 8, steps per second: 531, episode reward: -199.500, mean reward: -24.938 [-100.000, 1.500], mean action: 11.500 [4.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1040/10000: episode: 130, duration: 0.013s, episode steps: 8, steps per second: 632, episode reward: 11.000, mean reward: 1.375 [1.000, 2.250], mean action: 13.125 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1048/10000: episode: 131, duration: 0.014s, episode steps: 8, steps per second: 585, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.250], mean action: 11.125 [0.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1056/10000: episode: 132, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: 11.667, mean reward: 1.458 [1.000, 3.000], mean action: 15.000 [7.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1064/10000: episode: 133, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: -3.333, mean reward: -0.417 [-5.000, 1.333], mean action: 9.000 [3.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1072/10000: episode: 134, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 11.625 [1.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1080/10000: episode: 135, duration: 0.015s, episode steps: 8, steps per second: 538, episode reward: 11.500, mean reward: 1.438 [1.000, 2.083], mean action: 13.125 [6.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1088/10000: episode: 136, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 15.250 [3.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1096/10000: episode: 137, duration: 0.014s, episode steps: 8, steps per second: 588, episode reward: 12.833, mean reward: 1.604 [1.000, 2.667], mean action: 12.375 [3.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1104/10000: episode: 138, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: 14.583, mean reward: 1.823 [1.000, 3.000], mean action: 16.000 [7.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1112/10000: episode: 139, duration: 0.014s, episode steps: 8, steps per second: 576, episode reward: -8.222, mean reward: -1.028 [-5.000, 2.000], mean action: 16.375 [8.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1120/10000: episode: 140, duration: 0.015s, episode steps: 8, steps per second: 532, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 11.125 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1128/10000: episode: 141, duration: 0.023s, episode steps: 8, steps per second: 349, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 12.750 [3.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1136/10000: episode: 142, duration: 0.021s, episode steps: 8, steps per second: 383, episode reward: 5.667, mean reward: 0.708 [-5.000, 3.000], mean action: 11.500 [4.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1144/10000: episode: 143, duration: 0.021s, episode steps: 8, steps per second: 382, episode reward: -2.083, mean reward: -0.260 [-5.000, 2.083], mean action: 15.000 [9.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1152/10000: episode: 144, duration: 0.017s, episode steps: 8, steps per second: 471, episode reward: 2.667, mean reward: 0.333 [-5.000, 1.667], mean action: 16.375 [3.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1160/10000: episode: 145, duration: 0.016s, episode steps: 8, steps per second: 510, episode reward: -2.250, mean reward: -0.281 [-5.000, 2.250], mean action: 13.750 [2.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1168/10000: episode: 146, duration: 0.017s, episode steps: 8, steps per second: 475, episode reward: 6.028, mean reward: 0.753 [-5.000, 2.250], mean action: 9.250 [2.000, 16.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1176/10000: episode: 147, duration: 0.018s, episode steps: 8, steps per second: 455, episode reward: -86.000, mean reward: -10.750 [-100.000, 3.000], mean action: 9.750 [1.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1184/10000: episode: 148, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: 15.200, mean reward: 1.900 [1.000, 3.267], mean action: 12.000 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1192/10000: episode: 149, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 11.500 [1.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1200/10000: episode: 150, duration: 0.015s, episode steps: 8, steps per second: 523, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 13.875 [2.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1208/10000: episode: 151, duration: 0.036s, episode steps: 8, steps per second: 224, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 15.875 [10.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: 16.662500\n",
      " 1216/10000: episode: 152, duration: 0.028s, episode steps: 8, steps per second: 285, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 15.000 [7.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1224/10000: episode: 153, duration: 0.026s, episode steps: 8, steps per second: 303, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 16.875 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1232/10000: episode: 154, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 15.000 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 1240/10000: episode: 155, duration: 0.024s, episode steps: 8, steps per second: 332, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 11.000 [0.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1248/10000: episode: 156, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: -103.000, mean reward: -12.875 [-100.000, 2.000], mean action: 17.750 [2.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 1256/10000: episode: 157, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: 12.667, mean reward: 1.583 [1.000, 2.667], mean action: 11.750 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1264/10000: episode: 158, duration: 0.027s, episode steps: 8, steps per second: 298, episode reward: -2.583, mean reward: -0.323 [-5.000, 2.083], mean action: 12.250 [3.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1272/10000: episode: 159, duration: 0.031s, episode steps: 8, steps per second: 259, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 8.750 [3.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1280/10000: episode: 160, duration: 0.018s, episode steps: 8, steps per second: 445, episode reward: -192.583, mean reward: -24.073 [-100.000, 2.083], mean action: 14.500 [5.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1288/10000: episode: 161, duration: 0.018s, episode steps: 8, steps per second: 447, episode reward: 13.583, mean reward: 1.698 [1.000, 3.000], mean action: 8.500 [0.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1296/10000: episode: 162, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: 11.833, mean reward: 1.479 [1.000, 2.667], mean action: 9.625 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1304/10000: episode: 163, duration: 0.016s, episode steps: 8, steps per second: 491, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 11.375 [1.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1312/10000: episode: 164, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: 8.083, mean reward: 1.010 [-5.000, 3.000], mean action: 14.250 [5.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1320/10000: episode: 165, duration: 0.025s, episode steps: 8, steps per second: 315, episode reward: 14.556, mean reward: 1.819 [1.000, 3.000], mean action: 9.125 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1328/10000: episode: 166, duration: 0.016s, episode steps: 8, steps per second: 509, episode reward: 13.600, mean reward: 1.700 [1.000, 3.000], mean action: 13.000 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1336/10000: episode: 167, duration: 0.020s, episode steps: 8, steps per second: 407, episode reward: -84.833, mean reward: -10.604 [-100.000, 3.125], mean action: 15.750 [8.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1344/10000: episode: 168, duration: 0.020s, episode steps: 8, steps per second: 390, episode reward: 16.250, mean reward: 2.031 [1.000, 4.000], mean action: 13.375 [5.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1352/10000: episode: 169, duration: 0.019s, episode steps: 8, steps per second: 419, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 16.750 [2.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1360/10000: episode: 170, duration: 0.019s, episode steps: 8, steps per second: 432, episode reward: 14.583, mean reward: 1.823 [1.000, 2.667], mean action: 11.875 [3.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1368/10000: episode: 171, duration: 0.016s, episode steps: 8, steps per second: 495, episode reward: -1.000, mean reward: -0.125 [-5.000, 3.000], mean action: 14.875 [10.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1376/10000: episode: 172, duration: 0.016s, episode steps: 8, steps per second: 485, episode reward: -0.333, mean reward: -0.042 [-5.000, 3.000], mean action: 12.000 [1.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1384/10000: episode: 173, duration: 0.015s, episode steps: 8, steps per second: 523, episode reward: -98.000, mean reward: -12.250 [-100.000, 1.500], mean action: 13.875 [3.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1392/10000: episode: 174, duration: 0.020s, episode steps: 8, steps per second: 408, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 13.875 [5.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1400/10000: episode: 175, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 12.750 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1408/10000: episode: 176, duration: 0.020s, episode steps: 8, steps per second: 403, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 12.500 [0.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 1416/10000: episode: 177, duration: 0.015s, episode steps: 8, steps per second: 545, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 14.125 [1.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 1424/10000: episode: 178, duration: 0.023s, episode steps: 8, steps per second: 345, episode reward: -0.750, mean reward: -0.094 [-5.000, 2.250], mean action: 17.375 [8.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1432/10000: episode: 179, duration: 0.016s, episode steps: 8, steps per second: 492, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 13.500 [2.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1440/10000: episode: 180, duration: 0.023s, episode steps: 8, steps per second: 345, episode reward: 1.583, mean reward: 0.198 [-5.000, 2.667], mean action: 14.875 [10.000, 21.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1448/10000: episode: 181, duration: 0.021s, episode steps: 8, steps per second: 372, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 8.000 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1456/10000: episode: 182, duration: 0.019s, episode steps: 8, steps per second: 420, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 14.000 [2.000, 18.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 1464/10000: episode: 183, duration: 0.018s, episode steps: 8, steps per second: 445, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 12.500 [0.000, 21.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1472/10000: episode: 184, duration: 0.021s, episode steps: 8, steps per second: 377, episode reward: 10.556, mean reward: 1.319 [1.000, 2.778], mean action: 12.250 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1480/10000: episode: 185, duration: 0.017s, episode steps: 8, steps per second: 481, episode reward: -294.000, mean reward: -36.750 [-100.000, 1.500], mean action: 16.625 [2.000, 25.000], mean observation: 0.068 [0.000, 1.000], mean_best_reward: --\n",
      " 1488/10000: episode: 186, duration: 0.019s, episode steps: 8, steps per second: 422, episode reward: -8.222, mean reward: -1.028 [-5.000, 2.000], mean action: 10.125 [3.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1496/10000: episode: 187, duration: 0.017s, episode steps: 8, steps per second: 472, episode reward: -96.000, mean reward: -12.000 [-100.000, 2.000], mean action: 11.000 [4.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 1504/10000: episode: 188, duration: 0.018s, episode steps: 8, steps per second: 456, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 16.750 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 1512/10000: episode: 189, duration: 0.020s, episode steps: 8, steps per second: 395, episode reward: 13.056, mean reward: 1.632 [1.000, 3.000], mean action: 14.000 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1520/10000: episode: 190, duration: 0.016s, episode steps: 8, steps per second: 516, episode reward: 13.722, mean reward: 1.715 [1.000, 2.778], mean action: 11.875 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1528/10000: episode: 191, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 9.375 [0.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 1536/10000: episode: 192, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 13.375 [2.000, 22.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 1544/10000: episode: 193, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: 13.694, mean reward: 1.712 [1.000, 2.778], mean action: 12.875 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1552/10000: episode: 194, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 10.750 [2.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1560/10000: episode: 195, duration: 0.014s, episode steps: 8, steps per second: 586, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.875 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1568/10000: episode: 196, duration: 0.013s, episode steps: 8, steps per second: 603, episode reward: 13.925, mean reward: 1.741 [1.000, 3.125], mean action: 14.500 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1576/10000: episode: 197, duration: 0.016s, episode steps: 8, steps per second: 498, episode reward: 3.500, mean reward: 0.438 [-5.000, 1.500], mean action: 10.750 [2.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1584/10000: episode: 198, duration: 0.015s, episode steps: 8, steps per second: 534, episode reward: -97.875, mean reward: -12.234 [-100.000, 1.562], mean action: 13.250 [0.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 1592/10000: episode: 199, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 13.250 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1600/10000: episode: 200, duration: 0.015s, episode steps: 8, steps per second: 541, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 13.000 [2.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1608/10000: episode: 201, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: 12.000, mean reward: 1.500 [1.000, 2.000], mean action: 8.625 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: 13.129167\n",
      " 1616/10000: episode: 202, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: 18.778, mean reward: 2.347 [1.000, 4.000], mean action: 20.500 [14.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1624/10000: episode: 203, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: -198.500, mean reward: -24.812 [-100.000, 2.000], mean action: 15.000 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1632/10000: episode: 204, duration: 0.012s, episode steps: 8, steps per second: 667, episode reward: 4.167, mean reward: 0.521 [-5.000, 1.667], mean action: 12.000 [0.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1640/10000: episode: 205, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: 10.917, mean reward: 1.365 [1.000, 2.667], mean action: 13.750 [1.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1648/10000: episode: 206, duration: 0.013s, episode steps: 8, steps per second: 619, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 9.125 [3.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1656/10000: episode: 207, duration: 0.014s, episode steps: 8, steps per second: 578, episode reward: -205.000, mean reward: -25.625 [-100.000, 2.000], mean action: 15.500 [3.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 1664/10000: episode: 208, duration: 0.012s, episode steps: 8, steps per second: 651, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 11.375 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1672/10000: episode: 209, duration: 0.014s, episode steps: 8, steps per second: 567, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 13.125 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1680/10000: episode: 210, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 15.125 [10.000, 21.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 1688/10000: episode: 211, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 6.000, mean reward: 0.750 [-5.000, 3.000], mean action: 17.875 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1696/10000: episode: 212, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 11.375 [1.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1704/10000: episode: 213, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 11.300, mean reward: 1.413 [1.000, 2.400], mean action: 9.500 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1712/10000: episode: 214, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: 9.000, mean reward: 1.125 [1.000, 1.500], mean action: 8.625 [1.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1720/10000: episode: 215, duration: 0.015s, episode steps: 8, steps per second: 545, episode reward: -0.500, mean reward: -0.062 [-5.000, 2.250], mean action: 9.875 [2.000, 17.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 1728/10000: episode: 216, duration: 0.014s, episode steps: 8, steps per second: 570, episode reward: -3.333, mean reward: -0.417 [-5.000, 1.333], mean action: 6.750 [0.000, 13.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 1736/10000: episode: 217, duration: 0.013s, episode steps: 8, steps per second: 620, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 16.125 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1744/10000: episode: 218, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: 12.500, mean reward: 1.562 [1.000, 2.500], mean action: 10.250 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1752/10000: episode: 219, duration: 0.015s, episode steps: 8, steps per second: 551, episode reward: 12.500, mean reward: 1.562 [1.000, 2.250], mean action: 12.375 [4.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1760/10000: episode: 220, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: -88.000, mean reward: -11.000 [-100.000, 3.000], mean action: 9.375 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1768/10000: episode: 221, duration: 0.012s, episode steps: 8, steps per second: 652, episode reward: 4.056, mean reward: 0.507 [-5.000, 1.778], mean action: 15.750 [0.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1776/10000: episode: 222, duration: 0.015s, episode steps: 8, steps per second: 540, episode reward: 13.500, mean reward: 1.688 [1.000, 2.250], mean action: 11.625 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1784/10000: episode: 223, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.250], mean action: 11.500 [6.000, 18.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 1792/10000: episode: 224, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: 0.500, mean reward: 0.062 [-5.000, 2.250], mean action: 13.250 [4.000, 19.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 1800/10000: episode: 225, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: -6.167, mean reward: -0.771 [-5.000, 2.667], mean action: 5.250 [1.000, 12.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 1808/10000: episode: 226, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: 4.167, mean reward: 0.521 [-5.000, 1.667], mean action: 14.125 [0.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1816/10000: episode: 227, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: -90.944, mean reward: -11.368 [-100.000, 1.778], mean action: 14.000 [1.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1824/10000: episode: 228, duration: 0.014s, episode steps: 8, steps per second: 586, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 14.000 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1832/10000: episode: 229, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 12.500 [2.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1840/10000: episode: 230, duration: 0.012s, episode steps: 8, steps per second: 661, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 11.750 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1848/10000: episode: 231, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 11.750 [3.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1856/10000: episode: 232, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: -193.167, mean reward: -24.146 [-100.000, 1.500], mean action: 11.250 [1.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 1864/10000: episode: 233, duration: 0.013s, episode steps: 8, steps per second: 633, episode reward: 12.667, mean reward: 1.583 [1.000, 3.000], mean action: 10.375 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1872/10000: episode: 234, duration: 0.014s, episode steps: 8, steps per second: 587, episode reward: 12.650, mean reward: 1.581 [1.000, 2.450], mean action: 14.125 [8.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1880/10000: episode: 235, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 10.875 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1888/10000: episode: 236, duration: 0.011s, episode steps: 8, steps per second: 700, episode reward: 14.083, mean reward: 1.760 [1.000, 2.667], mean action: 13.750 [8.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1896/10000: episode: 237, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: -86.500, mean reward: -10.812 [-100.000, 3.125], mean action: 13.625 [2.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 1904/10000: episode: 238, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 14.625 [3.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1912/10000: episode: 239, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 7.625 [0.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 1920/10000: episode: 240, duration: 0.014s, episode steps: 8, steps per second: 589, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 11.375 [0.000, 19.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 1928/10000: episode: 241, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 12.500 [4.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1936/10000: episode: 242, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: -8.000, mean reward: -1.000 [-5.000, 2.000], mean action: 16.250 [2.000, 24.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 1944/10000: episode: 243, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 7.375 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 1952/10000: episode: 244, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: -95.167, mean reward: -11.896 [-100.000, 2.667], mean action: 13.375 [4.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 1960/10000: episode: 245, duration: 0.012s, episode steps: 8, steps per second: 673, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 12.875 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1968/10000: episode: 246, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 12.500 [3.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 1976/10000: episode: 247, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: -90.000, mean reward: -11.250 [-100.000, 3.000], mean action: 10.125 [2.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 1984/10000: episode: 248, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 12.500 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 1992/10000: episode: 249, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: 11.000, mean reward: 1.375 [1.000, 2.250], mean action: 11.000 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2000/10000: episode: 250, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 15.450, mean reward: 1.931 [1.000, 3.600], mean action: 12.625 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2008/10000: episode: 251, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: 14.583, mean reward: 1.823 [1.000, 3.000], mean action: 12.250 [2.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: 15.166667\n",
      " 2016/10000: episode: 252, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: 5.440, mean reward: 0.680 [-5.000, 1.800], mean action: 9.875 [3.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2024/10000: episode: 253, duration: 0.013s, episode steps: 8, steps per second: 633, episode reward: 18.111, mean reward: 2.264 [1.000, 4.083], mean action: 15.125 [10.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2032/10000: episode: 254, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 11.750 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2040/10000: episode: 255, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 16.625 [8.000, 23.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 2048/10000: episode: 256, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 13.625 [2.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 2056/10000: episode: 257, duration: 0.015s, episode steps: 8, steps per second: 551, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 10.875 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2064/10000: episode: 258, duration: 0.013s, episode steps: 8, steps per second: 599, episode reward: 6.556, mean reward: 0.819 [-5.000, 2.778], mean action: 16.625 [2.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2072/10000: episode: 259, duration: 0.013s, episode steps: 8, steps per second: 621, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 12.625 [3.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 2080/10000: episode: 260, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: 10.500, mean reward: 1.312 [1.000, 2.250], mean action: 14.500 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2088/10000: episode: 261, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 9.375 [7.000, 13.000], mean observation: 0.055 [0.000, 1.000], mean_best_reward: --\n",
      " 2096/10000: episode: 262, duration: 0.012s, episode steps: 8, steps per second: 677, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 14.250 [5.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2104/10000: episode: 263, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 6.661, mean reward: 0.833 [-5.000, 2.400], mean action: 13.375 [3.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2112/10000: episode: 264, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 9.125 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2120/10000: episode: 265, duration: 0.012s, episode steps: 8, steps per second: 687, episode reward: -1.722, mean reward: -0.215 [-5.000, 2.000], mean action: 9.250 [2.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2128/10000: episode: 266, duration: 0.015s, episode steps: 8, steps per second: 533, episode reward: -8.000, mean reward: -1.000 [-5.000, 2.000], mean action: 13.000 [1.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2136/10000: episode: 267, duration: 0.012s, episode steps: 8, steps per second: 659, episode reward: -2.833, mean reward: -0.354 [-5.000, 1.500], mean action: 12.125 [5.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 2144/10000: episode: 268, duration: 0.013s, episode steps: 8, steps per second: 640, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 10.750 [0.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2152/10000: episode: 269, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: -97.500, mean reward: -12.188 [-100.000, 2.000], mean action: 16.375 [3.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 2160/10000: episode: 270, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: 10.667, mean reward: 1.333 [1.000, 2.083], mean action: 11.125 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2168/10000: episode: 271, duration: 0.012s, episode steps: 8, steps per second: 687, episode reward: -14.500, mean reward: -1.812 [-5.000, 2.000], mean action: 11.125 [7.000, 14.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2176/10000: episode: 272, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: 10.778, mean reward: 1.347 [1.000, 2.000], mean action: 11.625 [4.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2184/10000: episode: 273, duration: 0.015s, episode steps: 8, steps per second: 547, episode reward: 12.806, mean reward: 1.601 [1.000, 2.250], mean action: 13.125 [5.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2192/10000: episode: 274, duration: 0.013s, episode steps: 8, steps per second: 640, episode reward: -199.000, mean reward: -24.875 [-100.000, 2.000], mean action: 11.875 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 2200/10000: episode: 275, duration: 0.012s, episode steps: 8, steps per second: 680, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 11.500 [2.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2208/10000: episode: 276, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 18.083, mean reward: 2.260 [1.000, 4.167], mean action: 8.750 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2216/10000: episode: 277, duration: 0.014s, episode steps: 8, steps per second: 561, episode reward: -3.667, mean reward: -0.458 [-5.000, 1.333], mean action: 14.000 [8.000, 19.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 2224/10000: episode: 278, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: 8.750, mean reward: 1.094 [-5.000, 3.125], mean action: 9.250 [1.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2232/10000: episode: 279, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 9.000, mean reward: 1.125 [1.000, 1.500], mean action: 13.625 [3.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2240/10000: episode: 280, duration: 0.012s, episode steps: 8, steps per second: 691, episode reward: 0.775, mean reward: 0.097 [-5.000, 3.125], mean action: 10.375 [2.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2248/10000: episode: 281, duration: 0.014s, episode steps: 8, steps per second: 555, episode reward: 0.833, mean reward: 0.104 [-5.000, 2.778], mean action: 7.000 [2.000, 22.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2256/10000: episode: 282, duration: 0.013s, episode steps: 8, steps per second: 612, episode reward: 6.375, mean reward: 0.797 [-5.000, 3.125], mean action: 18.500 [3.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2264/10000: episode: 283, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.750 [6.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2272/10000: episode: 284, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 13.625 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2280/10000: episode: 285, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: 6.333, mean reward: 0.792 [-5.000, 2.778], mean action: 12.500 [1.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2288/10000: episode: 286, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 13.875 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2296/10000: episode: 287, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 15.500 [5.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2304/10000: episode: 288, duration: 0.014s, episode steps: 8, steps per second: 561, episode reward: 5.000, mean reward: 0.625 [-5.000, 3.000], mean action: 7.000 [0.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2312/10000: episode: 289, duration: 0.011s, episode steps: 8, steps per second: 696, episode reward: 11.806, mean reward: 1.476 [1.000, 2.250], mean action: 12.125 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2320/10000: episode: 290, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 9.875 [0.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2328/10000: episode: 291, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: 3.250, mean reward: 0.406 [-5.000, 2.250], mean action: 10.875 [2.000, 19.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2336/10000: episode: 292, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: 16.667, mean reward: 2.083 [1.000, 3.000], mean action: 15.375 [6.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2344/10000: episode: 293, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: 11.028, mean reward: 1.378 [1.000, 2.250], mean action: 9.000 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2352/10000: episode: 294, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: 12.000, mean reward: 1.500 [1.000, 2.000], mean action: 9.000 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2360/10000: episode: 295, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: 4.083, mean reward: 0.510 [-5.000, 2.083], mean action: 11.875 [4.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2368/10000: episode: 296, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 12.125 [2.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2376/10000: episode: 297, duration: 0.012s, episode steps: 8, steps per second: 653, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 12.750 [3.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2384/10000: episode: 298, duration: 0.015s, episode steps: 8, steps per second: 548, episode reward: 5.392, mean reward: 0.674 [-5.000, 3.600], mean action: 11.125 [3.000, 16.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2392/10000: episode: 299, duration: 0.015s, episode steps: 8, steps per second: 534, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 7.875 [1.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2400/10000: episode: 300, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: -103.500, mean reward: -12.938 [-100.000, 2.000], mean action: 12.375 [4.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2408/10000: episode: 301, duration: 0.013s, episode steps: 8, steps per second: 629, episode reward: -98.000, mean reward: -12.250 [-100.000, 2.000], mean action: 11.125 [2.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: 16.680556\n",
      " 2416/10000: episode: 302, duration: 0.014s, episode steps: 8, steps per second: 586, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 11.000 [1.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2424/10000: episode: 303, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 9.875 [0.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2432/10000: episode: 304, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 14.583, mean reward: 1.823 [1.000, 3.000], mean action: 8.375 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2440/10000: episode: 305, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 11.875 [4.000, 23.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2448/10000: episode: 306, duration: 0.012s, episode steps: 8, steps per second: 648, episode reward: 3.333, mean reward: 0.417 [-5.000, 2.000], mean action: 10.250 [0.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2456/10000: episode: 307, duration: 0.020s, episode steps: 8, steps per second: 405, episode reward: -98.222, mean reward: -12.278 [-100.000, 1.778], mean action: 9.125 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 2464/10000: episode: 308, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: -109.722, mean reward: -13.715 [-100.000, 1.778], mean action: 12.500 [7.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 2472/10000: episode: 309, duration: 0.012s, episode steps: 8, steps per second: 652, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 17.750 [3.000, 24.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2480/10000: episode: 310, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 10.375 [2.000, 18.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2488/10000: episode: 311, duration: 0.017s, episode steps: 8, steps per second: 483, episode reward: 7.833, mean reward: 0.979 [-5.000, 2.667], mean action: 15.625 [4.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2496/10000: episode: 312, duration: 0.012s, episode steps: 8, steps per second: 666, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 11.750 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2504/10000: episode: 313, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 12.383, mean reward: 1.548 [1.000, 2.400], mean action: 10.375 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2512/10000: episode: 314, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -6.500, mean reward: -0.812 [-5.000, 2.250], mean action: 8.375 [0.000, 19.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 2520/10000: episode: 315, duration: 0.018s, episode steps: 8, steps per second: 437, episode reward: 3.667, mean reward: 0.458 [-5.000, 2.000], mean action: 10.000 [2.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2528/10000: episode: 316, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: 10.500, mean reward: 1.312 [-5.000, 4.167], mean action: 12.625 [1.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2536/10000: episode: 317, duration: 0.014s, episode steps: 8, steps per second: 581, episode reward: 8.417, mean reward: 1.052 [-5.000, 3.125], mean action: 10.875 [7.000, 20.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2544/10000: episode: 318, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: -89.500, mean reward: -11.188 [-100.000, 2.000], mean action: 12.125 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 2552/10000: episode: 319, duration: 0.014s, episode steps: 8, steps per second: 588, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 11.250 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 2560/10000: episode: 320, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: -295.000, mean reward: -36.875 [-100.000, 1.000], mean action: 17.750 [0.000, 25.000], mean observation: 0.062 [0.000, 1.000], mean_best_reward: --\n",
      " 2568/10000: episode: 321, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 8.500 [2.000, 22.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 2576/10000: episode: 322, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: -90.500, mean reward: -11.312 [-100.000, 2.000], mean action: 12.375 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2584/10000: episode: 323, duration: 0.015s, episode steps: 8, steps per second: 533, episode reward: -96.000, mean reward: -12.000 [-100.000, 2.000], mean action: 13.000 [0.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 2592/10000: episode: 324, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 15.250 [8.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 2600/10000: episode: 325, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 12.875 [3.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2608/10000: episode: 326, duration: 0.012s, episode steps: 8, steps per second: 689, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 10.625 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2616/10000: episode: 327, duration: 0.012s, episode steps: 8, steps per second: 673, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 13.125 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2624/10000: episode: 328, duration: 0.015s, episode steps: 8, steps per second: 540, episode reward: 3.250, mean reward: 0.406 [-5.000, 2.250], mean action: 10.375 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2632/10000: episode: 329, duration: 0.012s, episode steps: 8, steps per second: 677, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 16.875 [4.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2640/10000: episode: 330, duration: 0.015s, episode steps: 8, steps per second: 537, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 10.750 [0.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2648/10000: episode: 331, duration: 0.017s, episode steps: 8, steps per second: 458, episode reward: 14.750, mean reward: 1.844 [1.000, 3.125], mean action: 16.750 [5.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2656/10000: episode: 332, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 13.875 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2664/10000: episode: 333, duration: 0.013s, episode steps: 8, steps per second: 611, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 13.625 [2.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2672/10000: episode: 334, duration: 0.016s, episode steps: 8, steps per second: 485, episode reward: 13.583, mean reward: 1.698 [1.000, 2.667], mean action: 12.000 [1.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2680/10000: episode: 335, duration: 0.015s, episode steps: 8, steps per second: 533, episode reward: 5.833, mean reward: 0.729 [-5.000, 2.667], mean action: 10.625 [2.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 2688/10000: episode: 336, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 12.125 [1.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2696/10000: episode: 337, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: -193.000, mean reward: -24.125 [-100.000, 2.000], mean action: 18.000 [7.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 2704/10000: episode: 338, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: -192.000, mean reward: -24.000 [-100.000, 2.000], mean action: 15.750 [2.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 2712/10000: episode: 339, duration: 0.023s, episode steps: 8, steps per second: 346, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 14.500 [1.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2720/10000: episode: 340, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: -2.722, mean reward: -0.340 [-5.000, 1.778], mean action: 16.500 [4.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2728/10000: episode: 341, duration: 0.025s, episode steps: 8, steps per second: 315, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 9.500 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2736/10000: episode: 342, duration: 0.014s, episode steps: 8, steps per second: 589, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 12.875 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2744/10000: episode: 343, duration: 0.014s, episode steps: 8, steps per second: 565, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 12.500 [5.000, 17.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 2752/10000: episode: 344, duration: 0.022s, episode steps: 8, steps per second: 364, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 10.250 [0.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2760/10000: episode: 345, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 11.500 [4.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2768/10000: episode: 346, duration: 0.018s, episode steps: 8, steps per second: 457, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 11.250 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2776/10000: episode: 347, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 11.000 [0.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 2784/10000: episode: 348, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 12.625 [5.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2792/10000: episode: 349, duration: 0.032s, episode steps: 8, steps per second: 247, episode reward: 9.556, mean reward: 1.194 [1.000, 1.778], mean action: 9.375 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2800/10000: episode: 350, duration: 0.023s, episode steps: 8, steps per second: 348, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 12.500 [1.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2808/10000: episode: 351, duration: 0.025s, episode steps: 8, steps per second: 318, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 10.625 [0.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: 14.652778\n",
      " 2816/10000: episode: 352, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 16.625 [10.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 2824/10000: episode: 353, duration: 0.027s, episode steps: 8, steps per second: 296, episode reward: -2.444, mean reward: -0.306 [-5.000, 1.778], mean action: 14.875 [4.000, 20.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 2832/10000: episode: 354, duration: 0.020s, episode steps: 8, steps per second: 392, episode reward: 12.333, mean reward: 1.542 [1.000, 2.250], mean action: 9.750 [0.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2840/10000: episode: 355, duration: 0.031s, episode steps: 8, steps per second: 258, episode reward: -92.000, mean reward: -11.500 [-100.000, 1.500], mean action: 14.250 [5.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2848/10000: episode: 356, duration: 0.042s, episode steps: 8, steps per second: 188, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 11.375 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2856/10000: episode: 357, duration: 0.024s, episode steps: 8, steps per second: 336, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 12.125 [3.000, 25.000], mean observation: 0.068 [0.000, 1.000], mean_best_reward: --\n",
      " 2864/10000: episode: 358, duration: 0.021s, episode steps: 8, steps per second: 386, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 15.625 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2872/10000: episode: 359, duration: 0.026s, episode steps: 8, steps per second: 303, episode reward: 12.833, mean reward: 1.604 [1.000, 2.667], mean action: 14.250 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2880/10000: episode: 360, duration: 0.022s, episode steps: 8, steps per second: 371, episode reward: 9.467, mean reward: 1.183 [-5.000, 3.000], mean action: 12.875 [7.000, 20.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2888/10000: episode: 361, duration: 0.022s, episode steps: 8, steps per second: 364, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 13.125 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 2896/10000: episode: 362, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: -2.000, mean reward: -0.250 [-5.000, 3.000], mean action: 13.875 [4.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2904/10000: episode: 363, duration: 0.020s, episode steps: 8, steps per second: 407, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 8.625 [0.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2912/10000: episode: 364, duration: 0.023s, episode steps: 8, steps per second: 349, episode reward: -89.000, mean reward: -11.125 [-100.000, 3.000], mean action: 10.000 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 2920/10000: episode: 365, duration: 0.016s, episode steps: 8, steps per second: 495, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 10.125 [2.000, 19.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 2928/10000: episode: 366, duration: 0.018s, episode steps: 8, steps per second: 435, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 15.500 [0.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 2936/10000: episode: 367, duration: 0.027s, episode steps: 8, steps per second: 299, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 11.875 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 2944/10000: episode: 368, duration: 0.016s, episode steps: 8, steps per second: 494, episode reward: 14.000, mean reward: 1.750 [1.000, 3.000], mean action: 10.250 [1.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2952/10000: episode: 369, duration: 0.025s, episode steps: 8, steps per second: 320, episode reward: 12.333, mean reward: 1.542 [1.000, 2.778], mean action: 12.625 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2960/10000: episode: 370, duration: 0.015s, episode steps: 8, steps per second: 542, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 13.250 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2968/10000: episode: 371, duration: 0.018s, episode steps: 8, steps per second: 453, episode reward: 15.375, mean reward: 1.922 [1.000, 3.125], mean action: 12.375 [3.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 2976/10000: episode: 372, duration: 0.020s, episode steps: 8, steps per second: 409, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.250 [4.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2984/10000: episode: 373, duration: 0.016s, episode steps: 8, steps per second: 490, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 10.000 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 2992/10000: episode: 374, duration: 0.015s, episode steps: 8, steps per second: 518, episode reward: -90.500, mean reward: -11.312 [-100.000, 2.083], mean action: 8.250 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3000/10000: episode: 375, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 10.000 [1.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3008/10000: episode: 376, duration: 0.012s, episode steps: 8, steps per second: 647, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 17.875 [5.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 3016/10000: episode: 377, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: -8.000, mean reward: -1.000 [-5.000, 2.000], mean action: 9.875 [0.000, 21.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3024/10000: episode: 378, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 12.500 [3.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3032/10000: episode: 379, duration: 0.017s, episode steps: 8, steps per second: 484, episode reward: -90.000, mean reward: -11.250 [-100.000, 3.000], mean action: 15.375 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3040/10000: episode: 380, duration: 0.017s, episode steps: 8, steps per second: 478, episode reward: 14.083, mean reward: 1.760 [1.000, 2.667], mean action: 11.875 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3048/10000: episode: 381, duration: 0.013s, episode steps: 8, steps per second: 628, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 13.500 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3056/10000: episode: 382, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 11.375 [1.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3064/10000: episode: 383, duration: 0.017s, episode steps: 8, steps per second: 475, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 9.375 [0.000, 20.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3072/10000: episode: 384, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: -98.000, mean reward: -12.250 [-100.000, 2.000], mean action: 10.875 [0.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 3080/10000: episode: 385, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 16.000 [5.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3088/10000: episode: 386, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 12.125 [5.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3096/10000: episode: 387, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 9.375 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3104/10000: episode: 388, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 14.125 [3.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3112/10000: episode: 389, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: 14.450, mean reward: 1.806 [1.000, 3.267], mean action: 11.875 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3120/10000: episode: 390, duration: 0.015s, episode steps: 8, steps per second: 521, episode reward: -89.167, mean reward: -11.146 [-100.000, 2.667], mean action: 15.000 [5.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3128/10000: episode: 391, duration: 0.016s, episode steps: 8, steps per second: 514, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 14.375 [6.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3136/10000: episode: 392, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 12.875 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 3144/10000: episode: 393, duration: 0.013s, episode steps: 8, steps per second: 611, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 11.375 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3152/10000: episode: 394, duration: 0.014s, episode steps: 8, steps per second: 588, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 16.125 [8.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 3160/10000: episode: 395, duration: 0.015s, episode steps: 8, steps per second: 530, episode reward: -87.417, mean reward: -10.927 [-100.000, 2.667], mean action: 13.875 [3.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3168/10000: episode: 396, duration: 0.016s, episode steps: 8, steps per second: 515, episode reward: -7.750, mean reward: -0.969 [-5.000, 2.250], mean action: 10.500 [0.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3176/10000: episode: 397, duration: 0.020s, episode steps: 8, steps per second: 401, episode reward: -198.500, mean reward: -24.812 [-100.000, 2.000], mean action: 11.250 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3184/10000: episode: 398, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 10.375 [1.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3192/10000: episode: 399, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 10.500 [2.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3200/10000: episode: 400, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 10.125 [4.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3208/10000: episode: 401, duration: 0.015s, episode steps: 8, steps per second: 538, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 12.375 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: 15.850000\n",
      " 3216/10000: episode: 402, duration: 0.012s, episode steps: 8, steps per second: 676, episode reward: -205.000, mean reward: -25.625 [-100.000, 2.000], mean action: 14.375 [2.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 3224/10000: episode: 403, duration: 0.015s, episode steps: 8, steps per second: 530, episode reward: 15.917, mean reward: 1.990 [1.000, 3.000], mean action: 16.250 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3232/10000: episode: 404, duration: 0.015s, episode steps: 8, steps per second: 547, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 14.625 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3240/10000: episode: 405, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: 8.583, mean reward: 1.073 [-5.000, 2.667], mean action: 16.125 [4.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3248/10000: episode: 406, duration: 0.012s, episode steps: 8, steps per second: 660, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 13.625 [2.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3256/10000: episode: 407, duration: 0.012s, episode steps: 8, steps per second: 652, episode reward: -15.000, mean reward: -1.875 [-5.000, 2.000], mean action: 11.750 [1.000, 23.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 3264/10000: episode: 408, duration: 0.015s, episode steps: 8, steps per second: 551, episode reward: -88.750, mean reward: -11.094 [-100.000, 2.250], mean action: 13.125 [0.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3272/10000: episode: 409, duration: 0.015s, episode steps: 8, steps per second: 538, episode reward: 12.556, mean reward: 1.569 [1.000, 2.778], mean action: 11.125 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3280/10000: episode: 410, duration: 0.015s, episode steps: 8, steps per second: 551, episode reward: -3.222, mean reward: -0.403 [-5.000, 1.778], mean action: 10.875 [6.000, 18.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 3288/10000: episode: 411, duration: 0.014s, episode steps: 8, steps per second: 583, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 8.625 [0.000, 21.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 3296/10000: episode: 412, duration: 0.015s, episode steps: 8, steps per second: 517, episode reward: 14.083, mean reward: 1.760 [1.000, 2.667], mean action: 13.375 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3304/10000: episode: 413, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.083], mean action: 14.375 [9.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3312/10000: episode: 414, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: 12.056, mean reward: 1.507 [1.000, 2.000], mean action: 10.000 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3320/10000: episode: 415, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: 12.000, mean reward: 1.500 [1.000, 2.000], mean action: 10.500 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3328/10000: episode: 416, duration: 0.015s, episode steps: 8, steps per second: 532, episode reward: 13.333, mean reward: 1.667 [1.000, 2.778], mean action: 12.875 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3336/10000: episode: 417, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 15.000 [1.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3344/10000: episode: 418, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 13.375 [5.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3352/10000: episode: 419, duration: 0.016s, episode steps: 8, steps per second: 495, episode reward: 15.472, mean reward: 1.934 [1.000, 2.778], mean action: 14.750 [5.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3360/10000: episode: 420, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 10.125 [1.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 3368/10000: episode: 421, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: -0.944, mean reward: -0.118 [-5.000, 2.778], mean action: 8.125 [0.000, 14.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3376/10000: episode: 422, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: 6.667, mean reward: 0.833 [-5.000, 3.000], mean action: 15.250 [5.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3384/10000: episode: 423, duration: 0.015s, episode steps: 8, steps per second: 539, episode reward: 11.000, mean reward: 1.375 [1.000, 3.000], mean action: 13.375 [5.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3392/10000: episode: 424, duration: 0.015s, episode steps: 8, steps per second: 544, episode reward: 11.833, mean reward: 1.479 [1.000, 2.250], mean action: 13.500 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3400/10000: episode: 425, duration: 0.019s, episode steps: 8, steps per second: 427, episode reward: -90.750, mean reward: -11.344 [-100.000, 2.250], mean action: 16.750 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3408/10000: episode: 426, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: -88.000, mean reward: -11.000 [-100.000, 3.000], mean action: 15.000 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3416/10000: episode: 427, duration: 0.014s, episode steps: 8, steps per second: 578, episode reward: 14.792, mean reward: 1.849 [1.000, 3.062], mean action: 15.500 [7.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3424/10000: episode: 428, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: 12.333, mean reward: 1.542 [1.000, 2.778], mean action: 14.750 [7.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3432/10000: episode: 429, duration: 0.014s, episode steps: 8, steps per second: 586, episode reward: 9.833, mean reward: 1.229 [1.000, 2.000], mean action: 12.500 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3440/10000: episode: 430, duration: 0.012s, episode steps: 8, steps per second: 681, episode reward: -87.750, mean reward: -10.969 [-100.000, 3.125], mean action: 12.250 [4.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3448/10000: episode: 431, duration: 0.011s, episode steps: 8, steps per second: 703, episode reward: 10.167, mean reward: 1.271 [-5.000, 3.125], mean action: 15.125 [2.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3456/10000: episode: 432, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: 14.000, mean reward: 1.750 [1.000, 3.000], mean action: 12.375 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3464/10000: episode: 433, duration: 0.014s, episode steps: 8, steps per second: 581, episode reward: 5.062, mean reward: 0.633 [-5.000, 2.250], mean action: 8.625 [0.000, 18.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3472/10000: episode: 434, duration: 0.015s, episode steps: 8, steps per second: 526, episode reward: 4.917, mean reward: 0.615 [-5.000, 2.083], mean action: 15.250 [7.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3480/10000: episode: 435, duration: 0.016s, episode steps: 8, steps per second: 492, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 11.125 [0.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3488/10000: episode: 436, duration: 0.013s, episode steps: 8, steps per second: 610, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 10.125 [2.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3496/10000: episode: 437, duration: 0.012s, episode steps: 8, steps per second: 673, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 12.375 [1.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3504/10000: episode: 438, duration: 0.012s, episode steps: 8, steps per second: 678, episode reward: -3.104, mean reward: -0.388 [-5.000, 1.562], mean action: 7.750 [0.000, 16.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3512/10000: episode: 439, duration: 0.015s, episode steps: 8, steps per second: 536, episode reward: -1.833, mean reward: -0.229 [-5.000, 2.083], mean action: 14.500 [9.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3520/10000: episode: 440, duration: 0.015s, episode steps: 8, steps per second: 541, episode reward: -0.250, mean reward: -0.031 [-5.000, 2.667], mean action: 14.375 [6.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3528/10000: episode: 441, duration: 0.012s, episode steps: 8, steps per second: 667, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.000], mean action: 10.500 [0.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3536/10000: episode: 442, duration: 0.015s, episode steps: 8, steps per second: 536, episode reward: 8.250, mean reward: 1.031 [-5.000, 3.125], mean action: 9.750 [0.000, 17.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3544/10000: episode: 443, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: 0.000, mean reward: 0.000 [-5.000, 3.000], mean action: 11.750 [1.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3552/10000: episode: 444, duration: 0.011s, episode steps: 8, steps per second: 697, episode reward: -0.917, mean reward: -0.115 [-5.000, 2.083], mean action: 12.125 [3.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3560/10000: episode: 445, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 13.375 [3.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3568/10000: episode: 446, duration: 0.015s, episode steps: 8, steps per second: 517, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 13.125 [5.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 3576/10000: episode: 447, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: 13.056, mean reward: 1.632 [1.000, 2.778], mean action: 10.125 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3584/10000: episode: 448, duration: 0.014s, episode steps: 8, steps per second: 583, episode reward: 16.000, mean reward: 2.000 [1.000, 5.000], mean action: 17.000 [6.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3592/10000: episode: 449, duration: 0.015s, episode steps: 8, steps per second: 547, episode reward: 14.250, mean reward: 1.781 [1.000, 4.000], mean action: 14.625 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3600/10000: episode: 450, duration: 0.015s, episode steps: 8, steps per second: 521, episode reward: 12.200, mean reward: 1.525 [1.000, 2.450], mean action: 7.875 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3608/10000: episode: 451, duration: 0.023s, episode steps: 8, steps per second: 352, episode reward: -191.750, mean reward: -23.969 [-100.000, 2.250], mean action: 14.625 [5.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: 17.375000\n",
      " 3616/10000: episode: 452, duration: 0.018s, episode steps: 8, steps per second: 445, episode reward: 14.633, mean reward: 1.829 [1.000, 2.450], mean action: 8.625 [0.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3624/10000: episode: 453, duration: 0.019s, episode steps: 8, steps per second: 429, episode reward: 0.333, mean reward: 0.042 [-5.000, 2.778], mean action: 7.875 [2.000, 12.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 3632/10000: episode: 454, duration: 0.016s, episode steps: 8, steps per second: 506, episode reward: -1.667, mean reward: -0.208 [-5.000, 2.000], mean action: 12.000 [2.000, 24.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 3640/10000: episode: 455, duration: 0.013s, episode steps: 8, steps per second: 620, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 17.250 [10.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 3648/10000: episode: 456, duration: 0.015s, episode steps: 8, steps per second: 523, episode reward: -5.917, mean reward: -0.740 [-5.000, 3.000], mean action: 6.250 [2.000, 16.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3656/10000: episode: 457, duration: 0.015s, episode steps: 8, steps per second: 522, episode reward: 11.444, mean reward: 1.431 [1.000, 2.083], mean action: 13.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3664/10000: episode: 458, duration: 0.014s, episode steps: 8, steps per second: 570, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 11.625 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3672/10000: episode: 459, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: 19.375, mean reward: 2.422 [1.000, 4.500], mean action: 11.625 [3.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3680/10000: episode: 460, duration: 0.017s, episode steps: 8, steps per second: 460, episode reward: -102.833, mean reward: -12.854 [-100.000, 2.667], mean action: 10.500 [1.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3688/10000: episode: 461, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: -199.000, mean reward: -24.875 [-100.000, 2.000], mean action: 15.625 [6.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3696/10000: episode: 462, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: 20.000, mean reward: 2.500 [1.000, 4.000], mean action: 13.250 [5.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3704/10000: episode: 463, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 13.750 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3712/10000: episode: 464, duration: 0.018s, episode steps: 8, steps per second: 441, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 8.500 [3.000, 17.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3720/10000: episode: 465, duration: 0.016s, episode steps: 8, steps per second: 495, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 12.000 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3728/10000: episode: 466, duration: 0.019s, episode steps: 8, steps per second: 420, episode reward: 13.861, mean reward: 1.733 [1.000, 3.000], mean action: 8.750 [3.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3736/10000: episode: 467, duration: 0.021s, episode steps: 8, steps per second: 375, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 10.125 [0.000, 17.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3744/10000: episode: 468, duration: 0.018s, episode steps: 8, steps per second: 457, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 9.125 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3752/10000: episode: 469, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: -88.750, mean reward: -11.094 [-100.000, 2.250], mean action: 12.375 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3760/10000: episode: 470, duration: 0.027s, episode steps: 8, steps per second: 301, episode reward: 12.333, mean reward: 1.542 [1.000, 2.778], mean action: 10.000 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3768/10000: episode: 471, duration: 0.022s, episode steps: 8, steps per second: 368, episode reward: -110.500, mean reward: -13.812 [-100.000, 1.500], mean action: 15.125 [8.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 3776/10000: episode: 472, duration: 0.026s, episode steps: 8, steps per second: 303, episode reward: 2.042, mean reward: 0.255 [-5.000, 3.125], mean action: 7.375 [0.000, 18.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3784/10000: episode: 473, duration: 0.023s, episode steps: 8, steps per second: 350, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 15.250 [2.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3792/10000: episode: 474, duration: 0.013s, episode steps: 8, steps per second: 595, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 8.000 [0.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3800/10000: episode: 475, duration: 0.017s, episode steps: 8, steps per second: 464, episode reward: -90.500, mean reward: -11.312 [-100.000, 2.000], mean action: 13.125 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3808/10000: episode: 476, duration: 0.013s, episode steps: 8, steps per second: 607, episode reward: -90.833, mean reward: -11.354 [-100.000, 1.667], mean action: 12.375 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3816/10000: episode: 477, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 14.250 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3824/10000: episode: 478, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 9.278, mean reward: 1.160 [1.000, 1.778], mean action: 11.875 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3832/10000: episode: 479, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: -7.833, mean reward: -0.979 [-5.000, 2.667], mean action: 11.750 [2.000, 17.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3840/10000: episode: 480, duration: 0.017s, episode steps: 8, steps per second: 468, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 15.250 [3.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3848/10000: episode: 481, duration: 0.014s, episode steps: 8, steps per second: 583, episode reward: 8.333, mean reward: 1.042 [-5.000, 3.000], mean action: 8.875 [0.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3856/10000: episode: 482, duration: 0.012s, episode steps: 8, steps per second: 660, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 11.000 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3864/10000: episode: 483, duration: 0.014s, episode steps: 8, steps per second: 587, episode reward: -189.167, mean reward: -23.646 [-100.000, 2.667], mean action: 16.875 [3.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 3872/10000: episode: 484, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: -89.500, mean reward: -11.188 [-100.000, 2.250], mean action: 16.500 [1.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 3880/10000: episode: 485, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 12.500 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 3888/10000: episode: 486, duration: 0.013s, episode steps: 8, steps per second: 621, episode reward: -7.750, mean reward: -0.969 [-5.000, 2.250], mean action: 15.375 [3.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 3896/10000: episode: 487, duration: 0.017s, episode steps: 8, steps per second: 471, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 14.250 [0.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 3904/10000: episode: 488, duration: 0.014s, episode steps: 8, steps per second: 553, episode reward: 10.500, mean reward: 1.312 [1.000, 2.083], mean action: 14.000 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3912/10000: episode: 489, duration: 0.013s, episode steps: 8, steps per second: 629, episode reward: -94.500, mean reward: -11.812 [-100.000, 2.250], mean action: 13.250 [3.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 3920/10000: episode: 490, duration: 0.013s, episode steps: 8, steps per second: 634, episode reward: -0.083, mean reward: -0.010 [-5.000, 2.667], mean action: 15.125 [5.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 3928/10000: episode: 491, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: 4.056, mean reward: 0.507 [-5.000, 1.778], mean action: 14.375 [7.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3936/10000: episode: 492, duration: 0.016s, episode steps: 8, steps per second: 508, episode reward: 10.278, mean reward: 1.285 [1.000, 2.000], mean action: 11.500 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3944/10000: episode: 493, duration: 0.015s, episode steps: 8, steps per second: 518, episode reward: 6.250, mean reward: 0.781 [-5.000, 2.250], mean action: 11.750 [1.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3952/10000: episode: 494, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 14.750 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3960/10000: episode: 495, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: 14.583, mean reward: 1.823 [1.000, 3.000], mean action: 10.500 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 3968/10000: episode: 496, duration: 0.020s, episode steps: 8, steps per second: 404, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 13.500 [1.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 3976/10000: episode: 497, duration: 0.014s, episode steps: 8, steps per second: 564, episode reward: -88.000, mean reward: -11.000 [-100.000, 3.000], mean action: 14.250 [2.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 3984/10000: episode: 498, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: -86.667, mean reward: -10.833 [-100.000, 3.000], mean action: 16.375 [4.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 3992/10000: episode: 499, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: -89.167, mean reward: -11.146 [-100.000, 2.667], mean action: 11.500 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4000/10000: episode: 500, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 15.000 [0.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 4008/10000: episode: 501, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 13.000 [1.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: 15.027778\n",
      " 4016/10000: episode: 502, duration: 0.012s, episode steps: 8, steps per second: 662, episode reward: 12.583, mean reward: 1.573 [1.000, 2.667], mean action: 14.000 [4.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4024/10000: episode: 503, duration: 0.012s, episode steps: 8, steps per second: 687, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 15.500 [1.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 4032/10000: episode: 504, duration: 0.014s, episode steps: 8, steps per second: 553, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 10.375 [4.000, 17.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4040/10000: episode: 505, duration: 0.015s, episode steps: 8, steps per second: 547, episode reward: 10.778, mean reward: 1.347 [1.000, 2.000], mean action: 9.250 [4.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4048/10000: episode: 506, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 11.875 [0.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4056/10000: episode: 507, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: -187.500, mean reward: -23.438 [-100.000, 3.125], mean action: 16.500 [7.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 4064/10000: episode: 508, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 10.750 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4072/10000: episode: 509, duration: 0.014s, episode steps: 8, steps per second: 574, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 13.875 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4080/10000: episode: 510, duration: 0.012s, episode steps: 8, steps per second: 645, episode reward: 9.000, mean reward: 1.125 [1.000, 1.500], mean action: 13.000 [4.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4088/10000: episode: 511, duration: 0.012s, episode steps: 8, steps per second: 647, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 12.375 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4096/10000: episode: 512, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: 12.500, mean reward: 1.562 [1.000, 2.250], mean action: 10.125 [1.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4104/10000: episode: 513, duration: 0.013s, episode steps: 8, steps per second: 610, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 8.375 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4112/10000: episode: 514, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: -1.000, mean reward: -0.125 [-5.000, 3.000], mean action: 15.250 [1.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4120/10000: episode: 515, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: -190.500, mean reward: -23.812 [-100.000, 2.250], mean action: 15.375 [1.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 4128/10000: episode: 516, duration: 0.013s, episode steps: 8, steps per second: 597, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 10.750 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4136/10000: episode: 517, duration: 0.013s, episode steps: 8, steps per second: 640, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 9.250 [2.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4144/10000: episode: 518, duration: 0.012s, episode steps: 8, steps per second: 668, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 12.625 [2.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 4152/10000: episode: 519, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 12.250 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4160/10000: episode: 520, duration: 0.016s, episode steps: 8, steps per second: 499, episode reward: -186.583, mean reward: -23.323 [-100.000, 4.167], mean action: 15.250 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4168/10000: episode: 521, duration: 0.014s, episode steps: 8, steps per second: 578, episode reward: 13.000, mean reward: 1.625 [1.000, 3.000], mean action: 8.500 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4176/10000: episode: 522, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: -83.222, mean reward: -10.403 [-100.000, 4.000], mean action: 12.875 [7.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4184/10000: episode: 523, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: -3.667, mean reward: -0.458 [-5.000, 1.333], mean action: 14.875 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4192/10000: episode: 524, duration: 0.013s, episode steps: 8, steps per second: 632, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 16.750 [12.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 4200/10000: episode: 525, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: 2.778, mean reward: 0.347 [-5.000, 1.778], mean action: 12.125 [5.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4208/10000: episode: 526, duration: 0.014s, episode steps: 8, steps per second: 558, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 13.750 [3.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 4216/10000: episode: 527, duration: 0.012s, episode steps: 8, steps per second: 659, episode reward: -0.222, mean reward: -0.028 [-5.000, 3.000], mean action: 10.000 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4224/10000: episode: 528, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 11.000 [0.000, 21.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4232/10000: episode: 529, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.625 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4240/10000: episode: 530, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: 12.778, mean reward: 1.597 [1.000, 3.000], mean action: 9.625 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4248/10000: episode: 531, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: 11.250, mean reward: 1.406 [1.000, 2.250], mean action: 11.125 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4256/10000: episode: 532, duration: 0.013s, episode steps: 8, steps per second: 627, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 12.625 [0.000, 24.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4264/10000: episode: 533, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.250], mean action: 8.250 [1.000, 16.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4272/10000: episode: 534, duration: 0.015s, episode steps: 8, steps per second: 537, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.250], mean action: 11.875 [1.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 4280/10000: episode: 535, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: -190.000, mean reward: -23.750 [-100.000, 3.000], mean action: 16.875 [9.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4288/10000: episode: 536, duration: 0.015s, episode steps: 8, steps per second: 548, episode reward: 14.042, mean reward: 1.755 [1.000, 3.125], mean action: 14.375 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4296/10000: episode: 537, duration: 0.013s, episode steps: 8, steps per second: 595, episode reward: 9.903, mean reward: 1.238 [1.000, 1.778], mean action: 9.250 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4304/10000: episode: 538, duration: 0.016s, episode steps: 8, steps per second: 510, episode reward: 13.000, mean reward: 1.625 [1.000, 4.000], mean action: 8.375 [1.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4312/10000: episode: 539, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 13.388, mean reward: 1.673 [1.000, 2.250], mean action: 14.500 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4320/10000: episode: 540, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: 11.806, mean reward: 1.476 [1.000, 2.250], mean action: 13.875 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4328/10000: episode: 541, duration: 0.017s, episode steps: 8, steps per second: 477, episode reward: -87.000, mean reward: -10.875 [-100.000, 3.000], mean action: 15.000 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4336/10000: episode: 542, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 7.583, mean reward: 0.948 [-5.000, 2.667], mean action: 13.125 [6.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4344/10000: episode: 543, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: 9.000, mean reward: 1.125 [1.000, 1.500], mean action: 11.125 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4352/10000: episode: 544, duration: 0.012s, episode steps: 8, steps per second: 668, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 14.250 [2.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 4360/10000: episode: 545, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: 14.500, mean reward: 1.812 [1.000, 3.125], mean action: 6.250 [0.000, 15.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4368/10000: episode: 546, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: -89.833, mean reward: -11.229 [-100.000, 2.667], mean action: 14.250 [6.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 4376/10000: episode: 547, duration: 0.013s, episode steps: 8, steps per second: 595, episode reward: -98.333, mean reward: -12.292 [-100.000, 1.333], mean action: 11.500 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4384/10000: episode: 548, duration: 0.012s, episode steps: 8, steps per second: 647, episode reward: 15.100, mean reward: 1.887 [1.000, 3.600], mean action: 7.125 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4392/10000: episode: 549, duration: 0.013s, episode steps: 8, steps per second: 603, episode reward: 5.417, mean reward: 0.677 [-5.000, 2.250], mean action: 16.375 [1.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4400/10000: episode: 550, duration: 0.014s, episode steps: 8, steps per second: 558, episode reward: 13.750, mean reward: 1.719 [1.000, 3.125], mean action: 17.625 [4.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4408/10000: episode: 551, duration: 0.015s, episode steps: 8, steps per second: 529, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 17.000 [9.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: 14.527778\n",
      " 4416/10000: episode: 552, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 10.278, mean reward: 1.285 [1.000, 2.000], mean action: 10.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4424/10000: episode: 553, duration: 0.011s, episode steps: 8, steps per second: 696, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 10.750 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4432/10000: episode: 554, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 13.875 [6.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4440/10000: episode: 555, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 15.375 [1.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4448/10000: episode: 556, duration: 0.015s, episode steps: 8, steps per second: 524, episode reward: 3.583, mean reward: 0.448 [-5.000, 2.083], mean action: 11.875 [3.000, 19.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4456/10000: episode: 557, duration: 0.012s, episode steps: 8, steps per second: 695, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 6.000 [1.000, 17.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4464/10000: episode: 558, duration: 0.013s, episode steps: 8, steps per second: 612, episode reward: 11.250, mean reward: 1.406 [1.000, 2.250], mean action: 11.750 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4472/10000: episode: 559, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 4.556, mean reward: 0.569 [-5.000, 2.000], mean action: 13.375 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4480/10000: episode: 560, duration: 0.012s, episode steps: 8, steps per second: 668, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 8.750 [0.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4488/10000: episode: 561, duration: 0.012s, episode steps: 8, steps per second: 676, episode reward: -87.500, mean reward: -10.938 [-100.000, 3.000], mean action: 11.875 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4496/10000: episode: 562, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: -89.167, mean reward: -11.146 [-100.000, 2.667], mean action: 10.875 [2.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4504/10000: episode: 563, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: 4.361, mean reward: 0.545 [-5.000, 2.083], mean action: 7.750 [1.000, 20.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4512/10000: episode: 564, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: 6.833, mean reward: 0.854 [-5.000, 2.667], mean action: 14.125 [7.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 4520/10000: episode: 565, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 8.750 [0.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 4528/10000: episode: 566, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: 12.000, mean reward: 1.500 [1.000, 2.000], mean action: 13.250 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4536/10000: episode: 567, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: -102.417, mean reward: -12.802 [-100.000, 2.083], mean action: 10.875 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4544/10000: episode: 568, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: -189.833, mean reward: -23.729 [-100.000, 3.000], mean action: 13.500 [7.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4552/10000: episode: 569, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 15.375 [5.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4560/10000: episode: 570, duration: 0.013s, episode steps: 8, steps per second: 599, episode reward: 9.250, mean reward: 1.156 [1.000, 2.250], mean action: 13.625 [4.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4568/10000: episode: 571, duration: 0.013s, episode steps: 8, steps per second: 634, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 13.500 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4576/10000: episode: 572, duration: 0.012s, episode steps: 8, steps per second: 662, episode reward: -101.000, mean reward: -12.625 [-100.000, 3.000], mean action: 9.500 [2.000, 25.000], mean observation: 0.068 [0.000, 1.000], mean_best_reward: --\n",
      " 4584/10000: episode: 573, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: 13.250, mean reward: 1.656 [1.000, 3.000], mean action: 13.500 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4592/10000: episode: 574, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: -8.000, mean reward: -1.000 [-5.000, 2.000], mean action: 7.375 [1.000, 12.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 4600/10000: episode: 575, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 12.000, mean reward: 1.500 [1.000, 2.250], mean action: 14.250 [2.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4608/10000: episode: 576, duration: 0.012s, episode steps: 8, steps per second: 654, episode reward: 11.100, mean reward: 1.387 [1.000, 2.000], mean action: 9.125 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4616/10000: episode: 577, duration: 0.011s, episode steps: 8, steps per second: 712, episode reward: 0.833, mean reward: 0.104 [-5.000, 2.667], mean action: 7.250 [2.000, 18.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4624/10000: episode: 578, duration: 0.015s, episode steps: 8, steps per second: 544, episode reward: 12.800, mean reward: 1.600 [1.000, 3.000], mean action: 13.750 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4632/10000: episode: 579, duration: 0.013s, episode steps: 8, steps per second: 629, episode reward: -89.500, mean reward: -11.188 [-100.000, 2.250], mean action: 11.750 [2.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4640/10000: episode: 580, duration: 0.012s, episode steps: 8, steps per second: 661, episode reward: -87.000, mean reward: -10.875 [-100.000, 3.000], mean action: 19.000 [6.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4648/10000: episode: 581, duration: 0.012s, episode steps: 8, steps per second: 692, episode reward: -92.000, mean reward: -11.500 [-100.000, 1.500], mean action: 16.250 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4656/10000: episode: 582, duration: 0.012s, episode steps: 8, steps per second: 661, episode reward: -87.000, mean reward: -10.875 [-100.000, 3.000], mean action: 11.750 [3.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4664/10000: episode: 583, duration: 0.015s, episode steps: 8, steps per second: 518, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 11.750 [4.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4672/10000: episode: 584, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 10.500 [5.000, 19.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4680/10000: episode: 585, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 3.458, mean reward: 0.432 [-5.000, 1.562], mean action: 17.125 [8.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4688/10000: episode: 586, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.000], mean action: 10.375 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4696/10000: episode: 587, duration: 0.012s, episode steps: 8, steps per second: 674, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 11.125 [1.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4704/10000: episode: 588, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: 12.000, mean reward: 1.500 [1.000, 2.250], mean action: 12.500 [3.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4712/10000: episode: 589, duration: 0.012s, episode steps: 8, steps per second: 667, episode reward: 9.778, mean reward: 1.222 [1.000, 2.000], mean action: 9.125 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4720/10000: episode: 590, duration: 0.013s, episode steps: 8, steps per second: 627, episode reward: 7.333, mean reward: 0.917 [-5.000, 2.667], mean action: 10.625 [2.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4728/10000: episode: 591, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: 12.500, mean reward: 1.562 [1.000, 2.250], mean action: 10.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4736/10000: episode: 592, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 12.500 [1.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4744/10000: episode: 593, duration: 0.012s, episode steps: 8, steps per second: 683, episode reward: -87.278, mean reward: -10.910 [-100.000, 2.778], mean action: 11.625 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4752/10000: episode: 594, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 12.500 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4760/10000: episode: 595, duration: 0.014s, episode steps: 8, steps per second: 587, episode reward: 10.806, mean reward: 1.351 [1.000, 2.250], mean action: 8.375 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4768/10000: episode: 596, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: -197.000, mean reward: -24.625 [-100.000, 3.000], mean action: 16.375 [12.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4776/10000: episode: 597, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: 1.625, mean reward: 0.203 [-5.000, 3.125], mean action: 11.375 [2.000, 20.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4784/10000: episode: 598, duration: 0.011s, episode steps: 8, steps per second: 700, episode reward: 7.722, mean reward: 0.965 [-5.000, 2.778], mean action: 16.625 [10.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4792/10000: episode: 599, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 13.125 [2.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4800/10000: episode: 600, duration: 0.016s, episode steps: 8, steps per second: 490, episode reward: 20.250, mean reward: 2.531 [1.000, 4.500], mean action: 11.375 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4808/10000: episode: 601, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 12.000 [0.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: 13.458333\n",
      " 4816/10000: episode: 602, duration: 0.012s, episode steps: 8, steps per second: 666, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 14.625 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 4824/10000: episode: 603, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 10.625 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 4832/10000: episode: 604, duration: 0.015s, episode steps: 8, steps per second: 539, episode reward: 8.500, mean reward: 1.062 [1.000, 1.250], mean action: 12.500 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4840/10000: episode: 605, duration: 0.012s, episode steps: 8, steps per second: 693, episode reward: -197.750, mean reward: -24.719 [-100.000, 2.250], mean action: 16.000 [7.000, 25.000], mean observation: 0.058 [0.000, 1.000], mean_best_reward: --\n",
      " 4848/10000: episode: 606, duration: 0.012s, episode steps: 8, steps per second: 695, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 10.375 [0.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4856/10000: episode: 607, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 13.875 [6.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4864/10000: episode: 608, duration: 0.011s, episode steps: 8, steps per second: 698, episode reward: 5.125, mean reward: 0.641 [-5.000, 3.125], mean action: 12.000 [5.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 4872/10000: episode: 609, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 11.500, mean reward: 1.438 [1.000, 2.000], mean action: 12.875 [7.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4880/10000: episode: 610, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: -88.500, mean reward: -11.062 [-100.000, 2.250], mean action: 11.500 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 4888/10000: episode: 611, duration: 0.011s, episode steps: 8, steps per second: 697, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 13.750 [0.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 4896/10000: episode: 612, duration: 0.012s, episode steps: 8, steps per second: 657, episode reward: -91.500, mean reward: -11.438 [-100.000, 1.500], mean action: 18.750 [10.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 4904/10000: episode: 613, duration: 0.014s, episode steps: 8, steps per second: 576, episode reward: 12.458, mean reward: 1.557 [1.000, 3.062], mean action: 10.500 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4912/10000: episode: 614, duration: 0.014s, episode steps: 8, steps per second: 581, episode reward: 13.500, mean reward: 1.688 [1.000, 2.250], mean action: 12.875 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4920/10000: episode: 615, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 11.500 [0.000, 23.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 4928/10000: episode: 616, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: 13.000, mean reward: 1.625 [1.000, 3.000], mean action: 10.875 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4936/10000: episode: 617, duration: 0.012s, episode steps: 8, steps per second: 674, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.125 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4944/10000: episode: 618, duration: 0.012s, episode steps: 8, steps per second: 690, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 11.500 [1.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 4952/10000: episode: 619, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: 7.917, mean reward: 0.990 [-5.000, 2.667], mean action: 13.125 [6.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4960/10000: episode: 620, duration: 0.013s, episode steps: 8, steps per second: 597, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 9.500 [0.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4968/10000: episode: 621, duration: 0.012s, episode steps: 8, steps per second: 677, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 13.500 [4.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 4976/10000: episode: 622, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: -189.417, mean reward: -23.677 [-100.000, 3.000], mean action: 11.750 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 4984/10000: episode: 623, duration: 0.015s, episode steps: 8, steps per second: 541, episode reward: 11.753, mean reward: 1.469 [1.000, 2.000], mean action: 13.500 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 4992/10000: episode: 624, duration: 0.012s, episode steps: 8, steps per second: 689, episode reward: -2.750, mean reward: -0.344 [-5.000, 2.250], mean action: 12.375 [4.000, 19.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5000/10000: episode: 625, duration: 0.013s, episode steps: 8, steps per second: 620, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 10.000 [0.000, 21.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5008/10000: episode: 626, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: -91.750, mean reward: -11.469 [-100.000, 2.250], mean action: 12.750 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 5016/10000: episode: 627, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: -292.000, mean reward: -36.500 [-100.000, 3.000], mean action: 17.250 [1.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5024/10000: episode: 628, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: -206.000, mean reward: -25.750 [-100.000, 1.000], mean action: 14.375 [2.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5032/10000: episode: 629, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 13.875 [2.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5040/10000: episode: 630, duration: 0.017s, episode steps: 8, steps per second: 481, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 12.375 [2.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5048/10000: episode: 631, duration: 0.014s, episode steps: 8, steps per second: 561, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 16.000 [4.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5056/10000: episode: 632, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: 12.500, mean reward: 1.562 [1.000, 2.250], mean action: 11.875 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5064/10000: episode: 633, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 13.500 [2.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5072/10000: episode: 634, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: -191.333, mean reward: -23.917 [-100.000, 2.000], mean action: 11.250 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5080/10000: episode: 635, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: 13.500, mean reward: 1.688 [1.000, 3.000], mean action: 12.500 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5088/10000: episode: 636, duration: 0.014s, episode steps: 8, steps per second: 572, episode reward: -90.944, mean reward: -11.368 [-100.000, 1.778], mean action: 17.375 [10.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5096/10000: episode: 637, duration: 0.011s, episode steps: 8, steps per second: 706, episode reward: -205.500, mean reward: -25.688 [-100.000, 1.500], mean action: 18.750 [11.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 5104/10000: episode: 638, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 10.750 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5112/10000: episode: 639, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.250], mean action: 12.125 [2.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5120/10000: episode: 640, duration: 0.012s, episode steps: 8, steps per second: 686, episode reward: 2.250, mean reward: 0.281 [-5.000, 1.250], mean action: 10.125 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 5128/10000: episode: 641, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: 11.833, mean reward: 1.479 [1.000, 2.667], mean action: 13.750 [4.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5136/10000: episode: 642, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 14.250 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5144/10000: episode: 643, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: -89.833, mean reward: -11.229 [-100.000, 2.667], mean action: 12.750 [3.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 5152/10000: episode: 644, duration: 0.012s, episode steps: 8, steps per second: 693, episode reward: 13.000, mean reward: 1.625 [1.000, 3.000], mean action: 11.750 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5160/10000: episode: 645, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: 6.333, mean reward: 0.792 [-5.000, 2.667], mean action: 10.875 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5168/10000: episode: 646, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 6.750, mean reward: 0.844 [-5.000, 2.250], mean action: 10.000 [5.000, 14.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5176/10000: episode: 647, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 9.750 [2.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5184/10000: episode: 648, duration: 0.015s, episode steps: 8, steps per second: 542, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 7.375 [1.000, 15.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5192/10000: episode: 649, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: 17.033, mean reward: 2.129 [1.000, 3.267], mean action: 11.625 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5200/10000: episode: 650, duration: 0.012s, episode steps: 8, steps per second: 678, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 15.875 [3.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5208/10000: episode: 651, duration: 0.014s, episode steps: 8, steps per second: 553, episode reward: 16.556, mean reward: 2.069 [1.000, 4.000], mean action: 10.375 [3.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: 17.041667\n",
      " 5216/10000: episode: 652, duration: 0.011s, episode steps: 8, steps per second: 737, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 14.750 [5.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5224/10000: episode: 653, duration: 0.011s, episode steps: 8, steps per second: 703, episode reward: -86.550, mean reward: -10.819 [-100.000, 3.600], mean action: 12.250 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5232/10000: episode: 654, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: 5.944, mean reward: 0.743 [-5.000, 2.083], mean action: 10.625 [2.000, 19.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5240/10000: episode: 655, duration: 0.011s, episode steps: 8, steps per second: 718, episode reward: -95.556, mean reward: -11.944 [-100.000, 2.083], mean action: 18.250 [11.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5248/10000: episode: 656, duration: 0.016s, episode steps: 8, steps per second: 509, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.250], mean action: 12.500 [3.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5256/10000: episode: 657, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 8.875 [4.000, 18.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5264/10000: episode: 658, duration: 0.015s, episode steps: 8, steps per second: 526, episode reward: 3.625, mean reward: 0.453 [-5.000, 1.562], mean action: 10.375 [2.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5272/10000: episode: 659, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: 6.500, mean reward: 0.812 [-5.000, 2.250], mean action: 12.375 [2.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5280/10000: episode: 660, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: -0.194, mean reward: -0.024 [-5.000, 2.250], mean action: 15.500 [8.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5288/10000: episode: 661, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: 17.311, mean reward: 2.164 [1.000, 3.267], mean action: 10.000 [1.000, 16.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5296/10000: episode: 662, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: 14.278, mean reward: 1.785 [1.000, 3.000], mean action: 8.875 [0.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5304/10000: episode: 663, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 14.625 [3.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 5312/10000: episode: 664, duration: 0.013s, episode steps: 8, steps per second: 638, episode reward: 5.250, mean reward: 0.656 [-5.000, 2.250], mean action: 18.125 [3.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5320/10000: episode: 665, duration: 0.013s, episode steps: 8, steps per second: 632, episode reward: 14.250, mean reward: 1.781 [1.000, 3.125], mean action: 8.875 [1.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5328/10000: episode: 666, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 3.778, mean reward: 0.472 [-5.000, 2.000], mean action: 11.250 [0.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5336/10000: episode: 667, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 12.750 [4.000, 21.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 5344/10000: episode: 668, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: 13.722, mean reward: 1.715 [1.000, 2.778], mean action: 8.375 [0.000, 15.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5352/10000: episode: 669, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.250], mean action: 9.625 [0.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5360/10000: episode: 670, duration: 0.012s, episode steps: 8, steps per second: 665, episode reward: 1.750, mean reward: 0.219 [-5.000, 3.125], mean action: 13.000 [0.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5368/10000: episode: 671, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 8.500 [6.000, 17.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 5376/10000: episode: 672, duration: 0.014s, episode steps: 8, steps per second: 553, episode reward: 10.500, mean reward: 1.312 [1.000, 2.250], mean action: 7.875 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5384/10000: episode: 673, duration: 0.017s, episode steps: 8, steps per second: 470, episode reward: 13.750, mean reward: 1.719 [1.000, 3.125], mean action: 10.250 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5392/10000: episode: 674, duration: 0.012s, episode steps: 8, steps per second: 654, episode reward: 11.000, mean reward: 1.375 [-5.000, 4.167], mean action: 16.875 [4.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 5400/10000: episode: 675, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: -1.722, mean reward: -0.215 [-5.000, 2.000], mean action: 9.125 [4.000, 17.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5408/10000: episode: 676, duration: 0.015s, episode steps: 8, steps per second: 539, episode reward: 4.556, mean reward: 0.569 [-5.000, 2.000], mean action: 16.000 [5.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5416/10000: episode: 677, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 11.125 [1.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5424/10000: episode: 678, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: 11.278, mean reward: 1.410 [1.000, 2.000], mean action: 11.375 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5432/10000: episode: 679, duration: 0.013s, episode steps: 8, steps per second: 638, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 14.125 [0.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 5440/10000: episode: 680, duration: 0.013s, episode steps: 8, steps per second: 632, episode reward: 12.694, mean reward: 1.587 [-5.000, 4.000], mean action: 6.250 [0.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5448/10000: episode: 681, duration: 0.016s, episode steps: 8, steps per second: 506, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 11.750 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5456/10000: episode: 682, duration: 0.011s, episode steps: 8, steps per second: 714, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 15.125 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5464/10000: episode: 683, duration: 0.014s, episode steps: 8, steps per second: 578, episode reward: 14.125, mean reward: 1.766 [1.000, 3.125], mean action: 7.500 [1.000, 15.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5472/10000: episode: 684, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: 13.000, mean reward: 1.625 [1.000, 3.000], mean action: 10.125 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5480/10000: episode: 685, duration: 0.012s, episode steps: 8, steps per second: 681, episode reward: -7.833, mean reward: -0.979 [-5.000, 2.667], mean action: 13.875 [1.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5488/10000: episode: 686, duration: 0.011s, episode steps: 8, steps per second: 717, episode reward: -204.000, mean reward: -25.500 [-100.000, 2.000], mean action: 13.375 [2.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5496/10000: episode: 687, duration: 0.015s, episode steps: 8, steps per second: 520, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 8.375 [1.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5504/10000: episode: 688, duration: 0.012s, episode steps: 8, steps per second: 664, episode reward: -110.500, mean reward: -13.812 [-100.000, 1.500], mean action: 11.750 [3.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 5512/10000: episode: 689, duration: 0.016s, episode steps: 8, steps per second: 500, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 13.500 [0.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5520/10000: episode: 690, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 14.250 [4.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5528/10000: episode: 691, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: 11.500, mean reward: 1.438 [1.000, 2.000], mean action: 11.625 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5536/10000: episode: 692, duration: 0.017s, episode steps: 8, steps per second: 481, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 14.625 [6.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 5544/10000: episode: 693, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: 9.625, mean reward: 1.203 [-5.000, 3.125], mean action: 9.000 [2.000, 18.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5552/10000: episode: 694, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 10.250 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5560/10000: episode: 695, duration: 0.012s, episode steps: 8, steps per second: 673, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 11.500 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5568/10000: episode: 696, duration: 0.012s, episode steps: 8, steps per second: 686, episode reward: 9.556, mean reward: 1.194 [1.000, 1.778], mean action: 12.500 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5576/10000: episode: 697, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: 6.828, mean reward: 0.853 [-5.000, 2.400], mean action: 12.625 [1.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5584/10000: episode: 698, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: 5.000, mean reward: 0.625 [-5.000, 3.000], mean action: 12.000 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5592/10000: episode: 699, duration: 0.012s, episode steps: 8, steps per second: 651, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 10.250 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5600/10000: episode: 700, duration: 0.011s, episode steps: 8, steps per second: 727, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 17.000 [5.000, 22.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5608/10000: episode: 701, duration: 0.015s, episode steps: 8, steps per second: 548, episode reward: -192.000, mean reward: -24.000 [-100.000, 3.000], mean action: 14.750 [1.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: 19.041667\n",
      " 5616/10000: episode: 702, duration: 0.013s, episode steps: 8, steps per second: 640, episode reward: 14.711, mean reward: 1.839 [1.000, 3.267], mean action: 12.875 [5.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5624/10000: episode: 703, duration: 0.012s, episode steps: 8, steps per second: 650, episode reward: 12.583, mean reward: 1.573 [1.000, 2.250], mean action: 11.000 [0.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5632/10000: episode: 704, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 12.125 [1.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5640/10000: episode: 705, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: -0.083, mean reward: -0.010 [-5.000, 2.667], mean action: 12.875 [3.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5648/10000: episode: 706, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 12.625 [5.000, 23.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 5656/10000: episode: 707, duration: 0.016s, episode steps: 8, steps per second: 498, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 7.625 [1.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5664/10000: episode: 708, duration: 0.014s, episode steps: 8, steps per second: 587, episode reward: 11.500, mean reward: 1.438 [1.000, 2.000], mean action: 12.125 [5.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5672/10000: episode: 709, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: 10.650, mean reward: 1.331 [1.000, 2.400], mean action: 9.375 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5680/10000: episode: 710, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.750 [3.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5688/10000: episode: 711, duration: 0.012s, episode steps: 8, steps per second: 661, episode reward: -1.194, mean reward: -0.149 [-5.000, 2.250], mean action: 13.500 [6.000, 20.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 5696/10000: episode: 712, duration: 0.012s, episode steps: 8, steps per second: 640, episode reward: -191.333, mean reward: -23.917 [-100.000, 3.000], mean action: 14.750 [2.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5704/10000: episode: 713, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 12.625 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5712/10000: episode: 714, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: 14.583, mean reward: 1.823 [1.000, 2.667], mean action: 9.250 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5720/10000: episode: 715, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 13.000, mean reward: 1.625 [1.000, 3.000], mean action: 11.625 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5728/10000: episode: 716, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 13.375 [2.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5736/10000: episode: 717, duration: 0.012s, episode steps: 8, steps per second: 672, episode reward: -200.000, mean reward: -25.000 [-100.000, 1.000], mean action: 16.000 [1.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 5744/10000: episode: 718, duration: 0.012s, episode steps: 8, steps per second: 674, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 13.375 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5752/10000: episode: 719, duration: 0.012s, episode steps: 8, steps per second: 689, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 13.500 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 5760/10000: episode: 720, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: -15.000, mean reward: -1.875 [-5.000, 2.000], mean action: 7.125 [3.000, 20.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5768/10000: episode: 721, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.875 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5776/10000: episode: 722, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: 17.250, mean reward: 2.156 [1.000, 4.000], mean action: 10.500 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5784/10000: episode: 723, duration: 0.017s, episode steps: 8, steps per second: 466, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 11.125 [2.000, 21.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 5792/10000: episode: 724, duration: 0.015s, episode steps: 8, steps per second: 545, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 12.250 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5800/10000: episode: 725, duration: 0.011s, episode steps: 8, steps per second: 733, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 17.375 [8.000, 23.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 5808/10000: episode: 726, duration: 0.013s, episode steps: 8, steps per second: 617, episode reward: 3.933, mean reward: 0.492 [-5.000, 1.800], mean action: 11.750 [5.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5816/10000: episode: 727, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 12.750 [2.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 5824/10000: episode: 728, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 8.125 [0.000, 15.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5832/10000: episode: 729, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: -96.333, mean reward: -12.042 [-100.000, 2.083], mean action: 11.500 [2.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5840/10000: episode: 730, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: 12.500, mean reward: 1.562 [1.000, 2.000], mean action: 15.875 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5848/10000: episode: 731, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: -190.167, mean reward: -23.771 [-100.000, 2.667], mean action: 18.500 [10.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 5856/10000: episode: 732, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 14.375 [2.000, 24.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 5864/10000: episode: 733, duration: 0.012s, episode steps: 8, steps per second: 695, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 16.375 [7.000, 23.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 5872/10000: episode: 734, duration: 0.012s, episode steps: 8, steps per second: 667, episode reward: 13.153, mean reward: 1.644 [1.000, 2.250], mean action: 14.000 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5880/10000: episode: 735, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 0.000, mean reward: 0.000 [-5.000, 4.000], mean action: 7.875 [0.000, 20.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5888/10000: episode: 736, duration: 0.018s, episode steps: 8, steps per second: 454, episode reward: -2.583, mean reward: -0.323 [-5.000, 2.083], mean action: 15.000 [2.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 5896/10000: episode: 737, duration: 0.013s, episode steps: 8, steps per second: 617, episode reward: 12.028, mean reward: 1.503 [1.000, 2.250], mean action: 7.875 [2.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5904/10000: episode: 738, duration: 0.015s, episode steps: 8, steps per second: 526, episode reward: -89.000, mean reward: -11.125 [-100.000, 2.000], mean action: 12.375 [1.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5912/10000: episode: 739, duration: 0.015s, episode steps: 8, steps per second: 527, episode reward: 13.306, mean reward: 1.663 [1.000, 2.250], mean action: 11.250 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5920/10000: episode: 740, duration: 0.014s, episode steps: 8, steps per second: 588, episode reward: -89.000, mean reward: -11.125 [-100.000, 3.000], mean action: 13.875 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5928/10000: episode: 741, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 14.050, mean reward: 1.756 [1.000, 2.400], mean action: 14.625 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 5936/10000: episode: 742, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: -103.750, mean reward: -12.969 [-100.000, 2.250], mean action: 8.875 [0.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 5944/10000: episode: 743, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 11.875 [0.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 5952/10000: episode: 744, duration: 0.012s, episode steps: 8, steps per second: 650, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 11.375 [0.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 5960/10000: episode: 745, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 11.500 [2.000, 22.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 5968/10000: episode: 746, duration: 0.014s, episode steps: 8, steps per second: 589, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 15.250 [2.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 5976/10000: episode: 747, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.375 [3.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 5984/10000: episode: 748, duration: 0.016s, episode steps: 8, steps per second: 495, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 10.375 [1.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 5992/10000: episode: 749, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: -7.444, mean reward: -0.931 [-5.000, 2.778], mean action: 7.500 [2.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 6000/10000: episode: 750, duration: 0.014s, episode steps: 8, steps per second: 561, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 7.875 [0.000, 18.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6008/10000: episode: 751, duration: 0.016s, episode steps: 8, steps per second: 514, episode reward: 3.556, mean reward: 0.444 [-5.000, 1.778], mean action: 6.750 [0.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: 14.708333\n",
      " 6016/10000: episode: 752, duration: 0.015s, episode steps: 8, steps per second: 545, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 14.625 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6024/10000: episode: 753, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: 4.556, mean reward: 0.569 [-5.000, 2.000], mean action: 13.625 [4.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6032/10000: episode: 754, duration: 0.012s, episode steps: 8, steps per second: 660, episode reward: 9.556, mean reward: 1.194 [1.000, 1.778], mean action: 9.750 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6040/10000: episode: 755, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: 11.917, mean reward: 1.490 [1.000, 2.667], mean action: 16.125 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6048/10000: episode: 756, duration: 0.020s, episode steps: 8, steps per second: 409, episode reward: 13.483, mean reward: 1.685 [1.000, 2.450], mean action: 11.375 [4.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6056/10000: episode: 757, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: 5.667, mean reward: 0.708 [-5.000, 2.083], mean action: 9.375 [2.000, 18.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 6064/10000: episode: 758, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 14.500 [3.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6072/10000: episode: 759, duration: 0.014s, episode steps: 8, steps per second: 571, episode reward: 13.300, mean reward: 1.663 [1.000, 2.400], mean action: 10.750 [2.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6080/10000: episode: 760, duration: 0.020s, episode steps: 8, steps per second: 398, episode reward: 17.500, mean reward: 2.188 [1.000, 4.000], mean action: 11.375 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6088/10000: episode: 761, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 10.625 [0.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 6096/10000: episode: 762, duration: 0.013s, episode steps: 8, steps per second: 617, episode reward: -101.639, mean reward: -12.705 [-100.000, 2.083], mean action: 12.250 [4.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 6104/10000: episode: 763, duration: 0.015s, episode steps: 8, steps per second: 533, episode reward: 8.500, mean reward: 1.062 [-5.000, 3.125], mean action: 7.750 [1.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6112/10000: episode: 764, duration: 0.012s, episode steps: 8, steps per second: 686, episode reward: 12.000, mean reward: 1.500 [1.000, 3.000], mean action: 10.750 [2.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6120/10000: episode: 765, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: 18.000, mean reward: 2.250 [1.000, 4.000], mean action: 11.625 [3.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6128/10000: episode: 766, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 16.750 [3.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6136/10000: episode: 767, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 17.750 [8.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6144/10000: episode: 768, duration: 0.014s, episode steps: 8, steps per second: 588, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 17.125 [5.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6152/10000: episode: 769, duration: 0.012s, episode steps: 8, steps per second: 663, episode reward: 5.250, mean reward: 0.656 [-5.000, 2.250], mean action: 14.875 [6.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6160/10000: episode: 770, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: 12.383, mean reward: 1.548 [1.000, 2.400], mean action: 12.875 [5.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6168/10000: episode: 771, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: -87.417, mean reward: -10.927 [-100.000, 2.667], mean action: 11.625 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6176/10000: episode: 772, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 11.625 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6184/10000: episode: 773, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 14.625 [0.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6192/10000: episode: 774, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 11.750 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6200/10000: episode: 775, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 12.375 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6208/10000: episode: 776, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -102.000, mean reward: -12.750 [-100.000, 3.000], mean action: 16.500 [8.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 6216/10000: episode: 777, duration: 0.012s, episode steps: 8, steps per second: 657, episode reward: 10.500, mean reward: 1.312 [1.000, 2.250], mean action: 12.125 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6224/10000: episode: 778, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 16.125 [0.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 6232/10000: episode: 779, duration: 0.012s, episode steps: 8, steps per second: 657, episode reward: 14.472, mean reward: 1.809 [1.000, 2.778], mean action: 11.500 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6240/10000: episode: 780, duration: 0.016s, episode steps: 8, steps per second: 509, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 13.375 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 6248/10000: episode: 781, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 15.375 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6256/10000: episode: 782, duration: 0.015s, episode steps: 8, steps per second: 517, episode reward: 7.500, mean reward: 0.938 [-5.000, 3.000], mean action: 10.750 [0.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6264/10000: episode: 783, duration: 0.014s, episode steps: 8, steps per second: 557, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 13.250 [1.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6272/10000: episode: 784, duration: 0.015s, episode steps: 8, steps per second: 551, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 11.875 [4.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6280/10000: episode: 785, duration: 0.013s, episode steps: 8, steps per second: 617, episode reward: 8.583, mean reward: 1.073 [-5.000, 3.000], mean action: 18.375 [12.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6288/10000: episode: 786, duration: 0.017s, episode steps: 8, steps per second: 462, episode reward: -191.000, mean reward: -23.875 [-100.000, 3.000], mean action: 12.875 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6296/10000: episode: 787, duration: 0.016s, episode steps: 8, steps per second: 496, episode reward: -189.222, mean reward: -23.653 [-100.000, 3.000], mean action: 17.250 [1.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6304/10000: episode: 788, duration: 0.015s, episode steps: 8, steps per second: 526, episode reward: 14.000, mean reward: 1.750 [1.000, 3.000], mean action: 12.250 [4.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6312/10000: episode: 789, duration: 0.015s, episode steps: 8, steps per second: 524, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 11.625 [0.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6320/10000: episode: 790, duration: 0.015s, episode steps: 8, steps per second: 530, episode reward: 14.472, mean reward: 1.809 [1.000, 2.778], mean action: 12.125 [2.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6328/10000: episode: 791, duration: 0.012s, episode steps: 8, steps per second: 644, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 16.250 [7.000, 21.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 6336/10000: episode: 792, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: 10.162, mean reward: 1.270 [1.000, 1.800], mean action: 11.750 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6344/10000: episode: 793, duration: 0.012s, episode steps: 8, steps per second: 666, episode reward: -86.417, mean reward: -10.802 [-100.000, 2.778], mean action: 15.250 [5.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6352/10000: episode: 794, duration: 0.015s, episode steps: 8, steps per second: 518, episode reward: 13.083, mean reward: 1.635 [1.000, 2.083], mean action: 15.000 [7.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6360/10000: episode: 795, duration: 0.015s, episode steps: 8, steps per second: 520, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 14.125 [1.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6368/10000: episode: 796, duration: 0.015s, episode steps: 8, steps per second: 542, episode reward: 12.967, mean reward: 1.621 [1.000, 2.400], mean action: 11.500 [4.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6376/10000: episode: 797, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 13.117, mean reward: 1.640 [1.000, 2.450], mean action: 11.375 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6384/10000: episode: 798, duration: 0.018s, episode steps: 8, steps per second: 450, episode reward: 12.333, mean reward: 1.542 [1.000, 2.667], mean action: 8.125 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6392/10000: episode: 799, duration: 0.015s, episode steps: 8, steps per second: 541, episode reward: -88.556, mean reward: -11.069 [-100.000, 2.083], mean action: 11.875 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6400/10000: episode: 800, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: -90.750, mean reward: -11.344 [-100.000, 2.250], mean action: 12.750 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6408/10000: episode: 801, duration: 0.015s, episode steps: 8, steps per second: 532, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 13.250 [2.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: 17.625000\n",
      " 6416/10000: episode: 802, duration: 0.012s, episode steps: 8, steps per second: 678, episode reward: 0.500, mean reward: 0.062 [-5.000, 2.250], mean action: 14.750 [8.000, 23.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 6424/10000: episode: 803, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: 12.750, mean reward: 1.594 [1.000, 2.250], mean action: 11.750 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6432/10000: episode: 804, duration: 0.015s, episode steps: 8, steps per second: 544, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 15.125 [6.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6440/10000: episode: 805, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: 6.500, mean reward: 0.812 [-5.000, 3.000], mean action: 13.750 [2.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6448/10000: episode: 806, duration: 0.015s, episode steps: 8, steps per second: 542, episode reward: 9.750, mean reward: 1.219 [-5.000, 3.000], mean action: 9.125 [0.000, 16.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6456/10000: episode: 807, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 10.875 [1.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6464/10000: episode: 808, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: 14.817, mean reward: 1.852 [1.000, 2.667], mean action: 10.250 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6472/10000: episode: 809, duration: 0.017s, episode steps: 8, steps per second: 478, episode reward: 17.458, mean reward: 2.182 [1.000, 3.062], mean action: 9.125 [1.000, 16.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6480/10000: episode: 810, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: -9.222, mean reward: -1.153 [-5.000, 1.778], mean action: 9.875 [0.000, 17.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 6488/10000: episode: 811, duration: 0.016s, episode steps: 8, steps per second: 510, episode reward: -98.000, mean reward: -12.250 [-100.000, 2.000], mean action: 10.875 [2.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 6496/10000: episode: 812, duration: 0.016s, episode steps: 8, steps per second: 490, episode reward: -2.667, mean reward: -0.333 [-5.000, 2.000], mean action: 15.875 [8.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6504/10000: episode: 813, duration: 0.014s, episode steps: 8, steps per second: 583, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 13.000 [2.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6512/10000: episode: 814, duration: 0.012s, episode steps: 8, steps per second: 672, episode reward: 4.083, mean reward: 0.510 [-5.000, 2.083], mean action: 10.375 [2.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6520/10000: episode: 815, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 11.250 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6528/10000: episode: 816, duration: 0.016s, episode steps: 8, steps per second: 515, episode reward: 4.667, mean reward: 0.583 [-5.000, 2.083], mean action: 13.875 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6536/10000: episode: 817, duration: 0.014s, episode steps: 8, steps per second: 578, episode reward: 6.250, mean reward: 0.781 [-5.000, 2.250], mean action: 17.750 [6.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6544/10000: episode: 818, duration: 0.013s, episode steps: 8, steps per second: 596, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 10.875 [3.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6552/10000: episode: 819, duration: 0.017s, episode steps: 8, steps per second: 474, episode reward: -0.500, mean reward: -0.062 [-5.000, 2.000], mean action: 12.500 [0.000, 23.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 6560/10000: episode: 820, duration: 0.013s, episode steps: 8, steps per second: 620, episode reward: 11.500, mean reward: 1.438 [1.000, 2.000], mean action: 14.125 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6568/10000: episode: 821, duration: 0.014s, episode steps: 8, steps per second: 552, episode reward: 11.667, mean reward: 1.458 [1.000, 2.083], mean action: 9.125 [0.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6576/10000: episode: 822, duration: 0.016s, episode steps: 8, steps per second: 490, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 13.875 [2.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 6584/10000: episode: 823, duration: 0.015s, episode steps: 8, steps per second: 537, episode reward: -87.417, mean reward: -10.927 [-100.000, 2.667], mean action: 15.500 [4.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6592/10000: episode: 824, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 11.056, mean reward: 1.382 [1.000, 2.000], mean action: 11.625 [2.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6600/10000: episode: 825, duration: 0.012s, episode steps: 8, steps per second: 671, episode reward: -8.722, mean reward: -1.090 [-5.000, 1.778], mean action: 13.125 [4.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6608/10000: episode: 826, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: -1.000, mean reward: -0.125 [-5.000, 3.000], mean action: 13.000 [4.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6616/10000: episode: 827, duration: 0.015s, episode steps: 8, steps per second: 534, episode reward: 14.361, mean reward: 1.795 [1.000, 3.000], mean action: 10.875 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6624/10000: episode: 828, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 16.000 [8.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6632/10000: episode: 829, duration: 0.020s, episode steps: 8, steps per second: 404, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 16.250 [5.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6640/10000: episode: 830, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 11.625 [3.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6648/10000: episode: 831, duration: 0.012s, episode steps: 8, steps per second: 665, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 10.750 [5.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6656/10000: episode: 832, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 11.875 [4.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6664/10000: episode: 833, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: -102.833, mean reward: -12.854 [-100.000, 2.667], mean action: 13.000 [7.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 6672/10000: episode: 834, duration: 0.014s, episode steps: 8, steps per second: 570, episode reward: -92.500, mean reward: -11.562 [-100.000, 1.500], mean action: 12.625 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 6680/10000: episode: 835, duration: 0.015s, episode steps: 8, steps per second: 540, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 8.625 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6688/10000: episode: 836, duration: 0.013s, episode steps: 8, steps per second: 638, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.500], mean action: 13.375 [1.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6696/10000: episode: 837, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: -0.333, mean reward: -0.042 [-5.000, 2.667], mean action: 9.500 [2.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6704/10000: episode: 838, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -8.139, mean reward: -1.017 [-5.000, 2.083], mean action: 10.375 [1.000, 16.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 6712/10000: episode: 839, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: -98.000, mean reward: -12.250 [-100.000, 1.500], mean action: 14.875 [1.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 6720/10000: episode: 840, duration: 0.012s, episode steps: 8, steps per second: 687, episode reward: -193.500, mean reward: -24.188 [-100.000, 1.500], mean action: 14.250 [1.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 6728/10000: episode: 841, duration: 0.012s, episode steps: 8, steps per second: 662, episode reward: 11.833, mean reward: 1.479 [1.000, 2.667], mean action: 12.125 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6736/10000: episode: 842, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 13.250 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6744/10000: episode: 843, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: -6.722, mean reward: -0.840 [-5.000, 2.778], mean action: 5.500 [0.000, 11.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6752/10000: episode: 844, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: -189.667, mean reward: -23.708 [-100.000, 2.667], mean action: 17.125 [4.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 6760/10000: episode: 845, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: 13.806, mean reward: 1.726 [1.000, 2.778], mean action: 11.375 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6768/10000: episode: 846, duration: 0.012s, episode steps: 8, steps per second: 682, episode reward: 10.292, mean reward: 1.286 [-5.000, 3.125], mean action: 12.000 [6.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6776/10000: episode: 847, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 11.625 [2.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6784/10000: episode: 848, duration: 0.015s, episode steps: 8, steps per second: 535, episode reward: 11.500, mean reward: 1.438 [1.000, 2.083], mean action: 11.875 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6792/10000: episode: 849, duration: 0.013s, episode steps: 8, steps per second: 632, episode reward: 18.242, mean reward: 2.280 [1.000, 3.600], mean action: 13.375 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6800/10000: episode: 850, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: -191.500, mean reward: -23.938 [-100.000, 2.000], mean action: 15.875 [7.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6808/10000: episode: 851, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: 7.500, mean reward: 0.938 [-5.000, 3.000], mean action: 9.625 [3.000, 19.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: 16.479167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6816/10000: episode: 852, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: -89.972, mean reward: -11.247 [-100.000, 2.250], mean action: 15.500 [3.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6824/10000: episode: 853, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 14.875 [7.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6832/10000: episode: 854, duration: 0.013s, episode steps: 8, steps per second: 603, episode reward: 16.917, mean reward: 2.115 [1.000, 3.125], mean action: 14.625 [6.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6840/10000: episode: 855, duration: 0.012s, episode steps: 8, steps per second: 651, episode reward: -193.000, mean reward: -24.125 [-100.000, 2.000], mean action: 14.125 [0.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 6848/10000: episode: 856, duration: 0.012s, episode steps: 8, steps per second: 689, episode reward: 11.667, mean reward: 1.458 [1.000, 2.083], mean action: 11.750 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6856/10000: episode: 857, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 3.250, mean reward: 0.406 [-5.000, 2.250], mean action: 11.625 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6864/10000: episode: 858, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: -193.500, mean reward: -24.188 [-100.000, 1.500], mean action: 14.750 [3.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6872/10000: episode: 859, duration: 0.012s, episode steps: 8, steps per second: 673, episode reward: 10.056, mean reward: 1.257 [1.000, 1.778], mean action: 12.000 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6880/10000: episode: 860, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: 12.833, mean reward: 1.604 [1.000, 2.250], mean action: 10.750 [3.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6888/10000: episode: 861, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: -87.000, mean reward: -10.875 [-100.000, 3.000], mean action: 10.250 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6896/10000: episode: 862, duration: 0.012s, episode steps: 8, steps per second: 671, episode reward: 11.917, mean reward: 1.490 [1.000, 2.667], mean action: 13.125 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6904/10000: episode: 863, duration: 0.014s, episode steps: 8, steps per second: 567, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 11.250 [0.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 6912/10000: episode: 864, duration: 0.013s, episode steps: 8, steps per second: 628, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 11.750 [0.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 6920/10000: episode: 865, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 8.500, mean reward: 1.062 [1.000, 1.500], mean action: 9.875 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6928/10000: episode: 866, duration: 0.012s, episode steps: 8, steps per second: 648, episode reward: 12.250, mean reward: 1.531 [1.000, 2.250], mean action: 11.500 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6936/10000: episode: 867, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: 8.500, mean reward: 1.062 [1.000, 1.500], mean action: 15.125 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6944/10000: episode: 868, duration: 0.015s, episode steps: 8, steps per second: 544, episode reward: 12.000, mean reward: 1.500 [1.000, 2.250], mean action: 13.125 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 6952/10000: episode: 869, duration: 0.012s, episode steps: 8, steps per second: 670, episode reward: -110.500, mean reward: -13.812 [-100.000, 1.500], mean action: 15.250 [2.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 6960/10000: episode: 870, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 14.000 [7.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 6968/10000: episode: 871, duration: 0.012s, episode steps: 8, steps per second: 690, episode reward: -92.750, mean reward: -11.594 [-100.000, 3.125], mean action: 10.375 [2.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 6976/10000: episode: 872, duration: 0.012s, episode steps: 8, steps per second: 654, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 15.500 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 6984/10000: episode: 873, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: 6.500, mean reward: 0.812 [-5.000, 2.000], mean action: 8.375 [1.000, 18.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 6992/10000: episode: 874, duration: 0.012s, episode steps: 8, steps per second: 642, episode reward: -95.167, mean reward: -11.896 [-100.000, 2.667], mean action: 14.250 [1.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 7000/10000: episode: 875, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: 10.056, mean reward: 1.257 [1.000, 1.778], mean action: 15.500 [9.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7008/10000: episode: 876, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: 6.800, mean reward: 0.850 [-5.000, 2.500], mean action: 5.750 [2.000, 12.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7016/10000: episode: 877, duration: 0.015s, episode steps: 8, steps per second: 537, episode reward: 13.250, mean reward: 1.656 [1.000, 3.125], mean action: 12.750 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7024/10000: episode: 878, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: -7.750, mean reward: -0.969 [-5.000, 2.250], mean action: 13.500 [8.000, 21.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 7032/10000: episode: 879, duration: 0.011s, episode steps: 8, steps per second: 710, episode reward: -110.500, mean reward: -13.812 [-100.000, 1.500], mean action: 11.500 [4.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 7040/10000: episode: 880, duration: 0.016s, episode steps: 8, steps per second: 486, episode reward: -203.750, mean reward: -25.469 [-100.000, 2.250], mean action: 11.250 [1.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 7048/10000: episode: 881, duration: 0.017s, episode steps: 8, steps per second: 474, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 14.000 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7056/10000: episode: 882, duration: 0.013s, episode steps: 8, steps per second: 617, episode reward: 9.500, mean reward: 1.188 [-5.000, 2.667], mean action: 13.500 [0.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7064/10000: episode: 883, duration: 0.018s, episode steps: 8, steps per second: 435, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 12.625 [0.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7072/10000: episode: 884, duration: 0.021s, episode steps: 8, steps per second: 384, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 13.125 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7080/10000: episode: 885, duration: 0.013s, episode steps: 8, steps per second: 629, episode reward: 17.417, mean reward: 2.177 [1.000, 3.125], mean action: 8.375 [0.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7088/10000: episode: 886, duration: 0.020s, episode steps: 8, steps per second: 399, episode reward: -190.500, mean reward: -23.812 [-100.000, 3.000], mean action: 11.625 [1.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7096/10000: episode: 887, duration: 0.015s, episode steps: 8, steps per second: 532, episode reward: -92.333, mean reward: -11.542 [-100.000, 1.333], mean action: 12.750 [1.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7104/10000: episode: 888, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: 3.933, mean reward: 0.492 [-5.000, 1.800], mean action: 13.875 [2.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7112/10000: episode: 889, duration: 0.020s, episode steps: 8, steps per second: 392, episode reward: 10.278, mean reward: 1.285 [1.000, 2.000], mean action: 12.500 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7120/10000: episode: 890, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 8.875 [1.000, 19.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 7128/10000: episode: 891, duration: 0.014s, episode steps: 8, steps per second: 567, episode reward: -3.000, mean reward: -0.375 [-5.000, 1.500], mean action: 9.000 [0.000, 16.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 7136/10000: episode: 892, duration: 0.014s, episode steps: 8, steps per second: 570, episode reward: 13.500, mean reward: 1.688 [1.000, 3.000], mean action: 11.500 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7144/10000: episode: 893, duration: 0.012s, episode steps: 8, steps per second: 663, episode reward: 12.083, mean reward: 1.510 [1.000, 3.000], mean action: 14.625 [6.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7152/10000: episode: 894, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 13.750 [4.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7160/10000: episode: 895, duration: 0.012s, episode steps: 8, steps per second: 678, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.250], mean action: 14.250 [8.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7168/10000: episode: 896, duration: 0.012s, episode steps: 8, steps per second: 648, episode reward: -8.000, mean reward: -1.000 [-5.000, 2.000], mean action: 7.875 [4.000, 17.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7176/10000: episode: 897, duration: 0.016s, episode steps: 8, steps per second: 510, episode reward: 10.625, mean reward: 1.328 [1.000, 2.000], mean action: 14.000 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7184/10000: episode: 898, duration: 0.012s, episode steps: 8, steps per second: 653, episode reward: 9.125, mean reward: 1.141 [1.000, 1.562], mean action: 10.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7192/10000: episode: 899, duration: 0.017s, episode steps: 8, steps per second: 474, episode reward: 13.583, mean reward: 1.698 [1.000, 2.667], mean action: 10.000 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7200/10000: episode: 900, duration: 0.012s, episode steps: 8, steps per second: 654, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 10.750 [4.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7208/10000: episode: 901, duration: 0.013s, episode steps: 8, steps per second: 597, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 10.500 [1.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: 17.579167\n",
      " 7216/10000: episode: 902, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 13.500 [1.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 7224/10000: episode: 903, duration: 0.011s, episode steps: 8, steps per second: 708, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 9.750 [0.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7232/10000: episode: 904, duration: 0.013s, episode steps: 8, steps per second: 628, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 13.000 [0.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7240/10000: episode: 905, duration: 0.013s, episode steps: 8, steps per second: 599, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 11.875 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7248/10000: episode: 906, duration: 0.011s, episode steps: 8, steps per second: 699, episode reward: -88.000, mean reward: -11.000 [-100.000, 3.000], mean action: 12.250 [2.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7256/10000: episode: 907, duration: 0.016s, episode steps: 8, steps per second: 503, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.250], mean action: 11.000 [3.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7264/10000: episode: 908, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: -3.333, mean reward: -0.417 [-5.000, 1.333], mean action: 16.375 [7.000, 21.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 7272/10000: episode: 909, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: 8.417, mean reward: 1.052 [-5.000, 3.125], mean action: 9.125 [0.000, 19.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7280/10000: episode: 910, duration: 0.013s, episode steps: 8, steps per second: 621, episode reward: 5.833, mean reward: 0.729 [-5.000, 2.667], mean action: 12.750 [0.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7288/10000: episode: 911, duration: 0.017s, episode steps: 8, steps per second: 467, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 15.750 [6.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7296/10000: episode: 912, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: -212.000, mean reward: -26.500 [-100.000, 1.000], mean action: 18.125 [1.000, 25.000], mean observation: 0.062 [0.000, 1.000], mean_best_reward: --\n",
      " 7304/10000: episode: 913, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: 5.444, mean reward: 0.681 [-5.000, 2.083], mean action: 12.000 [0.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7312/10000: episode: 914, duration: 0.015s, episode steps: 8, steps per second: 540, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 11.125 [0.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7320/10000: episode: 915, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 9.625 [1.000, 20.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7328/10000: episode: 916, duration: 0.012s, episode steps: 8, steps per second: 663, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 10.000 [3.000, 24.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 7336/10000: episode: 917, duration: 0.017s, episode steps: 8, steps per second: 479, episode reward: 7.000, mean reward: 0.875 [-5.000, 3.000], mean action: 9.000 [2.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7344/10000: episode: 918, duration: 0.019s, episode steps: 8, steps per second: 426, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 11.125 [1.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7352/10000: episode: 919, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: -7.750, mean reward: -0.969 [-5.000, 2.250], mean action: 13.375 [3.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 7360/10000: episode: 920, duration: 0.017s, episode steps: 8, steps per second: 479, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 9.375 [2.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7368/10000: episode: 921, duration: 0.015s, episode steps: 8, steps per second: 547, episode reward: 13.833, mean reward: 1.729 [1.000, 4.167], mean action: 9.250 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7376/10000: episode: 922, duration: 0.018s, episode steps: 8, steps per second: 454, episode reward: 5.556, mean reward: 0.694 [-5.000, 2.778], mean action: 15.750 [4.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7384/10000: episode: 923, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: 12.167, mean reward: 1.521 [1.000, 2.083], mean action: 14.500 [6.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7392/10000: episode: 924, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: -200.000, mean reward: -25.000 [-100.000, 1.000], mean action: 18.375 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7400/10000: episode: 925, duration: 0.017s, episode steps: 8, steps per second: 484, episode reward: 10.167, mean reward: 1.271 [-5.000, 3.125], mean action: 11.375 [6.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7408/10000: episode: 926, duration: 0.013s, episode steps: 8, steps per second: 640, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 12.375 [3.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7416/10000: episode: 927, duration: 0.015s, episode steps: 8, steps per second: 517, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 11.625 [2.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7424/10000: episode: 928, duration: 0.014s, episode steps: 8, steps per second: 589, episode reward: -92.500, mean reward: -11.562 [-100.000, 1.500], mean action: 13.000 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7432/10000: episode: 929, duration: 0.012s, episode steps: 8, steps per second: 645, episode reward: 11.000, mean reward: 1.375 [-5.000, 4.167], mean action: 13.625 [1.000, 18.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7440/10000: episode: 930, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 12.375 [3.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7448/10000: episode: 931, duration: 0.012s, episode steps: 8, steps per second: 681, episode reward: -104.500, mean reward: -13.062 [-100.000, 1.500], mean action: 17.250 [2.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 7456/10000: episode: 932, duration: 0.012s, episode steps: 8, steps per second: 668, episode reward: -194.000, mean reward: -24.250 [-100.000, 1.000], mean action: 13.625 [0.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 7464/10000: episode: 933, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: 12.778, mean reward: 1.597 [1.000, 2.778], mean action: 11.750 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7472/10000: episode: 934, duration: 0.018s, episode steps: 8, steps per second: 439, episode reward: -85.944, mean reward: -10.743 [-100.000, 3.000], mean action: 14.375 [3.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7480/10000: episode: 935, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 13.000 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7488/10000: episode: 936, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: -0.500, mean reward: -0.062 [-5.000, 2.500], mean action: 11.375 [2.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7496/10000: episode: 937, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: 11.800, mean reward: 1.475 [1.000, 2.400], mean action: 13.500 [3.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7504/10000: episode: 938, duration: 0.013s, episode steps: 8, steps per second: 607, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 12.750 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7512/10000: episode: 939, duration: 0.012s, episode steps: 8, steps per second: 654, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 11.500 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7520/10000: episode: 940, duration: 0.016s, episode steps: 8, steps per second: 490, episode reward: -87.417, mean reward: -10.927 [-100.000, 2.667], mean action: 13.625 [3.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7528/10000: episode: 941, duration: 0.012s, episode steps: 8, steps per second: 641, episode reward: -7.750, mean reward: -0.969 [-5.000, 2.250], mean action: 11.750 [1.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 7536/10000: episode: 942, duration: 0.012s, episode steps: 8, steps per second: 651, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 14.500 [3.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7544/10000: episode: 943, duration: 0.014s, episode steps: 8, steps per second: 569, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 15.625 [7.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7552/10000: episode: 944, duration: 0.013s, episode steps: 8, steps per second: 629, episode reward: 5.917, mean reward: 0.740 [-5.000, 2.667], mean action: 10.875 [3.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7560/10000: episode: 945, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 9.167, mean reward: 1.146 [-5.000, 3.125], mean action: 18.625 [4.000, 24.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7568/10000: episode: 946, duration: 0.015s, episode steps: 8, steps per second: 551, episode reward: 14.028, mean reward: 1.753 [1.000, 3.000], mean action: 11.500 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7576/10000: episode: 947, duration: 0.013s, episode steps: 8, steps per second: 631, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 14.875 [2.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 7584/10000: episode: 948, duration: 0.014s, episode steps: 8, steps per second: 560, episode reward: -90.750, mean reward: -11.344 [-100.000, 2.250], mean action: 12.375 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7592/10000: episode: 949, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 10.875 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7600/10000: episode: 950, duration: 0.012s, episode steps: 8, steps per second: 680, episode reward: 5.056, mean reward: 0.632 [-5.000, 2.000], mean action: 11.000 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7608/10000: episode: 951, duration: 0.015s, episode steps: 8, steps per second: 548, episode reward: 6.250, mean reward: 0.781 [-5.000, 2.250], mean action: 15.375 [4.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: 14.355556\n",
      " 7616/10000: episode: 952, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 11.250 [1.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7624/10000: episode: 953, duration: 0.012s, episode steps: 8, steps per second: 660, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 11.250 [1.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 7632/10000: episode: 954, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: -89.500, mean reward: -11.188 [-100.000, 2.000], mean action: 13.125 [1.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7640/10000: episode: 955, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: -0.667, mean reward: -0.083 [-5.000, 2.250], mean action: 14.750 [6.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7648/10000: episode: 956, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 10.375 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7656/10000: episode: 957, duration: 0.015s, episode steps: 8, steps per second: 531, episode reward: 6.833, mean reward: 0.854 [-5.000, 2.667], mean action: 10.375 [3.000, 20.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7664/10000: episode: 958, duration: 0.012s, episode steps: 8, steps per second: 640, episode reward: 4.667, mean reward: 0.583 [-5.000, 2.667], mean action: 15.625 [7.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7672/10000: episode: 959, duration: 0.014s, episode steps: 8, steps per second: 567, episode reward: 12.583, mean reward: 1.573 [1.000, 2.667], mean action: 11.250 [1.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7680/10000: episode: 960, duration: 0.013s, episode steps: 8, steps per second: 619, episode reward: -1.639, mean reward: -0.205 [-5.000, 2.083], mean action: 15.500 [3.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7688/10000: episode: 961, duration: 0.011s, episode steps: 8, steps per second: 718, episode reward: 11.867, mean reward: 1.483 [-5.000, 3.600], mean action: 9.000 [5.000, 12.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7696/10000: episode: 962, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 11.250 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7704/10000: episode: 963, duration: 0.014s, episode steps: 8, steps per second: 564, episode reward: 15.583, mean reward: 1.948 [1.000, 3.000], mean action: 11.125 [3.000, 17.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7712/10000: episode: 964, duration: 0.012s, episode steps: 8, steps per second: 659, episode reward: -6.375, mean reward: -0.797 [-5.000, 3.125], mean action: 11.875 [1.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7720/10000: episode: 965, duration: 0.015s, episode steps: 8, steps per second: 529, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 11.250 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7728/10000: episode: 966, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 14.000 [1.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 7736/10000: episode: 967, duration: 0.013s, episode steps: 8, steps per second: 607, episode reward: 10.167, mean reward: 1.271 [1.000, 2.000], mean action: 14.875 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7744/10000: episode: 968, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: 14.333, mean reward: 1.792 [1.000, 3.000], mean action: 15.625 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7752/10000: episode: 969, duration: 0.012s, episode steps: 8, steps per second: 662, episode reward: -199.500, mean reward: -24.938 [-100.000, 1.500], mean action: 9.625 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7760/10000: episode: 970, duration: 0.017s, episode steps: 8, steps per second: 480, episode reward: -199.000, mean reward: -24.875 [-100.000, 1.500], mean action: 13.875 [5.000, 25.000], mean observation: 0.068 [0.000, 1.000], mean_best_reward: --\n",
      " 7768/10000: episode: 971, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 9.750 [4.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7776/10000: episode: 972, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 12.625 [0.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7784/10000: episode: 973, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: -6.000, mean reward: -0.750 [-5.000, 3.000], mean action: 20.625 [13.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7792/10000: episode: 974, duration: 0.013s, episode steps: 8, steps per second: 610, episode reward: -104.000, mean reward: -13.000 [-100.000, 1.500], mean action: 16.500 [8.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 7800/10000: episode: 975, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 13.500 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7808/10000: episode: 976, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7816/10000: episode: 977, duration: 0.011s, episode steps: 8, steps per second: 738, episode reward: -99.417, mean reward: -12.427 [-100.000, 2.667], mean action: 8.125 [2.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 7824/10000: episode: 978, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: -7.000, mean reward: -0.875 [-5.000, 3.000], mean action: 12.625 [1.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7832/10000: episode: 979, duration: 0.012s, episode steps: 8, steps per second: 680, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 11.000 [0.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 7840/10000: episode: 980, duration: 0.013s, episode steps: 8, steps per second: 618, episode reward: 4.917, mean reward: 0.615 [-5.000, 2.667], mean action: 12.375 [0.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7848/10000: episode: 981, duration: 0.012s, episode steps: 8, steps per second: 663, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 11.500 [1.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 7856/10000: episode: 982, duration: 0.012s, episode steps: 8, steps per second: 646, episode reward: 6.300, mean reward: 0.787 [-5.000, 2.400], mean action: 12.500 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7864/10000: episode: 983, duration: 0.012s, episode steps: 8, steps per second: 645, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 7.500 [0.000, 14.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7872/10000: episode: 984, duration: 0.025s, episode steps: 8, steps per second: 325, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 16.500 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7880/10000: episode: 985, duration: 0.016s, episode steps: 8, steps per second: 509, episode reward: 11.944, mean reward: 1.493 [1.000, 2.778], mean action: 9.000 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7888/10000: episode: 986, duration: 0.015s, episode steps: 8, steps per second: 534, episode reward: 3.933, mean reward: 0.492 [-5.000, 1.800], mean action: 13.500 [2.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7896/10000: episode: 987, duration: 0.013s, episode steps: 8, steps per second: 634, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 11.000 [4.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7904/10000: episode: 988, duration: 0.012s, episode steps: 8, steps per second: 677, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 12.625 [6.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 7912/10000: episode: 989, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 15.125 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7920/10000: episode: 990, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: 13.678, mean reward: 1.710 [1.000, 2.450], mean action: 9.625 [2.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 7928/10000: episode: 991, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: -192.000, mean reward: -24.000 [-100.000, 2.000], mean action: 15.875 [1.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 7936/10000: episode: 992, duration: 0.013s, episode steps: 8, steps per second: 603, episode reward: 6.500, mean reward: 0.812 [-5.000, 2.250], mean action: 10.875 [0.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7944/10000: episode: 993, duration: 0.014s, episode steps: 8, steps per second: 579, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 14.375 [1.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 7952/10000: episode: 994, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: -96.000, mean reward: -12.000 [-100.000, 3.000], mean action: 13.500 [2.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 7960/10000: episode: 995, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: -88.000, mean reward: -11.000 [-100.000, 2.500], mean action: 13.000 [5.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 7968/10000: episode: 996, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 15.250 [1.000, 23.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 7976/10000: episode: 997, duration: 0.014s, episode steps: 8, steps per second: 582, episode reward: 10.944, mean reward: 1.368 [-5.000, 3.000], mean action: 9.375 [2.000, 18.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 7984/10000: episode: 998, duration: 0.017s, episode steps: 8, steps per second: 481, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 10.250 [0.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 7992/10000: episode: 999, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 13.875 [1.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8000/10000: episode: 1000, duration: 0.014s, episode steps: 8, steps per second: 590, episode reward: 4.806, mean reward: 0.601 [-5.000, 2.250], mean action: 11.750 [2.000, 22.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8008/10000: episode: 1001, duration: 0.017s, episode steps: 8, steps per second: 483, episode reward: 4.056, mean reward: 0.507 [-5.000, 1.778], mean action: 13.375 [2.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: 16.433333\n",
      " 8016/10000: episode: 1002, duration: 0.015s, episode steps: 8, steps per second: 549, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 11.000 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8024/10000: episode: 1003, duration: 0.012s, episode steps: 8, steps per second: 682, episode reward: -92.000, mean reward: -11.500 [-100.000, 1.500], mean action: 13.750 [2.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 8032/10000: episode: 1004, duration: 0.020s, episode steps: 8, steps per second: 395, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 13.125 [2.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8040/10000: episode: 1005, duration: 0.016s, episode steps: 8, steps per second: 498, episode reward: 14.000, mean reward: 1.750 [1.000, 4.000], mean action: 11.375 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8048/10000: episode: 1006, duration: 0.015s, episode steps: 8, steps per second: 543, episode reward: 12.250, mean reward: 1.531 [-5.000, 4.167], mean action: 17.125 [10.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8056/10000: episode: 1007, duration: 0.016s, episode steps: 8, steps per second: 514, episode reward: 8.500, mean reward: 1.062 [1.000, 1.500], mean action: 13.000 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8064/10000: episode: 1008, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: 9.417, mean reward: 1.177 [1.000, 2.083], mean action: 12.625 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8072/10000: episode: 1009, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: -192.000, mean reward: -24.000 [-100.000, 2.000], mean action: 13.250 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8080/10000: episode: 1010, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 9.250 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8088/10000: episode: 1011, duration: 0.016s, episode steps: 8, steps per second: 490, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 13.625 [3.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8096/10000: episode: 1012, duration: 0.014s, episode steps: 8, steps per second: 565, episode reward: 6.800, mean reward: 0.850 [-5.000, 2.500], mean action: 19.500 [11.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8104/10000: episode: 1013, duration: 0.013s, episode steps: 8, steps per second: 627, episode reward: 12.056, mean reward: 1.507 [1.000, 3.000], mean action: 12.625 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8112/10000: episode: 1014, duration: 0.014s, episode steps: 8, steps per second: 553, episode reward: -88.500, mean reward: -11.062 [-100.000, 2.250], mean action: 13.875 [1.000, 25.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8120/10000: episode: 1015, duration: 0.013s, episode steps: 8, steps per second: 607, episode reward: 0.250, mean reward: 0.031 [-5.000, 3.000], mean action: 9.625 [2.000, 18.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8128/10000: episode: 1016, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: 7.583, mean reward: 0.948 [-5.000, 2.667], mean action: 13.375 [1.000, 24.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8136/10000: episode: 1017, duration: 0.012s, episode steps: 8, steps per second: 683, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 14.875 [2.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8144/10000: episode: 1018, duration: 0.012s, episode steps: 8, steps per second: 645, episode reward: 6.833, mean reward: 0.854 [-5.000, 2.778], mean action: 12.125 [4.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8152/10000: episode: 1019, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: -90.444, mean reward: -11.306 [-100.000, 2.000], mean action: 13.625 [2.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8160/10000: episode: 1020, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 16.500 [3.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8168/10000: episode: 1021, duration: 0.012s, episode steps: 8, steps per second: 678, episode reward: -98.333, mean reward: -12.292 [-100.000, 1.333], mean action: 11.250 [2.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 8176/10000: episode: 1022, duration: 0.012s, episode steps: 8, steps per second: 670, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 14.750 [4.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 8184/10000: episode: 1023, duration: 0.013s, episode steps: 8, steps per second: 634, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 16.500 [2.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 8192/10000: episode: 1024, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: 13.250, mean reward: 1.656 [1.000, 3.125], mean action: 11.375 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8200/10000: episode: 1025, duration: 0.014s, episode steps: 8, steps per second: 567, episode reward: 16.472, mean reward: 2.059 [1.000, 2.778], mean action: 10.875 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8208/10000: episode: 1026, duration: 0.012s, episode steps: 8, steps per second: 669, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 14.375 [3.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8216/10000: episode: 1027, duration: 0.012s, episode steps: 8, steps per second: 695, episode reward: -1.000, mean reward: -0.125 [-5.000, 3.000], mean action: 16.000 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8224/10000: episode: 1028, duration: 0.013s, episode steps: 8, steps per second: 629, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 14.625 [0.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 8232/10000: episode: 1029, duration: 0.012s, episode steps: 8, steps per second: 683, episode reward: 18.778, mean reward: 2.347 [1.000, 4.000], mean action: 13.875 [3.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8240/10000: episode: 1030, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.000], mean action: 13.500 [2.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8248/10000: episode: 1031, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: 10.500, mean reward: 1.312 [-5.000, 4.167], mean action: 16.375 [1.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8256/10000: episode: 1032, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: 18.250, mean reward: 2.281 [1.000, 4.000], mean action: 10.125 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8264/10000: episode: 1033, duration: 0.014s, episode steps: 8, steps per second: 570, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 8.500 [0.000, 18.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8272/10000: episode: 1034, duration: 0.014s, episode steps: 8, steps per second: 585, episode reward: -88.000, mean reward: -11.000 [-100.000, 3.000], mean action: 13.375 [0.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8280/10000: episode: 1035, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: -90.944, mean reward: -11.368 [-100.000, 1.778], mean action: 13.625 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8288/10000: episode: 1036, duration: 0.016s, episode steps: 8, steps per second: 507, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 11.500 [2.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8296/10000: episode: 1037, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: 4.250, mean reward: 0.531 [-5.000, 2.250], mean action: 11.125 [1.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8304/10000: episode: 1038, duration: 0.013s, episode steps: 8, steps per second: 620, episode reward: 6.000, mean reward: 0.750 [-5.000, 3.000], mean action: 14.625 [0.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8312/10000: episode: 1039, duration: 0.013s, episode steps: 8, steps per second: 594, episode reward: 16.433, mean reward: 2.054 [1.000, 3.267], mean action: 13.750 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8320/10000: episode: 1040, duration: 0.013s, episode steps: 8, steps per second: 595, episode reward: 5.667, mean reward: 0.708 [-5.000, 2.083], mean action: 10.125 [0.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8328/10000: episode: 1041, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 10.375 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8336/10000: episode: 1042, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: 12.250, mean reward: 1.531 [1.000, 2.250], mean action: 11.250 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8344/10000: episode: 1043, duration: 0.016s, episode steps: 8, steps per second: 487, episode reward: 14.500, mean reward: 1.812 [1.000, 3.125], mean action: 16.875 [8.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8352/10000: episode: 1044, duration: 0.025s, episode steps: 8, steps per second: 323, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 17.500 [7.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 8360/10000: episode: 1045, duration: 0.017s, episode steps: 8, steps per second: 462, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 13.125 [1.000, 25.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 8368/10000: episode: 1046, duration: 0.021s, episode steps: 8, steps per second: 390, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 15.000 [7.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8376/10000: episode: 1047, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: 5.500, mean reward: 0.688 [-5.000, 2.000], mean action: 15.125 [5.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8384/10000: episode: 1048, duration: 0.016s, episode steps: 8, steps per second: 502, episode reward: 11.917, mean reward: 1.490 [1.000, 2.667], mean action: 8.375 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8392/10000: episode: 1049, duration: 0.018s, episode steps: 8, steps per second: 446, episode reward: 8.500, mean reward: 1.062 [1.000, 1.500], mean action: 9.125 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8400/10000: episode: 1050, duration: 0.013s, episode steps: 8, steps per second: 625, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 13.000 [2.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8408/10000: episode: 1051, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 10.500 [2.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: 16.675000\n",
      " 8416/10000: episode: 1052, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 11.375 [4.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8424/10000: episode: 1053, duration: 0.016s, episode steps: 8, steps per second: 493, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 9.625 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8432/10000: episode: 1054, duration: 0.012s, episode steps: 8, steps per second: 677, episode reward: 13.583, mean reward: 1.698 [1.000, 2.667], mean action: 12.625 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8440/10000: episode: 1055, duration: 0.023s, episode steps: 8, steps per second: 351, episode reward: 11.250, mean reward: 1.406 [1.000, 2.250], mean action: 12.750 [0.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8448/10000: episode: 1056, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 8.500 [0.000, 19.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8456/10000: episode: 1057, duration: 0.013s, episode steps: 8, steps per second: 599, episode reward: 4.417, mean reward: 0.552 [-5.000, 4.167], mean action: 16.250 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8464/10000: episode: 1058, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 8.750 [0.000, 18.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8472/10000: episode: 1059, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 12.625 [0.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8480/10000: episode: 1060, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: 6.078, mean reward: 0.760 [-5.000, 2.400], mean action: 9.750 [1.000, 21.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8488/10000: episode: 1061, duration: 0.013s, episode steps: 8, steps per second: 612, episode reward: -2.000, mean reward: -0.250 [-5.000, 3.000], mean action: 15.250 [6.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8496/10000: episode: 1062, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 17.375 [12.000, 25.000], mean observation: 0.060 [0.000, 1.000], mean_best_reward: --\n",
      " 8504/10000: episode: 1063, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: -1.500, mean reward: -0.188 [-5.000, 2.250], mean action: 14.875 [2.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 8512/10000: episode: 1064, duration: 0.011s, episode steps: 8, steps per second: 708, episode reward: 6.333, mean reward: 0.792 [-5.000, 2.667], mean action: 12.250 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8520/10000: episode: 1065, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: 16.778, mean reward: 2.097 [1.000, 4.000], mean action: 10.250 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8528/10000: episode: 1066, duration: 0.012s, episode steps: 8, steps per second: 666, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 15.500 [1.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8536/10000: episode: 1067, duration: 0.011s, episode steps: 8, steps per second: 711, episode reward: 4.683, mean reward: 0.585 [-5.000, 1.800], mean action: 14.375 [1.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8544/10000: episode: 1068, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: -6.750, mean reward: -0.844 [-5.000, 2.667], mean action: 13.000 [3.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8552/10000: episode: 1069, duration: 0.014s, episode steps: 8, steps per second: 568, episode reward: 6.500, mean reward: 0.812 [-5.000, 3.000], mean action: 10.250 [5.000, 15.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8560/10000: episode: 1070, duration: 0.012s, episode steps: 8, steps per second: 680, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 10.500 [0.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8568/10000: episode: 1071, duration: 0.012s, episode steps: 8, steps per second: 668, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.250], mean action: 9.375 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8576/10000: episode: 1072, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: -92.500, mean reward: -11.562 [-100.000, 1.500], mean action: 11.625 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8584/10000: episode: 1073, duration: 0.016s, episode steps: 8, steps per second: 512, episode reward: -291.750, mean reward: -36.469 [-100.000, 2.250], mean action: 14.250 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8592/10000: episode: 1074, duration: 0.013s, episode steps: 8, steps per second: 627, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 11.375 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8600/10000: episode: 1075, duration: 0.015s, episode steps: 8, steps per second: 541, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 11.750 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8608/10000: episode: 1076, duration: 0.017s, episode steps: 8, steps per second: 477, episode reward: 6.250, mean reward: 0.781 [-5.000, 2.250], mean action: 8.625 [3.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8616/10000: episode: 1077, duration: 0.013s, episode steps: 8, steps per second: 621, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 13.500 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8624/10000: episode: 1078, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 12.500 [4.000, 22.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8632/10000: episode: 1079, duration: 0.015s, episode steps: 8, steps per second: 520, episode reward: 12.567, mean reward: 1.571 [1.000, 2.667], mean action: 10.375 [3.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8640/10000: episode: 1080, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 8.125 [0.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8648/10000: episode: 1081, duration: 0.012s, episode steps: 8, steps per second: 651, episode reward: -15.000, mean reward: -1.875 [-5.000, 2.000], mean action: 8.875 [5.000, 18.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 8656/10000: episode: 1082, duration: 0.011s, episode steps: 8, steps per second: 711, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 15.000 [3.000, 21.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 8664/10000: episode: 1083, duration: 0.012s, episode steps: 8, steps per second: 660, episode reward: 3.500, mean reward: 0.438 [-5.000, 2.000], mean action: 11.250 [0.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8672/10000: episode: 1084, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: 9.556, mean reward: 1.194 [1.000, 1.778], mean action: 14.125 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8680/10000: episode: 1085, duration: 0.014s, episode steps: 8, steps per second: 561, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 13.750 [8.000, 20.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8688/10000: episode: 1086, duration: 0.011s, episode steps: 8, steps per second: 733, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 7.875 [2.000, 22.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 8696/10000: episode: 1087, duration: 0.013s, episode steps: 8, steps per second: 614, episode reward: -0.750, mean reward: -0.094 [-5.000, 2.250], mean action: 11.750 [5.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8704/10000: episode: 1088, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: -191.833, mean reward: -23.979 [-100.000, 2.667], mean action: 12.625 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8712/10000: episode: 1089, duration: 0.015s, episode steps: 8, steps per second: 532, episode reward: -104.167, mean reward: -13.021 [-100.000, 1.500], mean action: 13.125 [2.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8720/10000: episode: 1090, duration: 0.014s, episode steps: 8, steps per second: 564, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 14.125 [1.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 8728/10000: episode: 1091, duration: 0.011s, episode steps: 8, steps per second: 725, episode reward: 13.500, mean reward: 1.688 [1.000, 2.250], mean action: 13.625 [5.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8736/10000: episode: 1092, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.750 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8744/10000: episode: 1093, duration: 0.012s, episode steps: 8, steps per second: 648, episode reward: 15.833, mean reward: 1.979 [1.000, 4.167], mean action: 12.125 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8752/10000: episode: 1094, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 18.625 [12.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 8760/10000: episode: 1095, duration: 0.014s, episode steps: 8, steps per second: 558, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 16.250 [5.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8768/10000: episode: 1096, duration: 0.012s, episode steps: 8, steps per second: 685, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 13.125 [7.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8776/10000: episode: 1097, duration: 0.014s, episode steps: 8, steps per second: 585, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 12.125 [6.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 8784/10000: episode: 1098, duration: 0.013s, episode steps: 8, steps per second: 598, episode reward: -88.333, mean reward: -11.042 [-100.000, 3.000], mean action: 11.750 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 8792/10000: episode: 1099, duration: 0.012s, episode steps: 8, steps per second: 675, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 14.625 [3.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8800/10000: episode: 1100, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: 12.944, mean reward: 1.618 [1.000, 2.778], mean action: 16.750 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8808/10000: episode: 1101, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 14.500 [4.000, 23.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: 17.388889\n",
      " 8816/10000: episode: 1102, duration: 0.016s, episode steps: 8, steps per second: 509, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 6.125 [0.000, 18.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8824/10000: episode: 1103, duration: 0.015s, episode steps: 8, steps per second: 542, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.250], mean action: 12.875 [3.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8832/10000: episode: 1104, duration: 0.012s, episode steps: 8, steps per second: 679, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 13.000 [0.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 8840/10000: episode: 1105, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: 6.833, mean reward: 0.854 [-5.000, 2.778], mean action: 18.875 [13.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 8848/10000: episode: 1106, duration: 0.011s, episode steps: 8, steps per second: 709, episode reward: -2.500, mean reward: -0.312 [-5.000, 2.000], mean action: 11.875 [2.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8856/10000: episode: 1107, duration: 0.012s, episode steps: 8, steps per second: 668, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 13.625 [2.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 8864/10000: episode: 1108, duration: 0.012s, episode steps: 8, steps per second: 662, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 11.375 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8872/10000: episode: 1109, duration: 0.014s, episode steps: 8, steps per second: 588, episode reward: 13.083, mean reward: 1.635 [1.000, 2.250], mean action: 9.375 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8880/10000: episode: 1110, duration: 0.012s, episode steps: 8, steps per second: 670, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 13.250 [1.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 8888/10000: episode: 1111, duration: 0.015s, episode steps: 8, steps per second: 542, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 9.875 [2.000, 16.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8896/10000: episode: 1112, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 16.125 [3.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 8904/10000: episode: 1113, duration: 0.015s, episode steps: 8, steps per second: 525, episode reward: 10.500, mean reward: 1.312 [1.000, 2.250], mean action: 13.875 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 8912/10000: episode: 1114, duration: 0.011s, episode steps: 8, steps per second: 706, episode reward: -3.500, mean reward: -0.438 [-5.000, 1.500], mean action: 17.375 [9.000, 20.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 8920/10000: episode: 1115, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: -92.000, mean reward: -11.500 [-100.000, 1.500], mean action: 13.000 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8928/10000: episode: 1116, duration: 0.013s, episode steps: 8, steps per second: 633, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 10.125 [0.000, 25.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 8936/10000: episode: 1117, duration: 0.014s, episode steps: 8, steps per second: 583, episode reward: -190.000, mean reward: -23.750 [-100.000, 3.000], mean action: 14.250 [8.000, 25.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8944/10000: episode: 1118, duration: 0.012s, episode steps: 8, steps per second: 687, episode reward: -88.333, mean reward: -11.042 [-100.000, 2.083], mean action: 10.000 [2.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8952/10000: episode: 1119, duration: 0.018s, episode steps: 8, steps per second: 447, episode reward: -0.944, mean reward: -0.118 [-5.000, 2.778], mean action: 12.125 [3.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8960/10000: episode: 1120, duration: 0.017s, episode steps: 8, steps per second: 482, episode reward: 3.250, mean reward: 0.406 [-5.000, 2.250], mean action: 8.750 [3.000, 21.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 8968/10000: episode: 1121, duration: 0.012s, episode steps: 8, steps per second: 647, episode reward: -0.444, mean reward: -0.056 [-5.000, 2.778], mean action: 15.875 [4.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8976/10000: episode: 1122, duration: 0.014s, episode steps: 8, steps per second: 581, episode reward: -0.944, mean reward: -0.118 [-5.000, 2.778], mean action: 13.750 [3.000, 22.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 8984/10000: episode: 1123, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 7.500, mean reward: 0.938 [-5.000, 3.000], mean action: 12.000 [1.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 8992/10000: episode: 1124, duration: 0.013s, episode steps: 8, steps per second: 616, episode reward: 3.000, mean reward: 0.375 [-5.000, 1.500], mean action: 12.125 [2.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9000/10000: episode: 1125, duration: 0.012s, episode steps: 8, steps per second: 655, episode reward: -193.167, mean reward: -24.146 [-100.000, 1.500], mean action: 14.750 [2.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 9008/10000: episode: 1126, duration: 0.011s, episode steps: 8, steps per second: 701, episode reward: 6.000, mean reward: 0.750 [-5.000, 2.000], mean action: 14.250 [8.000, 21.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9016/10000: episode: 1127, duration: 0.015s, episode steps: 8, steps per second: 547, episode reward: 12.000, mean reward: 1.500 [1.000, 2.000], mean action: 9.125 [0.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9024/10000: episode: 1128, duration: 0.015s, episode steps: 8, steps per second: 533, episode reward: 8.000, mean reward: 1.000 [-5.000, 3.000], mean action: 10.875 [0.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9032/10000: episode: 1129, duration: 0.012s, episode steps: 8, steps per second: 648, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 7.375 [1.000, 19.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9040/10000: episode: 1130, duration: 0.011s, episode steps: 8, steps per second: 700, episode reward: -97.000, mean reward: -12.125 [-100.000, 2.000], mean action: 16.500 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 9048/10000: episode: 1131, duration: 0.014s, episode steps: 8, steps per second: 582, episode reward: 14.083, mean reward: 1.760 [1.000, 3.000], mean action: 7.250 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9056/10000: episode: 1132, duration: 0.013s, episode steps: 8, steps per second: 601, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 11.250 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9064/10000: episode: 1133, duration: 0.014s, episode steps: 8, steps per second: 558, episode reward: 7.556, mean reward: 0.944 [-5.000, 2.778], mean action: 14.375 [5.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9072/10000: episode: 1134, duration: 0.014s, episode steps: 8, steps per second: 575, episode reward: 8.556, mean reward: 1.069 [-5.000, 3.000], mean action: 7.875 [0.000, 20.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9080/10000: episode: 1135, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: 12.250, mean reward: 1.531 [1.000, 2.250], mean action: 10.250 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9088/10000: episode: 1136, duration: 0.017s, episode steps: 8, steps per second: 478, episode reward: 10.833, mean reward: 1.354 [1.000, 2.000], mean action: 13.500 [2.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9096/10000: episode: 1137, duration: 0.012s, episode steps: 8, steps per second: 682, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 10.750 [0.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9104/10000: episode: 1138, duration: 0.014s, episode steps: 8, steps per second: 564, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 12.000 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9112/10000: episode: 1139, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: 12.167, mean reward: 1.521 [1.000, 2.083], mean action: 10.875 [3.000, 19.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9120/10000: episode: 1140, duration: 0.013s, episode steps: 8, steps per second: 626, episode reward: -87.000, mean reward: -10.875 [-100.000, 3.000], mean action: 14.250 [4.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9128/10000: episode: 1141, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 15.625 [6.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9136/10000: episode: 1142, duration: 0.014s, episode steps: 8, steps per second: 592, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 15.625 [3.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 9144/10000: episode: 1143, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: 9.625, mean reward: 1.203 [-5.000, 3.125], mean action: 12.125 [2.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9152/10000: episode: 1144, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: 11.125, mean reward: 1.391 [1.000, 3.125], mean action: 6.750 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9160/10000: episode: 1145, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: 18.575, mean reward: 2.322 [1.000, 3.267], mean action: 9.500 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9168/10000: episode: 1146, duration: 0.012s, episode steps: 8, steps per second: 666, episode reward: -0.167, mean reward: -0.021 [-5.000, 2.667], mean action: 15.125 [1.000, 21.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 9176/10000: episode: 1147, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: 7.583, mean reward: 0.948 [-5.000, 2.667], mean action: 6.875 [0.000, 21.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 9184/10000: episode: 1148, duration: 0.011s, episode steps: 8, steps per second: 719, episode reward: -88.000, mean reward: -11.000 [-100.000, 3.000], mean action: 12.250 [0.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9192/10000: episode: 1149, duration: 0.016s, episode steps: 8, steps per second: 500, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 15.250 [3.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9200/10000: episode: 1150, duration: 0.014s, episode steps: 8, steps per second: 591, episode reward: 13.021, mean reward: 1.628 [1.000, 3.062], mean action: 14.875 [7.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9208/10000: episode: 1151, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -6.750, mean reward: -0.844 [-5.000, 2.250], mean action: 16.125 [7.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: 16.258333\n",
      " 9216/10000: episode: 1152, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 12.000 [2.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9224/10000: episode: 1153, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 13.375 [7.000, 23.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 9232/10000: episode: 1154, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: 11.861, mean reward: 1.483 [1.000, 2.083], mean action: 14.250 [5.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9240/10000: episode: 1155, duration: 0.013s, episode steps: 8, steps per second: 610, episode reward: -7.500, mean reward: -0.938 [-5.000, 2.000], mean action: 13.375 [5.000, 20.000], mean observation: 0.070 [0.000, 1.000], mean_best_reward: --\n",
      " 9248/10000: episode: 1156, duration: 0.013s, episode steps: 8, steps per second: 606, episode reward: 10.917, mean reward: 1.365 [1.000, 2.667], mean action: 14.875 [3.000, 25.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9256/10000: episode: 1157, duration: 0.012s, episode steps: 8, steps per second: 670, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 10.500 [2.000, 22.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 9264/10000: episode: 1158, duration: 0.013s, episode steps: 8, steps per second: 615, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 12.750 [3.000, 21.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 9272/10000: episode: 1159, duration: 0.012s, episode steps: 8, steps per second: 692, episode reward: 5.028, mean reward: 0.628 [-5.000, 2.250], mean action: 14.625 [2.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9280/10000: episode: 1160, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 9.625 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9288/10000: episode: 1161, duration: 0.011s, episode steps: 8, steps per second: 697, episode reward: -9.000, mean reward: -1.125 [-5.000, 1.500], mean action: 17.250 [3.000, 22.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 9296/10000: episode: 1162, duration: 0.017s, episode steps: 8, steps per second: 481, episode reward: 6.000, mean reward: 0.750 [-5.000, 3.000], mean action: 10.625 [4.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9304/10000: episode: 1163, duration: 0.013s, episode steps: 8, steps per second: 613, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 17.750 [4.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9312/10000: episode: 1164, duration: 0.013s, episode steps: 8, steps per second: 599, episode reward: -89.500, mean reward: -11.188 [-100.000, 2.250], mean action: 14.750 [4.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9320/10000: episode: 1165, duration: 0.015s, episode steps: 8, steps per second: 537, episode reward: 5.833, mean reward: 0.729 [-5.000, 2.667], mean action: 10.500 [1.000, 17.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9328/10000: episode: 1166, duration: 0.011s, episode steps: 8, steps per second: 732, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 12.750 [1.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9336/10000: episode: 1167, duration: 0.013s, episode steps: 8, steps per second: 636, episode reward: 15.000, mean reward: 1.875 [1.000, 4.000], mean action: 11.500 [3.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9344/10000: episode: 1168, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 11.125 [0.000, 19.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 9352/10000: episode: 1169, duration: 0.014s, episode steps: 8, steps per second: 577, episode reward: -2.417, mean reward: -0.302 [-5.000, 2.083], mean action: 14.625 [7.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9360/10000: episode: 1170, duration: 0.012s, episode steps: 8, steps per second: 643, episode reward: -6.833, mean reward: -0.854 [-5.000, 2.667], mean action: 11.875 [5.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 9368/10000: episode: 1171, duration: 0.016s, episode steps: 8, steps per second: 510, episode reward: 20.222, mean reward: 2.528 [1.000, 4.083], mean action: 8.625 [2.000, 14.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9376/10000: episode: 1172, duration: 0.014s, episode steps: 8, steps per second: 567, episode reward: 13.478, mean reward: 1.685 [1.000, 2.450], mean action: 12.125 [3.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9384/10000: episode: 1173, duration: 0.013s, episode steps: 8, steps per second: 635, episode reward: 15.250, mean reward: 1.906 [1.000, 3.125], mean action: 11.375 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9392/10000: episode: 1174, duration: 0.013s, episode steps: 8, steps per second: 605, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 17.250 [9.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9400/10000: episode: 1175, duration: 0.013s, episode steps: 8, steps per second: 610, episode reward: -1.333, mean reward: -0.167 [-5.000, 2.667], mean action: 9.625 [0.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9408/10000: episode: 1176, duration: 0.013s, episode steps: 8, steps per second: 593, episode reward: 10.067, mean reward: 1.258 [1.000, 2.400], mean action: 12.625 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9416/10000: episode: 1177, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: 11.800, mean reward: 1.475 [1.000, 2.400], mean action: 12.375 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9424/10000: episode: 1178, duration: 0.017s, episode steps: 8, steps per second: 463, episode reward: 14.917, mean reward: 1.865 [1.000, 3.125], mean action: 13.375 [5.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9432/10000: episode: 1179, duration: 0.012s, episode steps: 8, steps per second: 665, episode reward: -92.000, mean reward: -11.500 [-100.000, 1.500], mean action: 12.750 [1.000, 25.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9440/10000: episode: 1180, duration: 0.014s, episode steps: 8, steps per second: 574, episode reward: 13.333, mean reward: 1.667 [1.000, 2.667], mean action: 9.875 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9448/10000: episode: 1181, duration: 0.014s, episode steps: 8, steps per second: 559, episode reward: -92.500, mean reward: -11.562 [-100.000, 1.500], mean action: 14.375 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9456/10000: episode: 1182, duration: 0.011s, episode steps: 8, steps per second: 720, episode reward: -99.000, mean reward: -12.375 [-100.000, 1.000], mean action: 17.625 [3.000, 25.000], mean observation: 0.080 [0.000, 1.000], mean_best_reward: --\n",
      " 9464/10000: episode: 1183, duration: 0.013s, episode steps: 8, steps per second: 604, episode reward: 9.000, mean reward: 1.125 [1.000, 2.000], mean action: 12.625 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9472/10000: episode: 1184, duration: 0.011s, episode steps: 8, steps per second: 708, episode reward: 13.583, mean reward: 1.698 [1.000, 2.667], mean action: 9.625 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9480/10000: episode: 1185, duration: 0.013s, episode steps: 8, steps per second: 600, episode reward: -105.750, mean reward: -13.219 [-100.000, 4.000], mean action: 7.500 [2.000, 25.000], mean observation: 0.072 [0.000, 1.000], mean_best_reward: --\n",
      " 9488/10000: episode: 1186, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 13.750 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9496/10000: episode: 1187, duration: 0.014s, episode steps: 8, steps per second: 563, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 11.125 [0.000, 23.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9504/10000: episode: 1188, duration: 0.014s, episode steps: 8, steps per second: 570, episode reward: 19.333, mean reward: 2.417 [1.000, 4.167], mean action: 12.125 [3.000, 21.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9512/10000: episode: 1189, duration: 0.020s, episode steps: 8, steps per second: 391, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 15.500 [2.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9520/10000: episode: 1190, duration: 0.016s, episode steps: 8, steps per second: 504, episode reward: 11.375, mean reward: 1.422 [1.000, 3.125], mean action: 11.625 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9528/10000: episode: 1191, duration: 0.012s, episode steps: 8, steps per second: 657, episode reward: 8.500, mean reward: 1.062 [1.000, 1.500], mean action: 9.875 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9536/10000: episode: 1192, duration: 0.013s, episode steps: 8, steps per second: 624, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 15.000 [5.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9544/10000: episode: 1193, duration: 0.012s, episode steps: 8, steps per second: 649, episode reward: -110.000, mean reward: -13.750 [-100.000, 2.000], mean action: 10.375 [1.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 9552/10000: episode: 1194, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 8.875 [1.000, 24.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 9560/10000: episode: 1195, duration: 0.018s, episode steps: 8, steps per second: 445, episode reward: -0.083, mean reward: -0.010 [-5.000, 2.667], mean action: 14.875 [1.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9568/10000: episode: 1196, duration: 0.015s, episode steps: 8, steps per second: 535, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 15.000 [4.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9576/10000: episode: 1197, duration: 0.022s, episode steps: 8, steps per second: 370, episode reward: 10.500, mean reward: 1.312 [-5.000, 4.167], mean action: 8.875 [1.000, 22.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9584/10000: episode: 1198, duration: 0.015s, episode steps: 8, steps per second: 522, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 10.750 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9592/10000: episode: 1199, duration: 0.014s, episode steps: 8, steps per second: 583, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 9.625 [2.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 9600/10000: episode: 1200, duration: 0.020s, episode steps: 8, steps per second: 396, episode reward: -103.000, mean reward: -12.875 [-100.000, 2.000], mean action: 14.500 [6.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 9608/10000: episode: 1201, duration: 0.021s, episode steps: 8, steps per second: 383, episode reward: -0.500, mean reward: -0.062 [-5.000, 2.250], mean action: 7.500 [0.000, 13.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: 17.791667\n",
      " 9616/10000: episode: 1202, duration: 0.020s, episode steps: 8, steps per second: 392, episode reward: 0.000, mean reward: 0.000 [-5.000, 3.000], mean action: 11.375 [4.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9624/10000: episode: 1203, duration: 0.034s, episode steps: 8, steps per second: 233, episode reward: 0.667, mean reward: 0.083 [-5.000, 3.000], mean action: 11.875 [3.000, 18.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 9632/10000: episode: 1204, duration: 0.031s, episode steps: 8, steps per second: 260, episode reward: 5.833, mean reward: 0.729 [-5.000, 2.667], mean action: 12.500 [2.000, 22.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9640/10000: episode: 1205, duration: 0.027s, episode steps: 8, steps per second: 298, episode reward: -10.000, mean reward: -1.250 [-5.000, 1.000], mean action: 15.375 [0.000, 23.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 9648/10000: episode: 1206, duration: 0.015s, episode steps: 8, steps per second: 524, episode reward: 5.917, mean reward: 0.740 [-5.000, 2.667], mean action: 10.875 [3.000, 23.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9656/10000: episode: 1207, duration: 0.026s, episode steps: 8, steps per second: 312, episode reward: 11.944, mean reward: 1.493 [1.000, 2.083], mean action: 12.000 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9664/10000: episode: 1208, duration: 0.017s, episode steps: 8, steps per second: 474, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 7.250 [0.000, 19.000], mean observation: 0.092 [0.000, 1.000], mean_best_reward: --\n",
      " 9672/10000: episode: 1209, duration: 0.016s, episode steps: 8, steps per second: 492, episode reward: 13.883, mean reward: 1.735 [1.000, 2.450], mean action: 13.500 [6.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9680/10000: episode: 1210, duration: 0.016s, episode steps: 8, steps per second: 513, episode reward: 13.000, mean reward: 1.625 [1.000, 2.250], mean action: 12.375 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9688/10000: episode: 1211, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 10.875 [0.000, 25.000], mean observation: 0.077 [0.000, 1.000], mean_best_reward: --\n",
      " 9696/10000: episode: 1212, duration: 0.016s, episode steps: 8, steps per second: 514, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 14.125 [3.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9704/10000: episode: 1213, duration: 0.013s, episode steps: 8, steps per second: 623, episode reward: 23.417, mean reward: 2.927 [1.000, 4.500], mean action: 16.375 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9712/10000: episode: 1214, duration: 0.017s, episode steps: 8, steps per second: 475, episode reward: -9.667, mean reward: -1.208 [-5.000, 1.333], mean action: 12.625 [4.000, 20.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9720/10000: episode: 1215, duration: 0.014s, episode steps: 8, steps per second: 556, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 12.375 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9728/10000: episode: 1216, duration: 0.014s, episode steps: 8, steps per second: 555, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 7.125 [0.000, 15.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 9736/10000: episode: 1217, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 11.750 [3.000, 23.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 9744/10000: episode: 1218, duration: 0.013s, episode steps: 8, steps per second: 608, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 15.625 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9752/10000: episode: 1219, duration: 0.016s, episode steps: 8, steps per second: 488, episode reward: 13.567, mean reward: 1.696 [1.000, 2.450], mean action: 13.125 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9760/10000: episode: 1220, duration: 0.020s, episode steps: 8, steps per second: 410, episode reward: 6.667, mean reward: 0.833 [-5.000, 2.083], mean action: 9.750 [3.000, 20.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 9768/10000: episode: 1221, duration: 0.024s, episode steps: 8, steps per second: 332, episode reward: -98.000, mean reward: -12.250 [-100.000, 1.500], mean action: 14.750 [1.000, 25.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 9776/10000: episode: 1222, duration: 0.021s, episode steps: 8, steps per second: 374, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 13.375 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9784/10000: episode: 1223, duration: 0.017s, episode steps: 8, steps per second: 484, episode reward: 0.062, mean reward: 0.008 [-5.000, 2.250], mean action: 8.375 [2.000, 16.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9792/10000: episode: 1224, duration: 0.021s, episode steps: 8, steps per second: 381, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 17.625 [5.000, 22.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 9800/10000: episode: 1225, duration: 0.032s, episode steps: 8, steps per second: 249, episode reward: 15.111, mean reward: 1.889 [1.000, 3.000], mean action: 9.375 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9808/10000: episode: 1226, duration: 0.021s, episode steps: 8, steps per second: 379, episode reward: -92.000, mean reward: -11.500 [-100.000, 1.500], mean action: 12.000 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 9816/10000: episode: 1227, duration: 0.022s, episode steps: 8, steps per second: 360, episode reward: 2.000, mean reward: 0.250 [-5.000, 1.000], mean action: 13.375 [0.000, 24.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9824/10000: episode: 1228, duration: 0.022s, episode steps: 8, steps per second: 370, episode reward: 5.000, mean reward: 0.625 [-5.000, 3.000], mean action: 8.875 [0.000, 19.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9832/10000: episode: 1229, duration: 0.017s, episode steps: 8, steps per second: 476, episode reward: 4.833, mean reward: 0.604 [-5.000, 2.000], mean action: 11.250 [0.000, 24.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9840/10000: episode: 1230, duration: 0.018s, episode steps: 8, steps per second: 438, episode reward: -1.500, mean reward: -0.188 [-5.000, 2.000], mean action: 10.125 [1.000, 19.000], mean observation: 0.083 [0.000, 1.000], mean_best_reward: --\n",
      " 9848/10000: episode: 1231, duration: 0.016s, episode steps: 8, steps per second: 491, episode reward: -199.000, mean reward: -24.875 [-100.000, 2.000], mean action: 16.875 [2.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9856/10000: episode: 1232, duration: 0.021s, episode steps: 8, steps per second: 378, episode reward: -395.000, mean reward: -49.375 [-100.000, 2.000], mean action: 15.250 [1.000, 25.000], mean observation: 0.075 [0.000, 1.000], mean_best_reward: --\n",
      " 9864/10000: episode: 1233, duration: 0.013s, episode steps: 8, steps per second: 597, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 9.875 [1.000, 21.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9872/10000: episode: 1234, duration: 0.015s, episode steps: 8, steps per second: 519, episode reward: -0.833, mean reward: -0.104 [-5.000, 2.667], mean action: 8.750 [1.000, 23.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9880/10000: episode: 1235, duration: 0.014s, episode steps: 8, steps per second: 585, episode reward: 4.861, mean reward: 0.608 [-5.000, 2.083], mean action: 16.375 [8.000, 23.000], mean observation: 0.098 [0.000, 1.000], mean_best_reward: --\n",
      " 9888/10000: episode: 1236, duration: 0.016s, episode steps: 8, steps per second: 496, episode reward: 14.444, mean reward: 1.806 [1.000, 3.000], mean action: 12.750 [5.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9896/10000: episode: 1237, duration: 0.015s, episode steps: 8, steps per second: 550, episode reward: -90.500, mean reward: -11.312 [-100.000, 2.083], mean action: 14.500 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], mean_best_reward: --\n",
      " 9904/10000: episode: 1238, duration: 0.014s, episode steps: 8, steps per second: 554, episode reward: -1.500, mean reward: -0.188 [-5.000, 2.000], mean action: 16.125 [8.000, 24.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9912/10000: episode: 1239, duration: 0.013s, episode steps: 8, steps per second: 607, episode reward: 14.083, mean reward: 1.760 [-5.000, 4.167], mean action: 16.375 [8.000, 22.000], mean observation: 0.100 [0.000, 1.000], mean_best_reward: --\n",
      " 9920/10000: episode: 1240, duration: 0.013s, episode steps: 8, steps per second: 637, episode reward: -3.000, mean reward: -0.375 [-5.000, 1.500], mean action: 15.625 [2.000, 24.000], mean observation: 0.087 [0.000, 1.000], mean_best_reward: --\n",
      " 9928/10000: episode: 1241, duration: 0.013s, episode steps: 8, steps per second: 640, episode reward: 10.167, mean reward: 1.271 [1.000, 2.667], mean action: 8.125 [1.000, 18.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9936/10000: episode: 1242, duration: 0.019s, episode steps: 8, steps per second: 426, episode reward: -1.500, mean reward: -0.188 [-5.000, 2.250], mean action: 7.250 [0.000, 17.000], mean observation: 0.102 [0.000, 1.000], mean_best_reward: --\n",
      " 9944/10000: episode: 1243, duration: 0.012s, episode steps: 8, steps per second: 654, episode reward: -211.000, mean reward: -26.375 [-100.000, 2.000], mean action: 17.625 [1.000, 25.000], mean observation: 0.062 [0.000, 1.000], mean_best_reward: --\n",
      " 9952/10000: episode: 1244, duration: 0.016s, episode steps: 8, steps per second: 486, episode reward: 14.883, mean reward: 1.860 [1.000, 3.000], mean action: 17.125 [9.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9960/10000: episode: 1245, duration: 0.018s, episode steps: 8, steps per second: 457, episode reward: -2.917, mean reward: -0.365 [-5.000, 4.167], mean action: 8.750 [2.000, 24.000], mean observation: 0.085 [0.000, 1.000], mean_best_reward: --\n",
      " 9968/10000: episode: 1246, duration: 0.014s, episode steps: 8, steps per second: 580, episode reward: -200.000, mean reward: -25.000 [-100.000, 1.000], mean action: 11.125 [1.000, 25.000], mean observation: 0.090 [0.000, 1.000], mean_best_reward: --\n",
      " 9976/10000: episode: 1247, duration: 0.015s, episode steps: 8, steps per second: 546, episode reward: 11.500, mean reward: 1.438 [1.000, 2.250], mean action: 12.750 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9984/10000: episode: 1248, duration: 0.017s, episode steps: 8, steps per second: 480, episode reward: 9.778, mean reward: 1.222 [1.000, 2.000], mean action: 14.125 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      " 9992/10000: episode: 1249, duration: 0.014s, episode steps: 8, steps per second: 555, episode reward: 9.500, mean reward: 1.188 [1.000, 2.000], mean action: 9.375 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10000/10000: episode: 1250, duration: 0.013s, episode steps: 8, steps per second: 599, episode reward: 13.250, mean reward: 1.656 [1.000, 4.000], mean action: 12.125 [2.000, 23.000], mean observation: 0.107 [0.000, 1.000], mean_best_reward: --\n",
      "done, took 19.988 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c9577b50>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=1000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "#cem.fit(env, nb_steps=10000, visualize=True, verbose=2)\n",
    "cem.fit(env, nb_steps=10000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prupes/Documents/Personal/Research/Geometry/gym-pack/gym_pack/envs/pack_env.py:150: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: -34.000, steps: 8\n",
      "Episode 2: reward: -34.000, steps: 8\n",
      "Episode 3: reward: -34.000, steps: 8\n",
      "Episode 4: reward: -34.000, steps: 8\n",
      "Episode 5: reward: -34.000, steps: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x3779ba650>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x11c6a18c0> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpost_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpost_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# IPython >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf_mgr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1709\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks_to_draw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m             \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;31m# scale up the axis label box to also find the neighbors, not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    295\u001b[0m         for artist in [self.gridline, self.tick1line, self.tick2line,\n\u001b[1;32m    296\u001b[0m                        self.label1, self.label2]:\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 self.get_markeredgecolor(), self._alpha)\n\u001b[1;32m    813\u001b[0m             fc_rgba = mcolors.to_rgba(\n\u001b[0;32m--> 814\u001b[0;31m                 self._get_markerfacecolor(), self._alpha)\n\u001b[0m\u001b[1;32m    815\u001b[0m             fcalt_rgba = mcolors.to_rgba(\n\u001b[1;32m    816\u001b[0m                 self._get_markerfacecolor(alt=True), self._alpha)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36m_get_markerfacecolor\u001b[0;34m(self, alt)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_markerfacecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_markerfacecoloralt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0malt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_markerfacecolor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str_lower_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fillstyle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x11c767c20> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2059\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n\u001b[0;32m-> 2060\u001b[0;31m                             bbox_extra_artists=bbox_artists)\n\u001b[0m\u001b[1;32m   2061\u001b[0m                     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pad_inches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   2376\u001b[0m                     bbox = ax.get_tightbbox(renderer,\n\u001b[0;32m-> 2377\u001b[0;31m                             bbox_extra_artists=bbox_extra_artists)\n\u001b[0m\u001b[1;32m   2378\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4359\u001b[0;31m             \u001b[0mbb_yaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4360\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbb_yaxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \"\"\"\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m         \u001b[0mmajor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_majorticklocs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;34m\"\"\"Get the array of major tick locations in data coordinates.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2077\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2078\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mtick_values\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2085\u001b[0m             vmin, vmax, expander=1e-13, tiny=1e-14)\n\u001b[0;32m-> 2086\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m_raw_ticks\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2024\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m                 nbins = np.clip(self.axis.get_tick_space(),\n\u001b[0m\u001b[1;32m   2026\u001b[0m                                 max(1, self._min_n_ticks - 1), 9)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tick_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m         \u001b[0mtick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m         \u001b[0;31m# Having a spacing of at least 2 just looks good.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   2221\u001b[0m             \u001b[0mtick_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mYTick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, axes, loc, label, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kw)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick1line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick1line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick2line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick2line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick1line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m                           \u001b[0mmarkeredgewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                           zorder=self._zorder)\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_yaxis_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tick1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# chance to init axes (and hence unit support)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickradius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_setattr_cm\u001b[0;34m(obj, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2013\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# close triggers gc.collect, which can be slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0m_pylab_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0m_pylab_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0m_pylab_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdestroy_all\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpl_disconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cidgcf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mmpl_disconnect\u001b[0;34m(self, cid)\u001b[0m\n\u001b[1;32m   2199\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpl_disconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m         \"\"\"\n\u001b[0;32m-> 2201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mdisconnect\u001b[0;34m(self, cid)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"Disconnect the callback registered with callback id *cid*.\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0meventname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbackd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mcallbackd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# After training is done, we save the best weights.\n",
    "\n",
    "#cem.save_weights('cem_{}_params.h5f'.format(\"TICTAC\"), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]]), array([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "1.0\n",
      "23.04\n",
      "23.04\n",
      "23.04\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "for action in range(env.num_possible_moves+1):\n",
    "    print(env.calculate_reward(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]]), array([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before using it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4baeb76509c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mfit_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0mfit_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must compile your model before using it.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before using it."
     ]
    }
   ],
   "source": [
    "model.fit(env.return_state.reshape((1,1,)+ env.observation_space.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.rl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-2596a7e7eeff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.rl'"
     ]
    }
   ],
   "source": [
    "def build_model(state_shape, num_actions):\n",
    "    input = Input(shape=(state_shape))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output = Dense(num_actions, activation='linear')(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "    8/10000: episode: 1, duration: 0.761s, episode steps: 8, steps per second: 11, episode reward: 6.833, mean reward: 0.854 [-5.000, 2.667], mean action: 11.500 [3.000, 18.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16/10000: episode: 2, duration: 0.010s, episode steps: 8, steps per second: 835, episode reward: -103.000, mean reward: -12.875 [-100.000, 2.000], mean action: 11.625 [4.000, 25.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24/10000: episode: 3, duration: 0.010s, episode steps: 8, steps per second: 808, episode reward: 11.900, mean reward: 1.488 [1.000, 2.400], mean action: 11.000 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   32/10000: episode: 4, duration: 0.010s, episode steps: 8, steps per second: 770, episode reward: 11.806, mean reward: 1.476 [1.000, 2.250], mean action: 13.250 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40/10000: episode: 5, duration: 0.010s, episode steps: 8, steps per second: 779, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 14.625 [4.000, 23.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48/10000: episode: 6, duration: 0.009s, episode steps: 8, steps per second: 939, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 8.375 [1.000, 19.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   56/10000: episode: 7, duration: 0.008s, episode steps: 8, steps per second: 946, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 11.375 [4.000, 24.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   64/10000: episode: 8, duration: 0.011s, episode steps: 8, steps per second: 720, episode reward: 3.556, mean reward: 0.444 [-5.000, 1.778], mean action: 15.125 [7.000, 23.000], mean observation: 0.090 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   72/10000: episode: 9, duration: 0.009s, episode steps: 8, steps per second: 935, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 15.125 [8.000, 25.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   80/10000: episode: 10, duration: 0.009s, episode steps: 8, steps per second: 896, episode reward: 11.944, mean reward: 1.493 [1.000, 2.083], mean action: 9.125 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   88/10000: episode: 11, duration: 0.009s, episode steps: 8, steps per second: 856, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 12.250 [0.000, 23.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   96/10000: episode: 12, duration: 0.010s, episode steps: 8, steps per second: 806, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 12.375 [1.000, 24.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  104/10000: episode: 13, duration: 0.011s, episode steps: 8, steps per second: 731, episode reward: 11.167, mean reward: 1.396 [1.000, 2.667], mean action: 11.500 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  112/10000: episode: 14, duration: 0.010s, episode steps: 8, steps per second: 776, episode reward: -189.083, mean reward: -23.635 [-100.000, 2.667], mean action: 12.750 [3.000, 25.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  120/10000: episode: 15, duration: 0.009s, episode steps: 8, steps per second: 878, episode reward: 13.333, mean reward: 1.667 [1.000, 3.000], mean action: 14.000 [7.000, 22.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  128/10000: episode: 16, duration: 0.010s, episode steps: 8, steps per second: 823, episode reward: -91.750, mean reward: -11.469 [-100.000, 2.250], mean action: 16.500 [5.000, 25.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  136/10000: episode: 17, duration: 0.009s, episode steps: 8, steps per second: 903, episode reward: -100.333, mean reward: -12.542 [-100.000, 3.000], mean action: 16.875 [6.000, 25.000], mean observation: 0.090 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  144/10000: episode: 18, duration: 0.009s, episode steps: 8, steps per second: 903, episode reward: 11.250, mean reward: 1.406 [1.000, 2.250], mean action: 14.625 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  152/10000: episode: 19, duration: 0.009s, episode steps: 8, steps per second: 922, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 15.625 [3.000, 25.000], mean observation: 0.090 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  160/10000: episode: 20, duration: 0.010s, episode steps: 8, steps per second: 767, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 11.375 [1.000, 23.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  168/10000: episode: 21, duration: 0.009s, episode steps: 8, steps per second: 863, episode reward: -98.000, mean reward: -12.250 [-100.000, 1.500], mean action: 11.375 [1.000, 25.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  176/10000: episode: 22, duration: 0.012s, episode steps: 8, steps per second: 656, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 9.875 [2.000, 18.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  184/10000: episode: 23, duration: 0.009s, episode steps: 8, steps per second: 876, episode reward: 7.583, mean reward: 0.948 [-5.000, 2.667], mean action: 9.875 [1.000, 21.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  192/10000: episode: 24, duration: 0.009s, episode steps: 8, steps per second: 940, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 10.375 [3.000, 21.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  200/10000: episode: 25, duration: 0.010s, episode steps: 8, steps per second: 788, episode reward: -88.750, mean reward: -11.094 [-100.000, 2.250], mean action: 13.625 [3.000, 25.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  208/10000: episode: 26, duration: 0.009s, episode steps: 8, steps per second: 869, episode reward: -96.333, mean reward: -12.042 [-100.000, 2.667], mean action: 16.125 [0.000, 25.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  216/10000: episode: 27, duration: 0.009s, episode steps: 8, steps per second: 906, episode reward: -91.000, mean reward: -11.375 [-100.000, 2.000], mean action: 12.875 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  224/10000: episode: 28, duration: 0.009s, episode steps: 8, steps per second: 935, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 12.375 [2.000, 25.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  232/10000: episode: 29, duration: 0.009s, episode steps: 8, steps per second: 881, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 15.875 [1.000, 23.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  240/10000: episode: 30, duration: 0.010s, episode steps: 8, steps per second: 819, episode reward: -192.500, mean reward: -24.062 [-100.000, 2.000], mean action: 12.625 [0.000, 25.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  248/10000: episode: 31, duration: 0.009s, episode steps: 8, steps per second: 898, episode reward: 12.500, mean reward: 1.562 [1.000, 2.500], mean action: 12.875 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  256/10000: episode: 32, duration: 0.010s, episode steps: 8, steps per second: 781, episode reward: -9.667, mean reward: -1.208 [-5.000, 1.333], mean action: 9.625 [4.000, 20.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  264/10000: episode: 33, duration: 0.008s, episode steps: 8, steps per second: 950, episode reward: 10.167, mean reward: 1.271 [1.000, 2.000], mean action: 12.750 [0.000, 23.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  272/10000: episode: 34, duration: 0.012s, episode steps: 8, steps per second: 684, episode reward: -94.000, mean reward: -11.750 [-100.000, 3.000], mean action: 14.875 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  280/10000: episode: 35, duration: 0.010s, episode steps: 8, steps per second: 791, episode reward: 5.167, mean reward: 0.646 [-5.000, 2.083], mean action: 7.375 [1.000, 18.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  288/10000: episode: 36, duration: 0.010s, episode steps: 8, steps per second: 834, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 7.875 [0.000, 20.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  296/10000: episode: 37, duration: 0.009s, episode steps: 8, steps per second: 868, episode reward: -87.417, mean reward: -10.927 [-100.000, 2.667], mean action: 13.000 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  304/10000: episode: 38, duration: 0.010s, episode steps: 8, steps per second: 784, episode reward: -6.222, mean reward: -0.778 [-5.000, 2.778], mean action: 12.500 [6.000, 18.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  312/10000: episode: 39, duration: 0.009s, episode steps: 8, steps per second: 923, episode reward: 6.333, mean reward: 0.792 [-5.000, 2.667], mean action: 12.375 [5.000, 22.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  320/10000: episode: 40, duration: 0.010s, episode steps: 8, steps per second: 841, episode reward: 6.500, mean reward: 0.812 [-5.000, 3.000], mean action: 12.375 [4.000, 21.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  328/10000: episode: 41, duration: 0.009s, episode steps: 8, steps per second: 857, episode reward: 11.250, mean reward: 1.406 [1.000, 2.250], mean action: 9.625 [3.000, 21.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  336/10000: episode: 42, duration: 0.011s, episode steps: 8, steps per second: 696, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 12.000 [0.000, 23.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  344/10000: episode: 43, duration: 0.011s, episode steps: 8, steps per second: 760, episode reward: 13.000, mean reward: 1.625 [-5.000, 4.167], mean action: 15.625 [3.000, 24.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  352/10000: episode: 44, duration: 0.008s, episode steps: 8, steps per second: 952, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 13.750 [4.000, 25.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  360/10000: episode: 45, duration: 0.008s, episode steps: 8, steps per second: 1015, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 13.500 [5.000, 25.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  368/10000: episode: 46, duration: 0.009s, episode steps: 8, steps per second: 909, episode reward: -95.556, mean reward: -11.944 [-100.000, 2.083], mean action: 8.625 [2.000, 25.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  376/10000: episode: 47, duration: 0.010s, episode steps: 8, steps per second: 805, episode reward: 10.167, mean reward: 1.271 [1.000, 2.667], mean action: 10.500 [0.000, 25.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  384/10000: episode: 48, duration: 0.008s, episode steps: 8, steps per second: 952, episode reward: 10.250, mean reward: 1.281 [1.000, 2.250], mean action: 13.375 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  392/10000: episode: 49, duration: 0.009s, episode steps: 8, steps per second: 912, episode reward: -2.000, mean reward: -0.250 [-5.000, 2.000], mean action: 9.125 [2.000, 20.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  400/10000: episode: 50, duration: 0.011s, episode steps: 8, steps per second: 733, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 9.000 [1.000, 18.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  408/10000: episode: 51, duration: 0.009s, episode steps: 8, steps per second: 874, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 16.125 [0.000, 24.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  416/10000: episode: 52, duration: 0.009s, episode steps: 8, steps per second: 941, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 17.875 [0.000, 24.000], mean observation: 0.090 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  424/10000: episode: 53, duration: 0.010s, episode steps: 8, steps per second: 794, episode reward: -4.000, mean reward: -0.500 [-5.000, 1.000], mean action: 12.500 [1.000, 22.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  432/10000: episode: 54, duration: 0.008s, episode steps: 8, steps per second: 944, episode reward: -8.500, mean reward: -1.062 [-5.000, 2.000], mean action: 14.750 [9.000, 22.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  440/10000: episode: 55, duration: 0.010s, episode steps: 8, steps per second: 762, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 10.625 [1.000, 25.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  448/10000: episode: 56, duration: 0.009s, episode steps: 8, steps per second: 869, episode reward: 3.556, mean reward: 0.444 [-5.000, 1.778], mean action: 9.500 [0.000, 20.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  456/10000: episode: 57, duration: 0.009s, episode steps: 8, steps per second: 916, episode reward: 12.833, mean reward: 1.604 [1.000, 2.667], mean action: 11.125 [4.000, 23.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  464/10000: episode: 58, duration: 0.009s, episode steps: 8, steps per second: 911, episode reward: 12.000, mean reward: 1.500 [1.000, 3.000], mean action: 12.875 [0.000, 22.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  472/10000: episode: 59, duration: 0.009s, episode steps: 8, steps per second: 861, episode reward: -191.667, mean reward: -23.958 [-100.000, 2.250], mean action: 14.625 [6.000, 25.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  480/10000: episode: 60, duration: 0.010s, episode steps: 8, steps per second: 827, episode reward: 18.492, mean reward: 2.311 [1.000, 3.600], mean action: 12.000 [0.000, 20.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  488/10000: episode: 61, duration: 0.009s, episode steps: 8, steps per second: 850, episode reward: -95.500, mean reward: -11.938 [-100.000, 2.250], mean action: 11.000 [1.000, 25.000], mean observation: 0.087 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  496/10000: episode: 62, duration: 0.010s, episode steps: 8, steps per second: 782, episode reward: -1.750, mean reward: -0.219 [-5.000, 2.250], mean action: 12.875 [2.000, 20.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  504/10000: episode: 63, duration: 0.011s, episode steps: 8, steps per second: 716, episode reward: -205.000, mean reward: -25.625 [-100.000, 2.000], mean action: 7.750 [0.000, 25.000], mean observation: 0.070 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  512/10000: episode: 64, duration: 0.009s, episode steps: 8, steps per second: 941, episode reward: 10.833, mean reward: 1.354 [1.000, 2.000], mean action: 12.125 [1.000, 20.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  520/10000: episode: 65, duration: 0.009s, episode steps: 8, steps per second: 916, episode reward: 4.500, mean reward: 0.562 [-5.000, 2.000], mean action: 10.250 [2.000, 22.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  528/10000: episode: 66, duration: 0.010s, episode steps: 8, steps per second: 788, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.000], mean action: 12.750 [3.000, 21.000], mean observation: 0.083 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  536/10000: episode: 67, duration: 0.009s, episode steps: 8, steps per second: 903, episode reward: 13.583, mean reward: 1.698 [1.000, 2.667], mean action: 12.250 [1.000, 22.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  544/10000: episode: 68, duration: 0.009s, episode steps: 8, steps per second: 904, episode reward: 3.000, mean reward: 0.375 [-5.000, 2.000], mean action: 9.375 [0.000, 19.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  552/10000: episode: 69, duration: 0.009s, episode steps: 8, steps per second: 934, episode reward: 4.000, mean reward: 0.500 [-5.000, 2.000], mean action: 12.375 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  560/10000: episode: 70, duration: 0.010s, episode steps: 8, steps per second: 799, episode reward: 19.694, mean reward: 2.462 [1.000, 4.000], mean action: 9.000 [1.000, 23.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  568/10000: episode: 71, duration: 0.009s, episode steps: 8, steps per second: 915, episode reward: -1.833, mean reward: -0.229 [-5.000, 2.667], mean action: 15.625 [5.000, 24.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  576/10000: episode: 72, duration: 0.009s, episode steps: 8, steps per second: 878, episode reward: -2.750, mean reward: -0.344 [-5.000, 2.250], mean action: 11.375 [7.000, 24.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  584/10000: episode: 73, duration: 0.008s, episode steps: 8, steps per second: 956, episode reward: 5.100, mean reward: 0.637 [-5.000, 2.000], mean action: 9.875 [0.000, 18.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  592/10000: episode: 74, duration: 0.008s, episode steps: 8, steps per second: 966, episode reward: -295.000, mean reward: -36.875 [-100.000, 1.000], mean action: 16.875 [4.000, 25.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  600/10000: episode: 75, duration: 0.010s, episode steps: 8, steps per second: 774, episode reward: 2.375, mean reward: 0.297 [-5.000, 3.125], mean action: 9.250 [3.000, 21.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  608/10000: episode: 76, duration: 0.010s, episode steps: 8, steps per second: 792, episode reward: 11.056, mean reward: 1.382 [1.000, 2.000], mean action: 13.500 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  616/10000: episode: 77, duration: 0.009s, episode steps: 8, steps per second: 873, episode reward: -3.000, mean reward: -0.375 [-5.000, 2.000], mean action: 6.875 [2.000, 17.000], mean observation: 0.087 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  624/10000: episode: 78, duration: 0.009s, episode steps: 8, steps per second: 907, episode reward: 5.000, mean reward: 0.625 [-5.000, 2.000], mean action: 13.500 [6.000, 20.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  632/10000: episode: 79, duration: 0.008s, episode steps: 8, steps per second: 987, episode reward: -111.000, mean reward: -13.875 [-100.000, 1.000], mean action: 13.250 [5.000, 25.000], mean observation: 0.077 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  640/10000: episode: 80, duration: 0.009s, episode steps: 8, steps per second: 868, episode reward: -205.000, mean reward: -25.625 [-100.000, 2.000], mean action: 13.500 [0.000, 25.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  648/10000: episode: 81, duration: 0.009s, episode steps: 8, steps per second: 882, episode reward: -7.000, mean reward: -0.875 [-5.000, 2.500], mean action: 12.000 [4.000, 23.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  656/10000: episode: 82, duration: 0.009s, episode steps: 8, steps per second: 901, episode reward: 5.250, mean reward: 0.656 [-5.000, 2.250], mean action: 8.125 [3.000, 20.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  664/10000: episode: 83, duration: 0.009s, episode steps: 8, steps per second: 925, episode reward: -199.000, mean reward: -24.875 [-100.000, 2.000], mean action: 12.125 [0.000, 25.000], mean observation: 0.087 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  672/10000: episode: 84, duration: 0.010s, episode steps: 8, steps per second: 816, episode reward: -15.000, mean reward: -1.875 [-5.000, 2.000], mean action: 6.000 [0.000, 19.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  680/10000: episode: 85, duration: 0.013s, episode steps: 8, steps per second: 609, episode reward: -6.000, mean reward: -0.750 [-5.000, 3.000], mean action: 7.875 [3.000, 13.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  688/10000: episode: 86, duration: 0.011s, episode steps: 8, steps per second: 738, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 12.500 [3.000, 23.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  696/10000: episode: 87, duration: 0.011s, episode steps: 8, steps per second: 697, episode reward: 10.500, mean reward: 1.312 [1.000, 2.000], mean action: 11.750 [2.000, 22.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  704/10000: episode: 88, duration: 0.008s, episode steps: 8, steps per second: 972, episode reward: 7.583, mean reward: 0.948 [-5.000, 2.667], mean action: 7.500 [0.000, 17.000], mean observation: 0.090 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  712/10000: episode: 89, duration: 0.014s, episode steps: 8, steps per second: 573, episode reward: 12.500, mean reward: 1.562 [1.000, 2.250], mean action: 14.750 [1.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  720/10000: episode: 90, duration: 0.008s, episode steps: 8, steps per second: 963, episode reward: -9.500, mean reward: -1.188 [-5.000, 1.500], mean action: 14.000 [8.000, 23.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  728/10000: episode: 91, duration: 0.008s, episode steps: 8, steps per second: 998, episode reward: -1.500, mean reward: -0.188 [-5.000, 2.000], mean action: 6.625 [1.000, 21.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  736/10000: episode: 92, duration: 0.009s, episode steps: 8, steps per second: 886, episode reward: 11.056, mean reward: 1.382 [1.000, 2.000], mean action: 15.750 [3.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  744/10000: episode: 93, duration: 0.009s, episode steps: 8, steps per second: 911, episode reward: 9.778, mean reward: 1.222 [1.000, 2.000], mean action: 14.875 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  752/10000: episode: 94, duration: 0.012s, episode steps: 8, steps per second: 658, episode reward: 6.667, mean reward: 0.833 [-5.000, 3.000], mean action: 7.125 [2.000, 20.000], mean observation: 0.098 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  760/10000: episode: 95, duration: 0.009s, episode steps: 8, steps per second: 850, episode reward: 2.500, mean reward: 0.312 [-5.000, 1.500], mean action: 12.500 [3.000, 23.000], mean observation: 0.092 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  768/10000: episode: 96, duration: 0.010s, episode steps: 8, steps per second: 802, episode reward: 15.867, mean reward: 1.983 [1.000, 3.600], mean action: 17.875 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  776/10000: episode: 97, duration: 0.009s, episode steps: 8, steps per second: 900, episode reward: 0.000, mean reward: 0.000 [-5.000, 3.000], mean action: 9.125 [2.000, 19.000], mean observation: 0.085 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  784/10000: episode: 98, duration: 0.009s, episode steps: 8, steps per second: 930, episode reward: -90.944, mean reward: -11.368 [-100.000, 1.778], mean action: 13.250 [5.000, 25.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  792/10000: episode: 99, duration: 0.010s, episode steps: 8, steps per second: 812, episode reward: 9.861, mean reward: 1.233 [1.000, 2.083], mean action: 11.375 [2.000, 20.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  800/10000: episode: 100, duration: 0.008s, episode steps: 8, steps per second: 961, episode reward: 0.000, mean reward: 0.000 [-5.000, 2.500], mean action: 14.375 [4.000, 23.000], mean observation: 0.075 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  808/10000: episode: 101, duration: 0.010s, episode steps: 8, steps per second: 842, episode reward: 3.917, mean reward: 0.490 [-5.000, 2.083], mean action: 13.500 [2.000, 20.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  816/10000: episode: 102, duration: 0.010s, episode steps: 8, steps per second: 819, episode reward: -1.000, mean reward: -0.125 [-5.000, 3.000], mean action: 16.875 [11.000, 24.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  824/10000: episode: 103, duration: 0.010s, episode steps: 8, steps per second: 780, episode reward: 9.000, mean reward: 1.125 [1.000, 1.500], mean action: 12.500 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  832/10000: episode: 104, duration: 0.009s, episode steps: 8, steps per second: 917, episode reward: -16.000, mean reward: -2.000 [-5.000, 1.000], mean action: 10.250 [0.000, 20.000], mean observation: 0.083 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  840/10000: episode: 105, duration: 0.009s, episode steps: 8, steps per second: 909, episode reward: -1.500, mean reward: -0.188 [-5.000, 2.250], mean action: 15.000 [3.000, 24.000], mean observation: 0.087 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  848/10000: episode: 106, duration: 0.008s, episode steps: 8, steps per second: 947, episode reward: -90.500, mean reward: -11.312 [-100.000, 2.000], mean action: 12.125 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  856/10000: episode: 107, duration: 0.009s, episode steps: 8, steps per second: 856, episode reward: -116.000, mean reward: -14.500 [-100.000, 2.000], mean action: 14.625 [9.000, 25.000], mean observation: 0.065 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  864/10000: episode: 108, duration: 0.008s, episode steps: 8, steps per second: 968, episode reward: -9.000, mean reward: -1.125 [-5.000, 2.000], mean action: 6.750 [0.000, 21.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  872/10000: episode: 109, duration: 0.019s, episode steps: 8, steps per second: 426, episode reward: 8.917, mean reward: 1.115 [-5.000, 3.125], mean action: 16.750 [8.000, 23.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  880/10000: episode: 110, duration: 0.011s, episode steps: 8, steps per second: 714, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 15.250 [4.000, 25.000], mean observation: 0.090 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  888/10000: episode: 111, duration: 0.011s, episode steps: 8, steps per second: 760, episode reward: 10.667, mean reward: 1.333 [1.000, 2.083], mean action: 9.750 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  896/10000: episode: 112, duration: 0.011s, episode steps: 8, steps per second: 708, episode reward: -1.000, mean reward: -0.125 [-5.000, 2.083], mean action: 13.000 [5.000, 24.000], mean observation: 0.080 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  904/10000: episode: 113, duration: 0.009s, episode steps: 8, steps per second: 885, episode reward: 11.000, mean reward: 1.375 [1.000, 2.000], mean action: 12.000 [0.000, 21.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  912/10000: episode: 114, duration: 0.009s, episode steps: 8, steps per second: 862, episode reward: 14.083, mean reward: 1.760 [1.000, 3.000], mean action: 14.000 [2.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  920/10000: episode: 115, duration: 0.010s, episode steps: 8, steps per second: 787, episode reward: 13.050, mean reward: 1.631 [1.000, 2.400], mean action: 14.250 [3.000, 23.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  928/10000: episode: 116, duration: 0.009s, episode steps: 8, steps per second: 886, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 11.250 [0.000, 24.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  936/10000: episode: 117, duration: 0.013s, episode steps: 8, steps per second: 603, episode reward: -88.639, mean reward: -11.080 [-100.000, 2.250], mean action: 14.250 [7.000, 25.000], mean observation: 0.087 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  944/10000: episode: 118, duration: 0.013s, episode steps: 8, steps per second: 602, episode reward: 11.444, mean reward: 1.431 [1.000, 2.083], mean action: 8.250 [1.000, 21.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  952/10000: episode: 119, duration: 0.010s, episode steps: 8, steps per second: 800, episode reward: -0.875, mean reward: -0.109 [-5.000, 3.125], mean action: 16.250 [0.000, 23.000], mean observation: 0.102 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  960/10000: episode: 120, duration: 0.014s, episode steps: 8, steps per second: 555, episode reward: 10.000, mean reward: 1.250 [1.000, 2.000], mean action: 15.250 [3.000, 25.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  968/10000: episode: 121, duration: 0.016s, episode steps: 8, steps per second: 492, episode reward: -90.000, mean reward: -11.250 [-100.000, 2.000], mean action: 10.625 [0.000, 25.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  976/10000: episode: 122, duration: 0.013s, episode steps: 8, steps per second: 639, episode reward: 12.500, mean reward: 1.562 [1.000, 2.500], mean action: 14.375 [2.000, 20.000], mean observation: 0.107 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  984/10000: episode: 123, duration: 0.010s, episode steps: 8, steps per second: 773, episode reward: -105.000, mean reward: -13.125 [-100.000, 1.000], mean action: 14.875 [4.000, 25.000], mean observation: 0.087 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  992/10000: episode: 124, duration: 0.010s, episode steps: 8, steps per second: 779, episode reward: -92.000, mean reward: -11.500 [-100.000, 2.000], mean action: 11.000 [2.000, 25.000], mean observation: 0.100 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 1000/10000: episode: 125, duration: 0.014s, episode steps: 8, steps per second: 584, episode reward: -104.000, mean reward: -13.000 [-100.000, 2.000], mean action: 12.000 [2.000, 25.000], mean observation: 0.095 [0.000, 1.000], loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-ed551bea2385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# slows down training quite a lot. You can always safely abort the training prematurely using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Ctrl + C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;31m# Force a terminal state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, reward, terminal)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_steps_warmup\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Start by extracting the necessary parameters (we use a vectorized implementation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=26, memory=memory, nb_steps_warmup=1000,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -34.000, steps: 8\n",
      "Episode 2: reward: -34.000, steps: 8\n",
      "Episode 3: reward: -34.000, steps: 8\n",
      "Episode 4: reward: -34.000, steps: 8\n",
      "Episode 5: reward: -34.000, steps: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2bf111490>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAImUlEQVR4nO3dz4tbhR6G8fe904qCF1w0C+mUOy5EKMKtEIrQXUGoP9CtBV0J3VyhgiC69B8QN26KihcURdCFFC9SsCKCtxq1im0VivRiRWiKiLpRqu9dJIsqneYkPSdn8uX5wMBkEpKXMk9PcjLMOIkA1PG3vgcAaBdRA8UQNVAMUQPFEDVQzLYu7nTHjh3Z2Njo4q4BSDp37pwuXrzoK13XSdQbGxsajUZd3DUAScPhcNPrePoNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0yhq2wdsf237rO0nux4FYHEzo7a9Juk5SXdL2i3poO3dXQ8DsJgmR+q9ks4m+SbJb5Jek/RAt7MALKpJ1DslfXvZ5fPTr/2J7UO2R7ZH4/G4rX0A5tTaibIkR5IMkwwHg0FbdwtgTk2i/k7Srssur0+/BmALahL1x5JutX2L7eskPSjprW5nAVjUzF/mn+SS7UclvSNpTdKLSU51vgzAQhr9hY4kb0t6u+MtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVDMzKhtv2j7gu0vlzEIwLVpcqR+SdKBjncAaMnMqJO8L+mHJWwB0AJeUwPFtBa17UO2R7ZH4/G4rbsFMKfWok5yJMkwyXAwGLR1twDmxNNvoJgmb2m9KulDSbfZPm/7ke5nAVjUtlk3SHJwGUMAtIOn30AxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDEzo7a9y/Zx26dtn7J9eBnDACxmW4PbXJL0eJJPbf9d0ie2jyU53fE2AAuYeaRO8n2ST6ef/yzpjKSdXQ8DsJi5XlPb3pB0h6QTV7jukO2R7dF4PG5nHYC5NY7a9o2S3pD0WJKf/np9kiNJhkmGg8GgzY0A5tAoatvbNQn6lSRvdjsJwLVocvbbkl6QdCbJM91PAnAtmhyp90l6WNJ+2yenH/d0vAvAgma+pZXkA0lewhYALeAnyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKGZm1Lavt/2R7c9tn7L99DKGAVjMtga3+VXS/iS/2N4u6QPb/0ny3463AVjAzKiTRNIv04vbpx/pchSAxTV6TW17zfZJSRckHUtyottZABbVKOokvyfZI2ld0l7bt//1NrYP2R7ZHo3H47Z3AmhorrPfSX6UdFzSgStcdyTJMMlwMBi0tQ/AnJqc/R7Yvmn6+Q2S7pL0VdfDACymydnvmyX92/aaJv8JvJ7kaLezACyqydnvLyTdsYQtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBMk998AizMdt8TGpv8NuzVx5EaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYhpHbXvN9me2j3Y5CMC1medIfVjSma6GAGhHo6htr0u6V9Lz3c4BcK2aHqmflfSEpD82u4HtQ7ZHtkfj8biVcQDmNzNq2/dJupDkk6vdLsmRJMMkw8Fg0NpAAPNpcqTeJ+l+2+ckvSZpv+2XO10FYGEzo07yVJL1JBuSHpT0bpKHOl8GYCG8Tw0UM9ef3UnynqT3OlkCoBUcqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKMZJ2r9Teyzpfy3f7Q5JF1u+zy6t0t5V2iqt1t6utv4jyRV/w2cnUXfB9ijJsO8dTa3S3lXaKq3W3j628vQbKIaogWJWKeojfQ+Y0yrtXaWt0mrtXfrWlXlNDaCZVTpSA2iAqIFiViJq2wdsf237rO0n+95zNbZftH3B9pd9b5nF9i7bx22ftn3K9uG+N23G9vW2P7L9+XTr031vasL2mu3PbB9d1mNu+ahtr0l6TtLdknZLOmh7d7+rruolSQf6HtHQJUmPJ9kt6U5J/9rC/7a/Stqf5J+S9kg6YPvOnjc1cVjSmWU+4JaPWtJeSWeTfJPkN03+8uYDPW/aVJL3Jf3Q944mknyf5NPp5z9r8s23s99VV5aJX6YXt08/tvRZXtvrku6V9PwyH3cVot4p6dvLLp/XFv3GW2W2NyTdIelEv0s2N30qe1LSBUnHkmzZrVPPSnpC0h/LfNBViBods32jpDckPZbkp773bCbJ70n2SFqXtNf27X1v2ozt+yRdSPLJsh97FaL+TtKuyy6vT7+GFtjerknQryR5s+89TST5UdJxbe1zF/sk3W/7nCYvGffbfnkZD7wKUX8s6Vbbt9i+TpM/fP9Wz5tKsG1JL0g6k+SZvvdcje2B7Zumn98g6S5JX/W7anNJnkqynmRDk+/Zd5M8tIzH3vJRJ7kk6VFJ72hyIuf1JKf6XbU5269K+lDSbbbP236k701XsU/Sw5ocRU5OP+7pe9QmbpZ03PYXmvxHfyzJ0t4mWiX8mChQzJY/UgOYD1EDxRA1UAxRA8UQNVAMUQPFEDVQzP8BQZjWmJ84uIYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAImUlEQVR4nO3dz4tbhR6G8fe904qCF1w0C+mUOy5EKMKtEIrQXUGoP9CtBV0J3VyhgiC69B8QN26KihcURdCFFC9SsCKCtxq1im0VivRiRWiKiLpRqu9dJIsqneYkPSdn8uX5wMBkEpKXMk9PcjLMOIkA1PG3vgcAaBdRA8UQNVAMUQPFEDVQzLYu7nTHjh3Z2Njo4q4BSDp37pwuXrzoK13XSdQbGxsajUZd3DUAScPhcNPrePoNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0yhq2wdsf237rO0nux4FYHEzo7a9Juk5SXdL2i3poO3dXQ8DsJgmR+q9ks4m+SbJb5Jek/RAt7MALKpJ1DslfXvZ5fPTr/2J7UO2R7ZH4/G4rX0A5tTaibIkR5IMkwwHg0FbdwtgTk2i/k7Srssur0+/BmALahL1x5JutX2L7eskPSjprW5nAVjUzF/mn+SS7UclvSNpTdKLSU51vgzAQhr9hY4kb0t6u+MtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVDMzKhtv2j7gu0vlzEIwLVpcqR+SdKBjncAaMnMqJO8L+mHJWwB0AJeUwPFtBa17UO2R7ZH4/G4rbsFMKfWok5yJMkwyXAwGLR1twDmxNNvoJgmb2m9KulDSbfZPm/7ke5nAVjUtlk3SHJwGUMAtIOn30AxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDEzo7a9y/Zx26dtn7J9eBnDACxmW4PbXJL0eJJPbf9d0ie2jyU53fE2AAuYeaRO8n2ST6ef/yzpjKSdXQ8DsJi5XlPb3pB0h6QTV7jukO2R7dF4PG5nHYC5NY7a9o2S3pD0WJKf/np9kiNJhkmGg8GgzY0A5tAoatvbNQn6lSRvdjsJwLVocvbbkl6QdCbJM91PAnAtmhyp90l6WNJ+2yenH/d0vAvAgma+pZXkA0lewhYALeAnyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKGZm1Lavt/2R7c9tn7L99DKGAVjMtga3+VXS/iS/2N4u6QPb/0ny3463AVjAzKiTRNIv04vbpx/pchSAxTV6TW17zfZJSRckHUtyottZABbVKOokvyfZI2ld0l7bt//1NrYP2R7ZHo3H47Z3AmhorrPfSX6UdFzSgStcdyTJMMlwMBi0tQ/AnJqc/R7Yvmn6+Q2S7pL0VdfDACymydnvmyX92/aaJv8JvJ7kaLezACyqydnvLyTdsYQtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBMk998AizMdt8TGpv8NuzVx5EaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYhpHbXvN9me2j3Y5CMC1medIfVjSma6GAGhHo6htr0u6V9Lz3c4BcK2aHqmflfSEpD82u4HtQ7ZHtkfj8biVcQDmNzNq2/dJupDkk6vdLsmRJMMkw8Fg0NpAAPNpcqTeJ+l+2+ckvSZpv+2XO10FYGEzo07yVJL1JBuSHpT0bpKHOl8GYCG8Tw0UM9ef3UnynqT3OlkCoBUcqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKMZJ2r9Teyzpfy3f7Q5JF1u+zy6t0t5V2iqt1t6utv4jyRV/w2cnUXfB9ijJsO8dTa3S3lXaKq3W3j628vQbKIaogWJWKeojfQ+Y0yrtXaWt0mrtXfrWlXlNDaCZVTpSA2iAqIFiViJq2wdsf237rO0n+95zNbZftH3B9pd9b5nF9i7bx22ftn3K9uG+N23G9vW2P7L9+XTr031vasL2mu3PbB9d1mNu+ahtr0l6TtLdknZLOmh7d7+rruolSQf6HtHQJUmPJ9kt6U5J/9rC/7a/Stqf5J+S9kg6YPvOnjc1cVjSmWU+4JaPWtJeSWeTfJPkN03+8uYDPW/aVJL3Jf3Q944mknyf5NPp5z9r8s23s99VV5aJX6YXt08/tvRZXtvrku6V9PwyH3cVot4p6dvLLp/XFv3GW2W2NyTdIelEv0s2N30qe1LSBUnHkmzZrVPPSnpC0h/LfNBViBods32jpDckPZbkp773bCbJ70n2SFqXtNf27X1v2ozt+yRdSPLJsh97FaL+TtKuyy6vT7+GFtjerknQryR5s+89TST5UdJxbe1zF/sk3W/7nCYvGffbfnkZD7wKUX8s6Vbbt9i+TpM/fP9Wz5tKsG1JL0g6k+SZvvdcje2B7Zumn98g6S5JX/W7anNJnkqynmRDk+/Zd5M8tIzH3vJRJ7kk6VFJ72hyIuf1JKf6XbU5269K+lDSbbbP236k701XsU/Sw5ocRU5OP+7pe9QmbpZ03PYXmvxHfyzJ0t4mWiX8mChQzJY/UgOYD1EDxRA1UAxRA8UQNVAMUQPFEDVQzP8BQZjWmJ84uIYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAImUlEQVR4nO3dz4tbhR6G8fe904qCF1w0C+mUOy5EKMKtEIrQXUGoP9CtBV0J3VyhgiC69B8QN26KihcURdCFFC9SsCKCtxq1im0VivRiRWiKiLpRqu9dJIsqneYkPSdn8uX5wMBkEpKXMk9PcjLMOIkA1PG3vgcAaBdRA8UQNVAMUQPFEDVQzLYu7nTHjh3Z2Njo4q4BSDp37pwuXrzoK13XSdQbGxsajUZd3DUAScPhcNPrePoNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0yhq2wdsf237rO0nux4FYHEzo7a9Juk5SXdL2i3poO3dXQ8DsJgmR+q9ks4m+SbJb5Jek/RAt7MALKpJ1DslfXvZ5fPTr/2J7UO2R7ZH4/G4rX0A5tTaibIkR5IMkwwHg0FbdwtgTk2i/k7Srssur0+/BmALahL1x5JutX2L7eskPSjprW5nAVjUzF/mn+SS7UclvSNpTdKLSU51vgzAQhr9hY4kb0t6u+MtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVDMzKhtv2j7gu0vlzEIwLVpcqR+SdKBjncAaMnMqJO8L+mHJWwB0AJeUwPFtBa17UO2R7ZH4/G4rbsFMKfWok5yJMkwyXAwGLR1twDmxNNvoJgmb2m9KulDSbfZPm/7ke5nAVjUtlk3SHJwGUMAtIOn30AxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDEzo7a9y/Zx26dtn7J9eBnDACxmW4PbXJL0eJJPbf9d0ie2jyU53fE2AAuYeaRO8n2ST6ef/yzpjKSdXQ8DsJi5XlPb3pB0h6QTV7jukO2R7dF4PG5nHYC5NY7a9o2S3pD0WJKf/np9kiNJhkmGg8GgzY0A5tAoatvbNQn6lSRvdjsJwLVocvbbkl6QdCbJM91PAnAtmhyp90l6WNJ+2yenH/d0vAvAgma+pZXkA0lewhYALeAnyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKGZm1Lavt/2R7c9tn7L99DKGAVjMtga3+VXS/iS/2N4u6QPb/0ny3463AVjAzKiTRNIv04vbpx/pchSAxTV6TW17zfZJSRckHUtyottZABbVKOokvyfZI2ld0l7bt//1NrYP2R7ZHo3H47Z3AmhorrPfSX6UdFzSgStcdyTJMMlwMBi0tQ/AnJqc/R7Yvmn6+Q2S7pL0VdfDACymydnvmyX92/aaJv8JvJ7kaLezACyqydnvLyTdsYQtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBMk998AizMdt8TGpv8NuzVx5EaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYhpHbXvN9me2j3Y5CMC1medIfVjSma6GAGhHo6htr0u6V9Lz3c4BcK2aHqmflfSEpD82u4HtQ7ZHtkfj8biVcQDmNzNq2/dJupDkk6vdLsmRJMMkw8Fg0NpAAPNpcqTeJ+l+2+ckvSZpv+2XO10FYGEzo07yVJL1JBuSHpT0bpKHOl8GYCG8Tw0UM9ef3UnynqT3OlkCoBUcqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKMZJ2r9Teyzpfy3f7Q5JF1u+zy6t0t5V2iqt1t6utv4jyRV/w2cnUXfB9ijJsO8dTa3S3lXaKq3W3j628vQbKIaogWJWKeojfQ+Y0yrtXaWt0mrtXfrWlXlNDaCZVTpSA2iAqIFiViJq2wdsf237rO0n+95zNbZftH3B9pd9b5nF9i7bx22ftn3K9uG+N23G9vW2P7L9+XTr031vasL2mu3PbB9d1mNu+ahtr0l6TtLdknZLOmh7d7+rruolSQf6HtHQJUmPJ9kt6U5J/9rC/7a/Stqf5J+S9kg6YPvOnjc1cVjSmWU+4JaPWtJeSWeTfJPkN03+8uYDPW/aVJL3Jf3Q944mknyf5NPp5z9r8s23s99VV5aJX6YXt08/tvRZXtvrku6V9PwyH3cVot4p6dvLLp/XFv3GW2W2NyTdIelEv0s2N30qe1LSBUnHkmzZrVPPSnpC0h/LfNBViBods32jpDckPZbkp773bCbJ70n2SFqXtNf27X1v2ozt+yRdSPLJsh97FaL+TtKuyy6vT7+GFtjerknQryR5s+89TST5UdJxbe1zF/sk3W/7nCYvGffbfnkZD7wKUX8s6Vbbt9i+TpM/fP9Wz5tKsG1JL0g6k+SZvvdcje2B7Zumn98g6S5JX/W7anNJnkqynmRDk+/Zd5M8tIzH3vJRJ7kk6VFJ72hyIuf1JKf6XbU5269K+lDSbbbP236k701XsU/Sw5ocRU5OP+7pe9QmbpZ03PYXmvxHfyzJ0t4mWiX8mChQzJY/UgOYD1EDxRA1UAxRA8UQNVAMUQPFEDVQzP8BQZjWmJ84uIYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAImUlEQVR4nO3dz4tbhR6G8fe904qCF1w0C+mUOy5EKMKtEIrQXUGoP9CtBV0J3VyhgiC69B8QN26KihcURdCFFC9SsCKCtxq1im0VivRiRWiKiLpRqu9dJIsqneYkPSdn8uX5wMBkEpKXMk9PcjLMOIkA1PG3vgcAaBdRA8UQNVAMUQPFEDVQzLYu7nTHjh3Z2Njo4q4BSDp37pwuXrzoK13XSdQbGxsajUZd3DUAScPhcNPrePoNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0yhq2wdsf237rO0nux4FYHEzo7a9Juk5SXdL2i3poO3dXQ8DsJgmR+q9ks4m+SbJb5Jek/RAt7MALKpJ1DslfXvZ5fPTr/2J7UO2R7ZH4/G4rX0A5tTaibIkR5IMkwwHg0FbdwtgTk2i/k7Srssur0+/BmALahL1x5JutX2L7eskPSjprW5nAVjUzF/mn+SS7UclvSNpTdKLSU51vgzAQhr9hY4kb0t6u+MtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVDMzKhtv2j7gu0vlzEIwLVpcqR+SdKBjncAaMnMqJO8L+mHJWwB0AJeUwPFtBa17UO2R7ZH4/G4rbsFMKfWok5yJMkwyXAwGLR1twDmxNNvoJgmb2m9KulDSbfZPm/7ke5nAVjUtlk3SHJwGUMAtIOn30AxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDEzo7a9y/Zx26dtn7J9eBnDACxmW4PbXJL0eJJPbf9d0ie2jyU53fE2AAuYeaRO8n2ST6ef/yzpjKSdXQ8DsJi5XlPb3pB0h6QTV7jukO2R7dF4PG5nHYC5NY7a9o2S3pD0WJKf/np9kiNJhkmGg8GgzY0A5tAoatvbNQn6lSRvdjsJwLVocvbbkl6QdCbJM91PAnAtmhyp90l6WNJ+2yenH/d0vAvAgma+pZXkA0lewhYALeAnyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKGZm1Lavt/2R7c9tn7L99DKGAVjMtga3+VXS/iS/2N4u6QPb/0ny3463AVjAzKiTRNIv04vbpx/pchSAxTV6TW17zfZJSRckHUtyottZABbVKOokvyfZI2ld0l7bt//1NrYP2R7ZHo3H47Z3AmhorrPfSX6UdFzSgStcdyTJMMlwMBi0tQ/AnJqc/R7Yvmn6+Q2S7pL0VdfDACymydnvmyX92/aaJv8JvJ7kaLezACyqydnvLyTdsYQtAFrAT5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBMk998AizMdt8TGpv8NuzVx5EaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYhpHbXvN9me2j3Y5CMC1medIfVjSma6GAGhHo6htr0u6V9Lz3c4BcK2aHqmflfSEpD82u4HtQ7ZHtkfj8biVcQDmNzNq2/dJupDkk6vdLsmRJMMkw8Fg0NpAAPNpcqTeJ+l+2+ckvSZpv+2XO10FYGEzo07yVJL1JBuSHpT0bpKHOl8GYCG8Tw0UM9ef3UnynqT3OlkCoBUcqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKMZJ2r9Teyzpfy3f7Q5JF1u+zy6t0t5V2iqt1t6utv4jyRV/w2cnUXfB9ijJsO8dTa3S3lXaKq3W3j628vQbKIaogWJWKeojfQ+Y0yrtXaWt0mrtXfrWlXlNDaCZVTpSA2iAqIFiViJq2wdsf237rO0n+95zNbZftH3B9pd9b5nF9i7bx22ftn3K9uG+N23G9vW2P7L9+XTr031vasL2mu3PbB9d1mNu+ahtr0l6TtLdknZLOmh7d7+rruolSQf6HtHQJUmPJ9kt6U5J/9rC/7a/Stqf5J+S9kg6YPvOnjc1cVjSmWU+4JaPWtJeSWeTfJPkN03+8uYDPW/aVJL3Jf3Q944mknyf5NPp5z9r8s23s99VV5aJX6YXt08/tvRZXtvrku6V9PwyH3cVot4p6dvLLp/XFv3GW2W2NyTdIelEv0s2N30qe1LSBUnHkmzZrVPPSnpC0h/LfNBViBods32jpDckPZbkp773bCbJ70n2SFqXtNf27X1v2ozt+yRdSPLJsh97FaL+TtKuyy6vT7+GFtjerknQryR5s+89TST5UdJxbe1zF/sk3W/7nCYvGffbfnkZD7wKUX8s6Vbbt9i+TpM/fP9Wz5tKsG1JL0g6k+SZvvdcje2B7Zumn98g6S5JX/W7anNJnkqynmRDk+/Zd5M8tIzH3vJRJ7kk6VFJ72hyIuf1JKf6XbU5269K+lDSbbbP236k701XsU/Sw5ocRU5OP+7pe9QmbpZ03PYXmvxHfyzJ0t4mWiX8mChQzJY/UgOYD1EDxRA1UAxRA8UQNVAMUQPFEDVQzP8BQZjWmJ84uIYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DQNAgent__policy',\n",
       " '_DQNAgent__test_policy',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_on_test_begin',\n",
       " '_on_test_end',\n",
       " '_on_train_begin',\n",
       " '_on_train_end',\n",
       " 'backward',\n",
       " 'batch_size',\n",
       " 'compile',\n",
       " 'compiled',\n",
       " 'compute_batch_q_values',\n",
       " 'compute_q_values',\n",
       " 'custom_model_objects',\n",
       " 'delta_clip',\n",
       " 'dueling_type',\n",
       " 'enable_double_dqn',\n",
       " 'enable_dueling_network',\n",
       " 'fit',\n",
       " 'forward',\n",
       " 'gamma',\n",
       " 'get_config',\n",
       " 'layers',\n",
       " 'load_weights',\n",
       " 'memory',\n",
       " 'memory_interval',\n",
       " 'metrics_names',\n",
       " 'model',\n",
       " 'nb_actions',\n",
       " 'nb_steps_warmup',\n",
       " 'policy',\n",
       " 'process_state_batch',\n",
       " 'processor',\n",
       " 'recent_action',\n",
       " 'recent_observation',\n",
       " 'reset_states',\n",
       " 'save_weights',\n",
       " 'step',\n",
       " 'target_model',\n",
       " 'target_model_update',\n",
       " 'test',\n",
       " 'test_policy',\n",
       " 'train_interval',\n",
       " 'trainable_model',\n",
       " 'training',\n",
       " 'update_target_model_hard']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.8869322e-38, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((5,5))\n",
    "a[0][0] = 1\n",
    "b = np.zeros((5,5))\n",
    "b[0][0] = 1\n",
    "dqn.compute_q_values(state=np.concatenate((b, a), axis=1).reshape(1,5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dqn.compute_q_values(state=np.concatenate((np.zeros((5,5)), a), axis=1).reshape(1,5,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(dqn.compute_q_values(state=np.concatenate((np.zeros((5,5)), a), axis=1).reshape(1,5,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
