{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Tuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.ndimage.measurements import label\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import copy\n",
    "class PackEnv2(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, board_shape = (5, 5), input_shapes=[],max_moves=100, replacement=True):\n",
    "        self.counter = 0\n",
    "        self.max_moves = max_moves\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.board_shape = board_shape\n",
    "        self.observation_space = np.zeros((board_shape[0], board_shape[1]*2))\n",
    "        self.action_space = Discrete(board_shape[0]*board_shape[1]+1)\n",
    "        self.state = [np.zeros(board_shape),np.zeros(board_shape)]\n",
    "        self.return_state = np.concatenate((self.state[0], self.state[1]), axis=1)\n",
    "        self.replace = replacement\n",
    "\n",
    "        self.num_possible_moves = board_shape[0]*board_shape[1]\n",
    "\n",
    "        if len(input_shapes) == 0:\n",
    "            mat = np.zeros(board_shape)\n",
    "            mat[0][0] = 1\n",
    "            self.shapes = [mat]\n",
    "        else:\n",
    "            self.shapes = []\n",
    "            for shape in input_shapes:\n",
    "                base_mat = np.zeros(board_shape)\n",
    "                for i in range(len(shape)):\n",
    "                    for j in range(len(shape[0])):\n",
    "                        base_mat[i][j] = shape[i][j]\n",
    "                self.shapes.append(base_mat)\n",
    "        self.remaining_shapes = copy.deepcopy(self.shapes)\n",
    "        val = random.choice(range(len(self.shapes)))\n",
    "        self.state[1] = self.shapes[val]\n",
    "        if not self.replace:\n",
    "            self.remaining_shapes.pop(val)\n",
    "\n",
    "    def reset(self):\n",
    "        val = random.choice(range(len(self.shapes)))\n",
    "        random_shape = self.shapes[val]\n",
    "        self.counter = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.state = [np.zeros(self.board_shape), random_shape]\n",
    "        self.return_state = np.concatenate((self.state[0], self.state[1]), axis=1)\n",
    "        self.remaining_shapes = copy.deepcopy(self.shapes)\n",
    "        if not self.replace:\n",
    "            self.remaining_shapes.pop(val)\n",
    "        return self.return_state\n",
    "\n",
    "\n",
    "    def valid_move(self, target):\n",
    "        state = self.state\n",
    "        board = state[0]\n",
    "        piece = state[1]\n",
    "        h = self.board_shape[0]\n",
    "        w = self.board_shape[1]\n",
    "\n",
    "        #do nothing\n",
    "        if target == h * w:\n",
    "            return True\n",
    "\n",
    "        if target > h*w or target < 0:\n",
    "            return False\n",
    "\n",
    "        h_offset = int(target / h)\n",
    "        w_offset = target % w\n",
    "\n",
    "        for H in range(len(piece)):\n",
    "            for W in range(len(piece[0])):\n",
    "                if piece[H][W] == 1:\n",
    "                    if (h_offset + H >= h) or (w_offset + W  >= w):\n",
    "                        return False\n",
    "                    if board[H+h_offset][W+w_offset] == 1:\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def calculate_reward(self, target):\n",
    "        state = self.state\n",
    "        board = state[0]\n",
    "        h = self.board_shape[0]\n",
    "        w = self.board_shape[1]\n",
    "        if target == self.num_possible_moves:\n",
    "            return -.05\n",
    "\n",
    "        #connection structure\n",
    "        #structure = np.ones((3, 3), dtype=np.int)\n",
    "        structure = np.array([[0,1,0],[1,1,1],[0,1,0]])\n",
    "        labeled, ncomponents = label(board, structure)\n",
    "        component_num = labeled[int(target/h)][target % w]\n",
    "        indices = np.indices(board.shape).T[:,:,[1, 0]]\n",
    "        component = indices[labeled == component_num]\n",
    "\n",
    "        size = len(component)\n",
    "        max_h = max([pair[0] for pair in component])\n",
    "        min_h = min([pair[0] for pair in component])\n",
    "        max_w = max([pair[1] for pair in component])\n",
    "        min_w = min([pair[1] for pair in component])\n",
    "        block_size = abs(max_h-min_h + 1)*abs(max_w-min_w + 1)\n",
    "        return size**2/block_size\n",
    "\n",
    "    def merge(self, target):\n",
    "        state = self.state\n",
    "        board = state[0]\n",
    "        piece = state[1]\n",
    "        h = self.board_shape[0]\n",
    "        w = self.board_shape[1]\n",
    "\n",
    "        #do nothing\n",
    "        if target == h * w:\n",
    "            return state[0]\n",
    "\n",
    "        h_offset = int(target / h)\n",
    "        w_offset = target % w\n",
    "\n",
    "        for H in range(len(piece)):\n",
    "            for W in range(len(piece[0])):\n",
    "                if piece[H][W] == 1:\n",
    "                    #print(\"HIIIIIII\")\n",
    "                    board[H+h_offset][W+w_offset] = 1\n",
    "        return board\n",
    "\n",
    "    def final_reward(self):\n",
    "        h = self.board_shape[0]\n",
    "        w = self.board_shape[1]\n",
    "        state = self.state\n",
    "        board = state[0]\n",
    "        if np.sum(board) == h*w:\n",
    "            return 1000000\n",
    "        else:\n",
    "            return -1000000\n",
    "        \n",
    "\n",
    "    def step(self, target):\n",
    "        h = self.board_shape[0]\n",
    "        w = self.board_shape[1]\n",
    "        if self.done == True:\n",
    "            self.reward = self.final_reward()\n",
    "            print(\"It's over\")\n",
    "            return [self.return_state, self.reward, self.done, {}]\n",
    "        elif target > self.num_possible_moves:\n",
    "            print(\"Impossible. Invalid position\")\n",
    "            return [self.return_state, self.reward, self.done, {}]\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            #print(\"counter\", self.counter)\n",
    "            if (self.counter == self.max_moves):\n",
    "                self.done = True\n",
    "                self.reward = self.final_reward()                \n",
    "                return [self.return_state, self.reward, self.done, {}]\n",
    "            #self.state[0][int(target/h)][target%k] = 1\n",
    "            if not self.valid_move(target):\n",
    "                self.reward = -50000000\n",
    "                return [self.return_state, self.reward, self.done, {}]\n",
    "\n",
    "            self.reward = self.calculate_reward(target)\n",
    "            updated_board = self.merge(target)\n",
    "            self.state[0] = updated_board\n",
    "            \n",
    "            #do nothing so same state\n",
    "            if (target == h*w):\n",
    "                return [self.return_state, self.reward, self.done, {}]\n",
    "            #no pieces left so we're done\n",
    "            if len(self.remaining_shapes) == 0:\n",
    "                print(\"hi\")\n",
    "                self.state[1] = np.zeros(self.board_shape)\n",
    "                self.return_state = np.concatenate((self.state[0], self.state[1]), axis=1)\n",
    "                self.done = True\n",
    "                self.reward = self.final_reward()\n",
    "                return [self.return_state, self.reward, self.done, {}]\n",
    "            else:\n",
    "                val = random.choice(range(len(self.remaining_shapes)))\n",
    "                self.state[1] = self.remaining_shapes[val]\n",
    "                if not self.replace:\n",
    "                    self.remaining_shapes.pop(val)\n",
    "                self.return_state = np.concatenate((self.state[0], self.state[1]), axis=1)\n",
    "                return [self.return_state, self.reward, self.done, {}]\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        fig, ax = plt.subplots()\n",
    "        # define the colors\n",
    "        cmap = mpl.colors.ListedColormap(['w', 'k'])\n",
    "\n",
    "        # create a normalize object the describes the limits of\n",
    "        # each color\n",
    "        bounds = [0., 0.5, 1.]\n",
    "        norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        # plot it\n",
    "        ax.imshow(self.state[0], interpolation='none', cmap=cmap, norm=norm)\n",
    "    def render_piece(self, mode='human'):\n",
    "        fig, ax = plt.subplots()\n",
    "        # define the colors\n",
    "        cmap = mpl.colors.ListedColormap(['w', 'k'])\n",
    "\n",
    "        # create a normalize object the describes the limits of\n",
    "        # each color\n",
    "        bounds = [0., 0.5, 1.]\n",
    "        norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        # plot it\n",
    "        ax.imshow(self.state[1], interpolation='none', cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import gym\n",
    "#import gym_pack\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, models\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrupeQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=10000)\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.01\n",
    "        self.tau = .08\n",
    "        self.min_tau = .02\n",
    "        self.tau_decay = .999\n",
    "\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "    \n",
    "    #def subgrid_q()\n",
    "    \n",
    "    \n",
    "    def grid_preprocess(self, board):\n",
    "        return\n",
    "    #input should be a binary array describing which action we are taking\n",
    "    #along with the current state space which is made smaller by solving \n",
    "    #using subproblems and then put through several convolutional layers\n",
    "    #then this is put through several fully connected layers and finally\n",
    "    #output through a single node\n",
    "    def create_model(self):\n",
    "        input1 = layers.Input(shape=(3,6,1))\n",
    "        input2 = layers.Input(shape=(10,))\n",
    "        conved1 = layers.Conv2D(32, (5, 5),padding=\"same\",input_shape=(3,6,1), activation=\"relu\")(input1)\n",
    "        conved2 = layers.Conv2D(32, (5, 5),padding=\"same\",input_shape=(3,6,1), activation=\"relu\")(conved1)\n",
    "        compressed = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(conved2)\n",
    "        x = Flatten()(compressed)\n",
    "        processed = Dense(10, activation=\"relu\")(x)\n",
    "\n",
    "        merged = keras.layers.Concatenate(axis=1)([processed, input2])\n",
    "        a = Dense(24, activation=\"relu\")(merged)\n",
    "        b = Dense(48, activation=\"relu\")(a)\n",
    "        #flat = Flatten()(b)\n",
    "        output = Dense(1, activation=\"relu\")(b)\n",
    "        model = keras.models.Model(inputs=[input1, input2], output=output)\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=.01))\n",
    "        print(model.summary())\n",
    "        #print(model.input_shape)\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        #print(state)\n",
    "        #state = state.reshape(5, 10,1)\n",
    "        u\n",
    "        for i in range(len(predictions)):\n",
    "            if not self.env.valid_move(i):\n",
    "                predictions[i] -= 100000000\n",
    "        print(predictions)\n",
    "        return np.argmax(predictions)\n",
    "    \n",
    "    def trained_act(self, state):\n",
    "        predictions = self.model.predict(state.reshape(1,2, 4,1))[0]\n",
    "        for i in range(len(predictions)):\n",
    "            if not self.env.valid_move(i):\n",
    "                predictions[i] -= 100000000\n",
    "        print(predictions)\n",
    "        predictions[-1] -=1000000000\n",
    "        return np.argmax(predictions)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            #print(\"sample state is \", state, action)\n",
    "            #state = state.reshape(5, 10,1)\n",
    "            target = self.target_model.predict(state.reshape(1,2, 4,1))\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                #print('REPLAY', self.target_model.predict(new_state)[0])\n",
    "                Q_future = max(self.target_model.predict(new_state.reshape(1,2, 4,1))[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        self.tau = max(self.min_tau, self.tau*self.tau_decay)\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "def main():\n",
    "    env = PackEnv(board_shape=(2,2), input_shapes=[[[1]]]*2+[[[1,1],[0,0]]], replacement=False, max_moves=4)\n",
    "    gamma   = 0.95\n",
    "    epsilon = .99\n",
    "\n",
    "    trials  = 500\n",
    "    trial_len = 50\n",
    "\n",
    "    # updateTargetNetwork = 1000\n",
    "    dqn_agent = DQN(env=env)\n",
    "    steps = []\n",
    "    for trial in range(trials):\n",
    "        #cur_state = env.reset().reshape(5,10)\n",
    "        cur_state = env.reset()\n",
    "        #print(\"cur\",cur_state)\n",
    "        cur_state = cur_state.reshape(1,2,4,1)\n",
    "        #print(\"updated, cur_state\")\n",
    "        for step in range(trial_len):\n",
    "            #print(\"cur_state\",cur_state)\n",
    "            action = dqn_agent.act(cur_state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            #print(\"info\", new_state, reward, done)\n",
    "\n",
    "            reward = reward/abs(reward) if not done else -.01\n",
    "            new_state = new_state.reshape(1,2,4,1)\n",
    "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "            \n",
    "            dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "            dqn_agent.target_train() # iterates target model\n",
    "\n",
    "            cur_state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        \"\"\"\n",
    "        if step >= 199:\n",
    "            print(\"Failed to complete in trial {}\".format(trial))\n",
    "            if step % 10 == 0:\n",
    "                dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "        else:\n",
    "            print(\"Completed in {} trials\".format(trial))\n",
    "            dqn_agent.save_model(\"success.model\")\n",
    "            break\n",
    "        \"\"\"\n",
    "        print(\"trial #{}\".format(trial))\n",
    "    return env, dqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 3, 6, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 3, 6, 32)     832         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 3, 6, 32)     25632       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 2, 3, 32)     0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 192)          0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 10)           1930        flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 20)           0           dense_25[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 24)           504         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 48)           1200        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1)            49          dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,147\n",
      "Trainable params: 30,147\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "input1 = layers.Input(shape=(3,6,1))\n",
    "input2 = layers.Input(shape=(10,))\n",
    "conved1 = layers.Conv2D(32, (5, 5),padding=\"same\",input_shape=(3,6,1), activation=\"relu\")(input1)\n",
    "conved2 = layers.Conv2D(32, (5, 5),padding=\"same\",input_shape=(3,6,1), activation=\"relu\")(conved1)\n",
    "compressed = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(conved2)\n",
    "x = Flatten()(compressed)\n",
    "processed = Dense(10, activation=\"relu\")(x)\n",
    "\n",
    "merged = keras.layers.Concatenate(axis=1)([processed, input2])\n",
    "a = Dense(24, activation=\"relu\")(merged)\n",
    "b = Dense(48, activation=\"relu\")(a)\n",
    "#flat = Flatten()(b)\n",
    "output = Dense(1, activation=\"relu\")(b)\n",
    "model = keras.models.Model(inputs=[input1, input2], output=output)\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "    optimizer=Adam(lr=.01))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
